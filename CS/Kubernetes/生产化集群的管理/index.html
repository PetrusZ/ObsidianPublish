<!doctype html><html lang=en><head><script async defer data-website-id=de560d7e-374a-4694-943f-f3f01b8dba46 src=https://umami.codeplayer.org:1443/umami.js></script>
<script>function _howxm(){_howxmQueue.push(arguments)}window._howxmQueue=window._howxmQueue||[],_howxm("setAppID","21c1239b-b2d0-45c5-8eab-246d8b16a9d6"),function(){if(t="howxm_script",!document.getElementById(t)){var t,e=document.createElement("script"),n=document.getElementsByTagName("script")[0];e.setAttribute("id",t),e.type="text/javascript",e.async=!0,e.src="https://static.howxm.com/sdk.js",n.parentNode.insertBefore(e,n)}}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-PCXDEM5E1Q"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-PCXDEM5E1Q",{anonymize_ip:!1})}</script><meta charset=utf-8><meta name=description content="计算节点 生产化集群的考量  计算节点：  如何批量安装和升级计算节点的操作系统? 如何管理配置计算节点的网络信息? 如何管理不同SKU（StockKeeping Unit）的计算节点? 如何快速下架故障的计算节点? 如何快速扩缩集群的规模?   控制平面：  如何在主节点上下载、安装和升级控制平面组件及其所需的配置文件? 如何确保集群所需的其他插件，例如CoreDNS、监控系统等部署完成? 如何准备控制平面组件的各种安全证书? 如何快速升级或回滚控制平面组件的版本?    操作系统的选择 操作系统的评估与选择  通用操作系统  Ubuntu Centos Fedora   专为容器优化的操作系统  最小化操作系统  CoreOS RedHat Atomic Snappy Ubuntu Core RancherOS      操作系统的评估与选择 操作系统评估和选型的标准"><meta property="og:title" content="生产化集群的管理"><meta property="og:description" content="计算节点 生产化集群的考量  计算节点：  如何批量安装和升级计算节点的操作系统? 如何管理配置计算节点的网络信息? 如何管理不同SKU（StockKeeping Unit）的计算节点? 如何快速下架故障的计算节点? 如何快速扩缩集群的规模?   控制平面：  如何在主节点上下载、安装和升级控制平面组件及其所需的配置文件? 如何确保集群所需的其他插件，例如CoreDNS、监控系统等部署完成? 如何准备控制平面组件的各种安全证书? 如何快速升级或回滚控制平面组件的版本?    操作系统的选择 操作系统的评估与选择  通用操作系统  Ubuntu Centos Fedora   专为容器优化的操作系统  最小化操作系统  CoreOS RedHat Atomic Snappy Ubuntu Core RancherOS      操作系统的评估与选择 操作系统评估和选型的标准"><meta property="og:type" content="website"><meta property="og:image" content="https://obsidian.codeplayer.org/icon.png"><meta property="og:url" content="https://obsidian.codeplayer.org/CS/Kubernetes/%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%9A%84%E7%AE%A1%E7%90%86/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="生产化集群的管理"><meta name=twitter:description content="计算节点 生产化集群的考量  计算节点：  如何批量安装和升级计算节点的操作系统? 如何管理配置计算节点的网络信息? 如何管理不同SKU（StockKeeping Unit）的计算节点? 如何快速下架故障的计算节点? 如何快速扩缩集群的规模?   控制平面：  如何在主节点上下载、安装和升级控制平面组件及其所需的配置文件? 如何确保集群所需的其他插件，例如CoreDNS、监控系统等部署完成? 如何准备控制平面组件的各种安全证书? 如何快速升级或回滚控制平面组件的版本?    操作系统的选择 操作系统的评估与选择  通用操作系统  Ubuntu Centos Fedora   专为容器优化的操作系统  最小化操作系统  CoreOS RedHat Atomic Snappy Ubuntu Core RancherOS      操作系统的评估与选择 操作系统评估和选型的标准"><meta name=twitter:image content="https://obsidian.codeplayer.org/icon.png"><title>生产化集群的管理</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://obsidian.codeplayer.org//icon.png><link href=https://obsidian.codeplayer.org/styles.80333fa2099c0bee674efa435fde378c.min.css rel=stylesheet><link href=https://obsidian.codeplayer.org/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://obsidian.codeplayer.org/js/darkmode.245a0a31da505f6b327aec1fcae860c8.min.js></script>
<script src=https://obsidian.codeplayer.org/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script async src=https://obsidian.codeplayer.org/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://obsidian.codeplayer.org/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://obsidian.codeplayer.org/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://obsidian.codeplayer.org/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://obsidian.codeplayer.org/",fetchData=Promise.all([fetch("https://obsidian.codeplayer.org/indices/linkIndex.d6793f11a0950a8f17c9e572b667f655.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://obsidian.codeplayer.org/indices/contentIndex.376e6f0633db05f6506178a172f67803.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://obsidian.codeplayer.org",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://obsidian.codeplayer.org",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/obsidian.codeplayer.org\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=obsidian.codeplayer.org src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://obsidian.codeplayer.org/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://obsidian.codeplayer.org/>Avalon Obsidian Vault</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>生产化集群的管理</h1><p class=meta>Last updated
Apr 9, 2023
<a href=https://github.com/PetrusZ/avalon-obsidian-vault/tree/main/CS/Kubernetes/%e7%94%9f%e4%ba%a7%e5%8c%96%e9%9b%86%e7%be%a4%e7%9a%84%e7%ae%a1%e7%90%86.md rel=noopener>Edit Source</a></p><ul class=tags><li><a href=https://obsidian.codeplayer.org/tags/CS/Kubernetes/>Cs kubernetes</a></li></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#计算节点>计算节点</a><ol><li><a href=#生产化集群的考量>生产化集群的考量</a></li></ol></li><li><a href=#操作系统的选择>操作系统的选择</a><ol><li><a href=#操作系统的评估与选择>操作系统的评估与选择</a></li><li><a href=#操作系统的评估与选择-1>操作系统的评估与选择</a></li><li><a href=#生态系统与成熟度>生态系统与成熟度</a></li><li><a href=#云原生的原则>云原生的原则</a></li><li><a href=#atomic>Atomic</a></li><li><a href=#最小化主机操作系统>最小化主机操作系统</a></li><li><a href=#操作系统构建流程>操作系统构建流程</a></li><li><a href=#ostree>ostree</a></li><li><a href=#构建ostree>构建ostree</a></li><li><a href=#加载ostree>加载ostree</a></li><li><a href=#操作系统加载>操作系统加载</a></li></ol></li><li><a href=#节点资源管理>节点资源管理</a><ol><li><a href=#numa-node>NUMA Node</a></li><li><a href=#节点资源管理-1>节点资源管理</a></li><li><a href=#状态上报>状态上报</a></li><li><a href=#lease>Lease</a></li><li><a href=#资源预留>资源预留</a></li><li><a href=#capacity和allocatable>Capacity和Allocatable</a></li><li><a href=#节点磁盘管理>节点磁盘管理</a></li><li><a href=#驱逐管理>驱逐管理</a></li><li><a href=#资源可用额监控>资源可用额监控</a></li><li><a href=#驱逐策略>驱逐策略</a></li><li><a href=#基于内存压力的驱逐>基于内存压力的驱逐</a></li><li><a href=#基于磁盘压力的驱逐>基于磁盘压力的驱逐</a></li><li><a href=#容器和资源配置>容器和资源配置</a></li><li><a href=#cpu-cgroup配置>CPU CGroup配置</a></li><li><a href=#内存cgroup配置>内存CGroup配置</a></li><li><a href=#oom-killer行为>OOM Killer行为</a></li><li><a href=#imagepngassetsimage_1666279569545_0png><img src=Assets/image_1666279569545_0.png alt=image.png></a></li><li><a href=#日志管理>日志管理</a></li><li><a href=#docker卷管理>Docker卷管理</a></li><li><a href=#网络资源>网络资源</a></li><li><a href=#进程数>进程数</a></li></ol></li><li><a href=#节点异常检测>节点异常检测</a><ol><li><a href=#kubernetes集群可能存在的问题>Kubernetes集群可能存在的问题</a></li><li><a href=#node-problem-detector>node-problem-detector</a></li><li><a href=#故障分类>故障分类</a></li><li><a href=#问题汇报手段>问题汇报手段</a></li><li><a href=#使用插件-pod-启用npd>使用插件 pod 启用NPD</a></li><li><a href=#npd的异常处理行为>NPD的异常处理行为</a></li></ol></li><li><a href=#常用节点问题排查手段>常用节点问题排查手段</a><ol><li><a href=#ssh到内网节点>ssh到内网节点</a></li><li><a href=#查看日志>查看日志</a></li></ol></li><li><a href=#基于extended-resource扩展节点资源>基于extended resource扩展节点资源</a><ol><li><a href=#扩展资源>扩展资源</a></li><li><a href=#管理扩展资源>管理扩展资源</a></li><li><a href=#为节点配置资源>为节点配置资源</a></li><li><a href=#使用扩展资源>使用扩展资源</a></li><li><a href=#集群层面的扩展资源>集群层面的扩展资源</a></li></ol></li><li><a href=#构建和管理高可用集群>构建和管理高可用集群</a><ol><li><a href=#kubernetes高可用层级>Kubernetes高可用层级</a></li><li><a href=#imagepngassetsimage_1667027450698_0png><img src=Assets/image_1667027450698_0.png alt=image.png></a></li><li><a href=#高可用的数据中心>高可用的数据中心</a></li><li><a href=#node的生命周期管理>Node的生命周期管理</a></li><li><a href=#主机管理>主机管理</a></li><li><a href=#生产化集群管理>生产化集群管理</a></li><li><a href=#企业公共服务>企业公共服务</a></li><li><a href=#控制平面的高可用保证>控制平面的高可用保证</a></li><li><a href=#高可用集群>高可用集群</a></li><li><a href=#集群安装方法比较>集群安装方法比较</a></li><li><a href=#用kubespray搭建高可用集群搭建>用Kubespray搭建高可用集群搭建</a></li><li><a href=#基于声明式api管理集群>基于声明式API管理集群</a></li><li><a href=#kubernetes-cluster-api>Kubernetes Cluster API</a></li><li><a href=#参与角色>参与角色</a></li><li><a href=#涉及模型>涉及模型</a></li><li><a href=#日常运营中的节点问题归类>日常运营中的节点问题归类</a></li><li><a href=#故障监测和自动恢复>故障监测和自动恢复</a></li></ol></li><li><a href=#cluster-autoscaler>Cluster Autoscaler</a><ol><li><a href=#工作机制>工作机制</a></li><li><a href=#cluster-autoscaler架构>Cluster AutoScaler架构</a></li><li><a href=#cluster-autoscaler的扩展机制>Cluster Autoscaler的扩展机制</a></li></ol></li><li><a href=#多租户集群管理>多租户集群管理</a><ol><li><a href=#租户>租户</a></li><li><a href=#认证-实现多租户的基础>认证-实现多租户的基础</a></li><li><a href=#隔离>隔离</a></li><li><a href=#租户隔离手段>租户隔离手段</a></li><li><a href=#权限隔离>权限隔离</a></li><li><a href=#quota管理>Quota管理</a></li><li><a href=#节点资源隔离>节点资源隔离</a></li></ol></li></ol></nav></details></aside><a href=#计算节点><h1 id=计算节点><span class=hanchor arialabel=Anchor># </span>计算节点</h1></a><a href=#生产化集群的考量><h2 id=生产化集群的考量><span class=hanchor arialabel=Anchor># </span>生产化集群的考量</h2></a><ul><li>计算节点：<ul><li>如何批量安装和升级计算节点的操作系统?</li><li>如何管理配置计算节点的网络信息?</li><li>如何管理不同SKU（StockKeeping Unit）的计算节点?</li><li>如何快速下架故障的计算节点?</li><li>如何快速扩缩集群的规模?</li></ul></li><li>控制平面：<ul><li>如何在主节点上下载、安装和升级控制平面组件及其所需的配置文件?</li><li>如何确保集群所需的其他插件，例如CoreDNS、监控系统等部署完成?</li><li>如何准备控制平面组件的各种安全证书?</li><li>如何快速升级或回滚控制平面组件的版本?</li></ul></li></ul><a href=#操作系统的选择><h1 id=操作系统的选择><span class=hanchor arialabel=Anchor># </span>操作系统的选择</h1></a><a href=#操作系统的评估与选择><h2 id=操作系统的评估与选择><span class=hanchor arialabel=Anchor># </span>操作系统的评估与选择</h2></a><ul><li>通用操作系统<ul><li>Ubuntu</li><li>Centos</li><li>Fedora</li></ul></li><li>专为容器优化的操作系统<ul><li>最小化操作系统<ul><li>CoreOS</li><li>RedHat Atomic</li><li>Snappy Ubuntu Core</li><li>RancherOS</li></ul></li></ul></li></ul><a href=#操作系统的评估与选择-1><h2 id=操作系统的评估与选择-1><span class=hanchor arialabel=Anchor># </span>操作系统的评估与选择</h2></a><p>操作系统评估和选型的标准</p><ul><li>是否有生态系统</li><li>成熟度</li><li>内核版本</li><li>对运行时的支持</li><li>Init System</li><li>包管理和系统升级</li><li>安全</li></ul><a href=#生态系统与成熟度><h2 id=生态系统与成熟度><span class=hanchor arialabel=Anchor># </span>生态系统与成熟度</h2></a><p>容器优化操作系统的优势</p><ul><li>小</li><li>原子级升级和回退</li><li>更高的安全性</li></ul><p><img src=https://obsidian.codeplayer.org//Assets/image_1666270636230_0.png width=auto alt=image.png></p><a href=#云原生的原则><h2 id=云原生的原则><span class=hanchor arialabel=Anchor># </span>云原生的原则</h2></a><p>可变基础设施的风险</p><ul><li>在灾难发生的时候，难以重新构建服务。持续过多的手工操作，缺乏记录，会导致很难由标准初始化后的服务器来重新构建起等效的服务。</li><li>在服务运行过程中，持续的修改服务器，就犹如程序中的可变变量的值发生变化而引入的状态不一致的并发风险。这些对于服务器的修改，同样会引入中间状态，从而导致不可预知的问题。</li></ul><p>不可变基础设施（immutable infrastructure）</p><ul><li>不可变的容器镜像</li><li>不可变的主机操作系统</li></ul><a href=#atomic><h2 id=atomic><span class=hanchor arialabel=Anchor># </span>Atomic</h2></a><ul><li>由Red Hat支持的软件包安装系统</li><li>多种Distro<ul><li>Fedora</li><li>CentOS</li><li>RHEL</li></ul></li><li>优势<ul><li>不可变操作系统，面向容器优化的基础设施<ul><li>灵活和安全性较好</li><li>只有/etc和/var可以修改，其他目录均为只读</li></ul></li><li>基于rpm-ostree管理系统包<ul><li>rpm-ostree是一个开源项目，使得生产系统中构建镜像非常简单</li><li>支持操作系统升级和回滚的原子操作</li></ul></li></ul></li></ul><a href=#最小化主机操作系统><h2 id=最小化主机操作系统><span class=hanchor arialabel=Anchor># </span>最小化主机操作系统</h2></a><ul><li>原则：</li><li>最小化主机操作系统</li><li>只安装必要的工具<ul><li>必要：支持系统运行的最小工具集</li><li>任何调试工具，比如性能排查，网络排查工具，均可以后期以容器形式运行</li></ul></li><li>意义<ul><li>性能</li><li>稳定性</li><li>安全保障</li></ul></li></ul><a href=#操作系统构建流程><h2 id=操作系统构建流程><span class=hanchor arialabel=Anchor># </span>操作系统构建流程</h2></a><p><img src=https://obsidian.codeplayer.org//Assets/image_1666271335385_0.png width=auto alt=image.png></p><a href=#ostree><h2 id=ostree><span class=hanchor arialabel=Anchor># </span>ostree</h2></a><p>提供一个共享库（libostree）和一些列命令行</p><p>提供与git命令行一致的体验，可以提交或者下载一个完整的可启动的文件系统树</p><p>提供将ostree部署进bootloader的机制</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>https://github.com/ostreedev/ostree/blob/main/src/boot/dracut/module-setup.sh
</span></span><span class=line><span class=cl>install<span class=o>()</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>	dracut_install /usr/lib/ostree/ostree-prepare-root
</span></span><span class=line><span class=cl>	inst simple <span class=s2>&#34;</span><span class=nv>$systemdsystemunitdir</span><span class=s2>}/ostree-prepare-root.service&#34;</span>
</span></span><span class=line><span class=cl>	mkdir-p<span class=s2>&#34;</span><span class=si>${</span><span class=nv>initdir</span><span class=si>}${</span><span class=nv>systemdsystemconfdir</span><span class=si>}</span><span class=s2>/initrd-root-fs.target.wants&#34;</span>
</span></span><span class=line><span class=cl>	ln_r<span class=s2>&#34;</span><span class=si>${</span><span class=nv>systemdsystemunitdir</span><span class=si>}</span><span class=s2>/ostree-prepare-root.service&#34;</span><span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>		<span class=s2>&#34;</span><span class=nv>$systemdsystemconfdiry</span><span class=s2>/initrd-root-fs.target.wants/ostree-prepare-root.service&#34;</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span></code></pre></td></tr></table></div></div><a href=#构建ostree><h2 id=构建ostree><span class=hanchor arialabel=Anchor># </span>构建ostree</h2></a><ul><li>rpm-ostree<ul><li>基于treefile将rpm包构建成为ostree 管理ostree以及bootloader配置</li></ul></li><li>treefile<ul><li>refer：分支名（版本，cpu架构）</li><li>repo:：rpm package repositories</li><li>packages：待安装组件</li></ul></li><li>将rpm构建成ostree<ul><li>rpm-ostree compose tree &ndash;unified-core &ndash;cachedir=cache&ndash;repo=./build-repo /path/to/treefile.json</li></ul></li></ul><p><img src=https://obsidian.codeplayer.org//Assets/image_1666271634113_0.png width=auto alt=image.png></p><a href=#加载ostree><h2 id=加载ostree><span class=hanchor arialabel=Anchor># </span>加载ostree</h2></a><p>初始化项目</p><p>ostree admin os-init centos-atomic-host</p><p>导入ostree repo</p><p>ostree remote add atomic
<a href=http://ostree.svr/ostree rel=noopener>http://ostree.svr/ostree</a></p><p><img src=https://obsidian.codeplayer.org//Assets/image_1666271736326_0.png width=auto alt=image.png></p><p>拉取ostree</p><p>ostree pull atomic centos-atomic-host/8/x86_64/standard</p><p>部署os</p><p>ostree admin deploy&ndash;os=centos-atomic-host centos-atomic-host/8/x86_64/standard&ndash;karg=&lsquo;root=/dev/atomicos/root&rsquo;</p><a href=#操作系统加载><h2 id=操作系统加载><span class=hanchor arialabel=Anchor># </span>操作系统加载</h2></a><ul><li>物理机<ul><li>物理机通常需要通过foreman启动，foreman通过pxe boot，并加载kickstart</li><li>kickstart通过ostree deploy即可完成操作系统的部署</li></ul></li><li>虚拟机<ul><li>需要通过镜像工具将ostree构建成qcow2格式，vhd，raw等模式</li></ul></li></ul><a href=#节点资源管理><h1 id=节点资源管理><span class=hanchor arialabel=Anchor># </span>节点资源管理</h1></a><a href=#numa-node><h2 id=numa-node><span class=hanchor arialabel=Anchor># </span>NUMA Node</h2></a><p>Non-Uniform Memory Access是一种内存访问方式，是为多处理器计算机设计的内存架构</p><p><img src=https://obsidian.codeplayer.org//Assets/image_1666276412414_0.png width=auto alt=image.png></p><a href=#节点资源管理-1><h2 id=节点资源管理-1><span class=hanchor arialabel=Anchor># </span>节点资源管理</h2></a><ul><li>状态汇报</li><li>资源预留</li><li>防止节点资源耗尽的防御机制驱逐</li><li>容器和系统资源的配置</li></ul><a href=#状态上报><h2 id=状态上报><span class=hanchor arialabel=Anchor># </span>状态上报</h2></a><p>kubelet周期性地向API Server进行汇报，并更新节点的相关健康和资源使用信息</p><ul><li>节点基础信息，包括IP地址、操作系统、内核、运行时、kubelet、kube-proxy版本信息。</li><li>节点资源信息包括CPU、内存、HugePage、临时存储、GPU等注册设备，以及这些资源中可以分配给容器使用的部分。</li><li>调度器在为Pod选择节点时会将机器的状态信息作为依据。</li></ul><p><img src=https://obsidian.codeplayer.org//Assets/image_1666276786048_0.png width=auto alt=image.png></p><a href=#lease><h2 id=lease><span class=hanchor arialabel=Anchor># </span>Lease</h2></a><p>在早期版本kubelet的状态上报直接更新node对象，而上报的信息包含状态信息和资源信息，因此需要传输的数据包较大，给APIServer和etcd造成的压力较大。</p><p>后引入Lease对象用来保存健康信息，在默认40s的nodeleaseDurationSeconds周期内，若Lease对象没有被更新，则对应节点可以被判定为不健康。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>coordination.k8s.io/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Lease</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>creationTimestamp</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;2021-08-19T02:50:09Z&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>k8snode</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>kube-node-lease</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>ownerReferences</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>V1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Node</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>k8snode</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>uid</span><span class=p>:</span><span class=w> </span><span class=m>58679942</span>-<span class=l>e2dd-4ead-aada-385f099d5f56</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>resourceVersion</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;1293702&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>uid</span><span class=p>:</span><span class=w> </span><span class=l>1bf51951-b832-49da-8708-4b224b1ec3ed</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>holderldentity</span><span class=p>:</span><span class=w> </span><span class=l>k8snode</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>leaseDurationSeconds</span><span class=p>:</span><span class=w> </span><span class=m>40</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>renewTime</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;2021-09-08T01:34:16.489589Z&#34;</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><a href=#资源预留><h2 id=资源预留><span class=hanchor arialabel=Anchor># </span>资源预留</h2></a><p>计算节点除用户容器外，还存在很多支撑系统运行的基础服务，譬如systemd、journald、sshd、dockerd、Containerd、kubelet等。</p><p>为了使服务进程能够正常运行，要确保它们在任何时候都可以获取足够的系统资源，所以我们要为这些系统进程预留资源。</p><p>kubelet可以通过众多启动参数为系统预留CPU、内存、PID等资源，比如SystemReserved、KubeReserved等。</p><a href=#capacity和allocatable><h2 id=capacity和allocatable><span class=hanchor arialabel=Anchor># </span>Capacity和Allocatable</h2></a><p>容量资源（Capacity）是指kubelet获取的计算节点当前的资源信息。</p><ul><li>CPU是从/proc/cpuinfo文件中获取的节点CPU核数；</li><li>memory是从/proc/memoryinfo中获取的节点内存大小；</li><li>ephemeral-storage是指节点根分区的大小。</li></ul><p>资源可分配额（Allocatable）是用户Pod可用的资源，是资源容量减去分配给系统的资源的剩余部分。</p><p><img src=https://obsidian.codeplayer.org//Assets/image_1666277432159_0.png width=auto alt=image.png></p><a href=#节点磁盘管理><h2 id=节点磁盘管理><span class=hanchor arialabel=Anchor># </span>节点磁盘管理</h2></a><ul><li>系统分区 nodefs<ul><li>工作目录和容器日志</li></ul></li><li>容器运行时分区 imagefs<ul><li>用户镜像和容器可写层</li><li>容器运行时分区是可选的，可以合并到系统分区中</li></ul></li></ul><a href=#驱逐管理><h2 id=驱逐管理><span class=hanchor arialabel=Anchor># </span>驱逐管理</h2></a><ul><li>kubelet会在系统资源不够时中止一些容器进程，以空出系统资源，保证节点的稳定性。</li><li>但由kubelet发起的驱逐只停止Pod的所有容器进程，并不会直接删除Pod。<ul><li>Pod的status.phase会被标记为Failed</li><li>status.reason会被设置为Evicted</li><li>status.message则会记录被驱逐的原因</li></ul></li></ul><a href=#资源可用额监控><h2 id=资源可用额监控><span class=hanchor arialabel=Anchor># </span>资源可用额监控</h2></a><p>kubelet依赖内嵌的开源软件cAdvisor，周期性检查节点资源使用情况</p><p>CPU是可压缩资源，根据不同进程分配时间配额和权重，CPU可被多个进程竞相使用</p><p>驱逐策略是基于磁盘和内存资源用量进行的，因为两者属于不可压缩的资源，当此类资源使用耗尽时将无法再申请</p><p><img src=https://obsidian.codeplayer.org//Assets/image_1666277809205_0.png width=auto alt=image.png></p><a href=#驱逐策略><h2 id=驱逐策略><span class=hanchor arialabel=Anchor># </span>驱逐策略</h2></a><p>kubelet获得节点的可用额信息后，会结合节点的容量信息来判断当前节点运行的Pod是否满足驱逐条件。</p><p>驱逐条件可以是绝对值或百分比，当监控资源的可使用额少干设定的数值或百分比时，kubelet就会发起驱逐操作。</p><p>kubelet参数evictionMinimumReclaim可以设置每次回收的资源的最小值，以防止小资源的多次回收。</p><p><img src=https://obsidian.codeplayer.org//Assets/image_1666278219341_0.png width=auto alt=image.png></p><a href=#基于内存压力的驱逐><h2 id=基于内存压力的驱逐><span class=hanchor arialabel=Anchor># </span>基于内存压力的驱逐</h2></a><p>memory.avaiable表示当前系统的可用内存情况。</p><p>kubelet默认设置了memory.avaiable&lt;100Mi的硬驱逐条件
当kubelet检测到当前节点可用内存资源紧张并满足驱逐条件时，会将节点的MemoryPressure状
态设置为True，调度器会阻止BestEffort Pod调度到内存承压的节点。</p><p>kubelet启动对内存不足的驱逐操作时，会依照如下的顺序选取目标Pod：</p><p>（1）判断Pod所有容器的内存使用量总和是否超出了请求的内存量，超出请求资源的Pod会成为
备选目标。</p><p>（2）查询Pod的调度优先级，低优先级的Pod被优先驱逐。</p><p>（3）计算Pod所有容器的内存使用量和Pod请求的内存量的差值，差值越小，越不容易被驱逐。</p><a href=#基于磁盘压力的驱逐><h2 id=基于磁盘压力的驱逐><span class=hanchor arialabel=Anchor># </span>基于磁盘压力的驱逐</h2></a><ul><li>以下任何一项满足驱逐条件时，它会将节点的DiskPressure状态设置为True，调度器不会再调度任何Pod到该节点上<ul><li>nodefs.available</li><li>nodefs.inodesFree</li><li>imagefs.available</li><li>imagefs.inodesFree</li></ul></li><li>驱逐行为<ul><li>有容器运行时分区<ul><li>nodefs达到驱逐阈值，那么kubelet删除已经退出的容器</li><li>Imagefs达到驱逐阈值，那么kubelet删除所有未使用的镜像。</li></ul></li></ul></li><li>无容器运行时分区<ul><li>kubelet同时删除未运行的容器和未使用的镜像。</li></ul></li></ul><hr><ul><li>回收已经退出的容器和未使用的镜像后，如果节点依然满足驱逐条件，kubelet就会开始驱逐正在运行的Pod，进一步释放磁盘空间。</li><li>判断Pod的磁盘使用量是否超过请求的大小，超出请求资源的Pod会成为备选目标。</li><li>查询Pod的调度优先级，低优先级的Pod优先驱逐。</li><li>根据磁盘使用超过请求的数量进行排序，差值越小，越不容易被驱逐。</li></ul><a href=#容器和资源配置><h2 id=容器和资源配置><span class=hanchor arialabel=Anchor># </span>容器和资源配置</h2></a><p>针对不同QoS Class的Pod，Kubneretes按如下Hierarchy组织cgroup中的CPU子系统</p><p><img src=https://obsidian.codeplayer.org//Assets/image_1666279135958_0.png width=auto alt=image.png></p><a href=#cpu-cgroup配置><h2 id=cpu-cgroup配置><span class=hanchor arialabel=Anchor># </span>CPU CGroup配置</h2></a><p><img src=https://obsidian.codeplayer.org//Assets/image_1666279354888_0.png width=auto alt=image.png></p><a href=#内存cgroup配置><h2 id=内存cgroup配置><span class=hanchor arialabel=Anchor># </span>内存CGroup配置</h2></a><p><img src=https://obsidian.codeplayer.org//Assets/image_1666279387571_0.png width=auto alt=image.png></p><a href=#oom-killer行为><h2 id=oom-killer行为><span class=hanchor arialabel=Anchor># </span>OOM Killer行为</h2></a><p>系统的OOMKiller可能会采取OOM的方式来中止某些容器的进程，进行必要的内存回收操作而系统根据进程的oom score来进行优先级排序，，选择待终止的进程，且进程的oom score，越高，越容易被终止。</p><p>进程的oom score是根据当前进程使用的内存占节点总内存的比例值乘以10，再加上oom_SCore_adj综合得到的</p><p>而容器进程的oom score adj正是kubelet根据memory.request进行设置的</p><h2 id=imagepngassetsimage_1666279569545_0png><img src=https://obsidian.codeplayer.org//Assets/image_1666279569545_0.png width=auto alt=image.png></h2><a href=#日志管理><h2 id=日志管理><span class=hanchor arialabel=Anchor># </span>日志管理</h2></a><p>节点上需要通过运行logrotate的定时任务对系统服务日志进行rotate清理，以防止系统服务日志占用大量的磁盘空间。</p><ul><li>logrotate的执行周期不能过长，以防日志短时间内大量增长。</li><li>同时配置日志的rotate条件，在日志不占用太多空间的情况下，保证有足够的日志可供查看。</li><li>Docker<ul><li>除了基于系统logrotate管理日志，还可以依赖Docker自带的日志管理功能来设置容器日志的数量和每个日志文件的大小。</li><li>Docker写入数据之前会对日志大小进行检查和rotate操作，确保日志文件不会超过配置的数量和大小。</li></ul></li><li>Containerd<ul><li>日志的管理是通过kubelet定期（默认为10s）执行du命令，来检查容器日志的数量和文件的大小的。</li><li>每个容器日志的大小和可以保留的文件个数，可以通过kubelet的配置参数container-log-maxsize 和container-log-max-files进行调整。</li></ul></li></ul><a href=#docker卷管理><h2 id=docker卷管理><span class=hanchor arialabel=Anchor># </span>Docker卷管理</h2></a><p>在构建容器镜像时，可以在Dockerfile中通过VOLUME指令声明一个存储卷，目前Kubernetes尚未将其纳入管控范围，不建议使用。</p><p>如果容器进程在可写层或emptyDir卷进行大量读写操作，就会导致磁盘I/O过高，从而影响其他容器进程甚至系统进程。</p><p>Docker和Containerd运行时都基于CGroup v1。对于块设备，只支持对DirectI/O限速，而对于Buffer I/O还不具备有效的支持。因此，针对设备限速的问题，目前还没有完美的解决方案，对于有特殊I/O需求的容器，建议使用独立的磁盘空间。</p><a href=#网络资源><h2 id=网络资源><span class=hanchor arialabel=Anchor># </span>网络资源</h2></a><p>由网络插件通过Linux Traffic Control为Pod限制带宽</p><p>可利用CNI社区提供的bandwidth插件</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiversion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Pod</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>	</span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>		</span><span class=nt>kubernetes.io/ingress-bandwidth</span><span class=p>:</span><span class=w> </span><span class=l>10MB</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    	</span><span class=nt>kubernetes.io/egress-bandwidth</span><span class=p>:</span><span class=w> </span><span class=l>10MB</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=l>··</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><a href=#进程数><h2 id=进程数><span class=hanchor arialabel=Anchor># </span>进程数</h2></a><p>kubelet默认不限制Pod可以创建的子进程数量，但可以通过启动参数podPidsLimit开启限制，还可以由reserved参数为系统进程预留进程数。</p><ul><li>kubelet通过系统调用周期性地获取当前系统的PID的使用量，并读取/proc/sys/kernel/pid_max，获取系统支持的PID上限。</li><li>如果当前的可用进程数少于设定阈值，那么kubelet会将节点对象的PIDPressure标记为True</li><li>kube-scheduler在进行调度时，会从备选节点中对处于NodeUnderPIDPressure状态的节点进行过滤。</li></ul><a href=#节点异常检测><h1 id=节点异常检测><span class=hanchor arialabel=Anchor># </span>节点异常检测</h1></a><a href=#kubernetes集群可能存在的问题><h2 id=kubernetes集群可能存在的问题><span class=hanchor arialabel=Anchor># </span>Kubernetes集群可能存在的问题</h2></a><ul><li>基础架构守护程序问题：NTP服务关闭；</li><li>硬件问题：CPU，内存或磁盘损坏；</li><li>内核问题：内核死锁，文件系统损坏；</li><li>容器运行时问题：运行时守护程序无响应</li><li>&mldr;</li></ul><p>当kubernetes中节点发生上述问题，在整个集群中，k8s服务组件并不会感知以上问题，就会导致pod仍会调度至问题节点。</p><a href=#node-problem-detector><h2 id=node-problem-detector><span class=hanchor arialabel=Anchor># </span>node-problem-detector</h2></a><p>为了解决这个问题，社区引入了守护进程node-problem-detector，从各个守护进程收集节点问题，并使它们对上游层可见。</p><p>Kubernetes节点诊断的工具，可以将节点的异常，例如</p><ul><li>Runtime无响应</li><li>Linux Kernel无响应</li><li>网络异常</li><li>文件描述符异常</li><li>硬件问题如CPU，内存或者磁盘故障</li></ul><a href=#故障分类><h2 id=故障分类><span class=hanchor arialabel=Anchor># </span>故障分类</h2></a><p><img src=https://obsidian.codeplayer.org//Assets/image_1666282939883_0.png width=auto alt=image.png></p><a href=#问题汇报手段><h2 id=问题汇报手段><span class=hanchor arialabel=Anchor># </span>问题汇报手段</h2></a><p>node-problem-detector通过设置NodeCondition或者创建Event对象来汇报问题</p><p>NodeCondition：针对永久性故障，会通过设置NodeCondition来改变节点状态</p><p>Event：临时故障通过Event来提醒相关对象，比如通知当前节点运行的所有Pod</p><a href=#使用插件-pod-启用npd><h2 id=使用插件-pod-启用npd><span class=hanchor arialabel=Anchor># </span>使用插件 pod 启用NPD</h2></a><p>如果你使用的是自定义集群引导解决方案，不需要覆盖默认配置，可以利用插件 Pod进一步自动化部署。</p><p>创建 node-strick-detector.yaml，并在控制平面节点上保存配置到插件 Pod 的目录/etc/kubernetes/addons/node-problem-detector。</p><a href=#npd的异常处理行为><h2 id=npd的异常处理行为><span class=hanchor arialabel=Anchor># </span>NPD的异常处理行为</h2></a><p>NPD只负责获取异常事件，并修改node condition，不会对节点状态和调度产生影响</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>lastHeartbeatTime</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;2021-11-06T15:44:46Z&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>lastTransitionTime</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;2021-11-06T15:29:43Z&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>message:&#39;kernel</span><span class=p>:</span><span class=w> </span><span class=l>INFO:task docker:20744 blocked for more than 120 seconds.&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>reason</span><span class=p>:</span><span class=w> </span><span class=l>DockerHung</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>status</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;True&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>KernelDeadlock</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>需要自定义控制器，监听NPD汇报的condition，taint node，阻止pod调度到故障节点</p><p>问题修复后，重启NPD Pod来清理错误事件</p><a href=#常用节点问题排查手段><h1 id=常用节点问题排查手段><span class=hanchor arialabel=Anchor># </span>常用节点问题排查手段</h1></a><a href=#ssh到内网节点><h2 id=ssh到内网节点><span class=hanchor arialabel=Anchor># </span>ssh到内网节点</h2></a><p>创建一个支持ssh的pod</p><p>并通过负载均衡器转发ssh请求</p><a href=#查看日志><h2 id=查看日志><span class=hanchor arialabel=Anchor># </span>查看日志</h2></a><ul><li>针对使用systemd拉起的服务<ul><li><code>journalctl-afu kubelet-S "2019-08-26 15:00:00"</code><ul><li>-u unit，对应的systemd拉起的组件，如kubelet</li><li>-f folloW，跟踪最新日志</li><li>-a show all，现实所有日志列</li><li>-S since，从某一时间开始-S &ldquo;2019-08-26 15∶00∶00&rdquo;</li></ul></li></ul></li><li>对于标准的容器日志<ul><li><code>kubectl logs -f -c &lt;containername> &lt;podname></code></li><li><code>kubectl logs -f --all -containers &lt;podname></code></li><li><code>kubectl logs -f -c &lt;podname> -- previous</code></li></ul></li><li>如果容器日志被shell转储到文件，则需通过exec<ul><li><code>kubectl exec -it xxx --tail -f /path/to/log</code></li></ul></li></ul><a href=#基于extended-resource扩展节点资源><h1 id=基于extended-resource扩展节点资源><span class=hanchor arialabel=Anchor># </span>基于extended resource扩展节点资源</h1></a><a href=#扩展资源><h2 id=扩展资源><span class=hanchor arialabel=Anchor># </span>扩展资源</h2></a><p>扩展资源是 kubernetes.io 域名之外的标准资源名称。 它们使得集群管理员能够颁布非Kubernetes 内置资源，而用户可以使用他们。</p><p>自定义扩展资源无法使用kubernetes.io作为资源域名</p><a href=#管理扩展资源><h2 id=管理扩展资源><span class=hanchor arialabel=Anchor># </span>管理扩展资源</h2></a><ul><li>节点级扩展资源<ul><li>节点级扩展资源绑定到节点</li></ul></li><li>设备插件管理的资源<ul><li>发布在各节点上由设备插件所管理的资源，如GPU，智能网卡等</li></ul></li></ul><a href=#为节点配置资源><h2 id=为节点配置资源><span class=hanchor arialabel=Anchor># </span>为节点配置资源</h2></a><p>集群操作员可以向 API服务器提交 PATCH HTTP请求，以在集群中节点的 status.capacity 中为其配置可用数量。</p><p>完成此操作后，节点的 status.capacity 字段中将包含新资源。</p><p>kubelet会异步地对 status.allocatable字段执行自动更新操作，使之包含新资源。</p><p>调度器在评估 Pod 是否适合在某节点上执行时会使用节点的 status.allocatable 值，在更新节点容量使之包含新资源之后和请求该资源的第一个 Pod 被调度到该节点之间，可能会有短暂的延迟。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl --key admin.key --cert admin.crt --header <span class=s2>&#34;Content-Type: application/jsonpatch+ison&#34;</span><span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>--request PATCH-k<span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>--data <span class=s1>&#39;[{&#34;op&#34;:&#34;add&#34;,&#34;path&#34;:&#34;/status/capacity/cncamp.com~1reclaimed-cpu&#34;, &#34;Value&#34;:&#34;2&#34;}]&#39;</span>
</span></span><span class=line><span class=cl>https://192.168.34.2:6443/api/v1/nodes/cadmin/status
</span></span></code></pre></td></tr></table></div></div><a href=#使用扩展资源><h2 id=使用扩展资源><span class=hanchor arialabel=Anchor># </span>使用扩展资源</h2></a><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nginx</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>nginx</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>resources</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>limits</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>cncamp.com/reclaimed-cpu</span><span class=p>:</span><span class=w> </span><span class=m>3</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>requests</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>cncamp.com/reclaimed-cpu</span><span class=p>:</span><span class=w> </span><span class=m>3</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><a href=#集群层面的扩展资源><h2 id=集群层面的扩展资源><span class=hanchor arialabel=Anchor># </span>集群层面的扩展资源</h2></a><ul><li>可选择由默认调度器管理资源，默认调度器像管理其他资源一样管理扩展资源<ul><li>Request与Limit必须一致，因为Kubernetes无法确保扩展资源的超售</li></ul></li><li>更常见的场景是，由调度器扩展程序（Scheduler Extenders）管理，这些程序处理资源消耗和资源配额<ul><li>修改调度器策略配置ignoredByScheduler字段可配置调度器不要检查自定义资源</li></ul></li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;kind&#34;</span><span class=p>:</span> <span class=s2>&#34;Policy&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;apiVersion&#34;</span><span class=p>:</span> <span class=s2>&#34;v1&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;extenders&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=nt>&#34;urlPrefix&#34;</span><span class=p>:</span> <span class=s2>&#34;&lt;extender-endpoint&gt;&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=nt>&#34;bindVerb&#34;</span><span class=p>:</span> <span class=s2>&#34;bind&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=nt>&#34;managedResources&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>                <span class=p>{</span>
</span></span><span class=line><span class=cl>                    <span class=nt>&#34;name&#34;</span><span class=p>:</span> <span class=s2>&#34;example.com/foo&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=nt>&#34;ignoredByScheduler&#34;</span><span class=p>:</span> <span class=kc>true</span>
</span></span><span class=line><span class=cl>                <span class=p>}</span>
</span></span><span class=line><span class=cl>            <span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><a href=#构建和管理高可用集群><h1 id=构建和管理高可用集群><span class=hanchor arialabel=Anchor># </span>构建和管理高可用集群</h1></a><a href=#kubernetes高可用层级><h2 id=kubernetes高可用层级><span class=hanchor arialabel=Anchor># </span>Kubernetes高可用层级</h2></a><h2 id=imagepngassetsimage_1667027450698_0png><img src=https://obsidian.codeplayer.org//Assets/image_1667027450698_0.png width=auto alt=image.png></h2><a href=#高可用的数据中心><h2 id=高可用的数据中心><span class=hanchor arialabel=Anchor># </span>高可用的数据中心</h2></a><ul><li>多地部署</li><li>每个数据中心需要划分成具有独立供电、制冷、网络设备的高可用区</li><li>每个高可用区管理独立的硬件资产，包括机架、计算节点、存储、负载均衡器、防火墙等硬件设备</li></ul><a href=#node的生命周期管理><h2 id=node的生命周期管理><span class=hanchor arialabel=Anchor># </span>Node的生命周期管理</h2></a><p>运营Kubernetes集群，不仅仅是集群搭建那么简单，运营需要对集群中所有节点的完整申明周期负责。</p><ul><li>集群搭建</li><li>集群扩容/缩容</li><li>集群销毁（很少）</li><li>无论是集群搭建还是扩容，核心是Node的生命周期管理<ul><li>Onboard<ul><li>物理资产上架</li><li>操作系统安装</li><li>网络配置</li><li>Kubernetes组件安装</li><li>创建Node 对象</li></ul></li><li>故障处理<ul><li>临时故障?重启大法好</li><li>永久故障?机器下架</li></ul></li><li>Offboard<ul><li>删除Node对象</li><li>物理资产下架，送修/报废</li></ul></li></ul></li></ul><a href=#主机管理><h2 id=主机管理><span class=hanchor arialabel=Anchor># </span>主机管理</h2></a><p>选定哪个版本的系统内核、哪个发行版、安装哪些工具集、主机网络如何规划等。</p><p>日常的主机镜像升级更新也可能是造成服务不可用的因素之一。</p><p>主机镜像更新可以通过A/B系统OTA（Over The Air）升级方式进行。</p><p>分别使用A、B两个存储空间，共享一份用户数据。在升级过程中，OTA更新即往其中一个存储空间写入升级包，同时保证了另一个系统可以正常运行，而不会打断用户。如果OTA失败，那么设备会启动到OTA之前的磁盘分区，并且仍然可以使用。</p><a href=#生产化集群管理><h2 id=生产化集群管理><span class=hanchor arialabel=Anchor># </span>生产化集群管理</h2></a><ul><li>如何设定单个集群规模<ul><li>社区声明单一集群可支持5000节点，在如此规模的集群中，大规模部署应用是有诸多挑战的。应该更多还是更少?如何权衡?</li></ul></li><li>如何根据地域划分集群<ul><li>不同地域的计算节点划分到同一集群</li><li>将同一地域的节点划分到同一集群</li></ul></li><li>如何规划集群的网络<ul><li>企业办公环境、测试环境、预生产环境和生产环境应如何进行网络分离</li><li>不同租户之间应如何进行网络隔离</li></ul></li><li>如何自动化搭建集群<ul><li>如何自动化搭建和升级集群，包括自动化部署控制平面和数据平面的核心组件</li><li>如何与企业的公共服务集成</li></ul></li></ul><a href=#企业公共服务><h2 id=企业公共服务><span class=hanchor arialabel=Anchor># </span>企业公共服务</h2></a><p>需要与企业认证平台集成，这样企业用户就能通过统一认证平台接入Kubernetes集群，而无须重新设计和管理一套用户系统。</p><p>集成企业的域名服务、负载均衡服务，提供集群服务对企业外发布的访问入口</p><p>在与企业的公共服务集成时，需要考虑它们的服务是否可靠</p><p>对于不能异步调用的请求，采用同步调用需要设置合理的超时时间</p><p>过长的超时时间，，会延迟结果等待时间，导致整体的链路调用时间延长，从而降低整体的TPS</p><p>有些失败是短暂的、偶然的（比如网络抖动），进行重试即可。而有些失败是必然的，重试反而会造成调用请求量放大，加重对调用系统的负担</p><a href=#控制平面的高可用保证><h2 id=控制平面的高可用保证><span class=hanchor arialabel=Anchor># </span>控制平面的高可用保证</h2></a><p>针对大规模的集群，应该为控制平面组件划分单独节点，减少业务容器对控制平面容器或守护进程的干扰和资源抢占</p><p>控制平面所在的节点，应确保在不同机架上，以防止因为某些机架的交换机或电源出问题，造成所有的控制面节点都无法工作</p><p>保证控制平面的每个组件有足够的CPU、内存和磁盘资源，过于严苛的资源限制会导致系统效率低下，降低集群可用性</p><p>应尽可能地减少或消除外部依赖。在Kubneretes初期版本中存在较多Cloud Provider API的调用，导致在运营过程中，当Cloud Provider API出现故障时，会使得Kubernetes集群也无法正常工作。</p><p>应尽可能地将控制平面和数据平面解耦，确保控制平面组件出现故障时，将业务影响降到最低。</p><p>Kubernetes还有一些核心插件，是以普通的Pod形式加载运行的，可能会被调度到任意工作节点，与普通应用竞争资源。这些插件是否正常运行也决定了集群的可用性。</p><a href=#高可用集群><h2 id=高可用集群><span class=hanchor arialabel=Anchor># </span>高可用集群</h2></a><p><img src=https://obsidian.codeplayer.org//Assets/image_1667030270184_0.png width=auto alt=image.png></p><a href=#集群安装方法比较><h2 id=集群安装方法比较><span class=hanchor arialabel=Anchor># </span>集群安装方法比较</h2></a><p><img src=https://obsidian.codeplayer.org//Assets/image_1667030574489_0.png width=auto alt=image.png></p><a href=#用kubespray搭建高可用集群搭建><h2 id=用kubespray搭建高可用集群搭建><span class=hanchor arialabel=Anchor># </span>用Kubespray搭建高可用集群搭建</h2></a><p><img src=https://obsidian.codeplayer.org//Assets/image_1667037652776_0.png width=auto alt=image.png></p><a href=#基于声明式api管理集群><h2 id=基于声明式api管理集群><span class=hanchor arialabel=Anchor># </span>基于声明式API管理集群</h2></a><ul><li>集群管理不仅仅包括集群搭建，还有更多功能需要支持<ul><li>集群扩缩容</li><li>节点健康检查和自动修复</li><li>Kubernetes升级</li><li>操作系统升级</li></ul></li><li>云原生场景中集群应该按照我们的期望的状态运行，这意味着我们应该将集群管理建立在声明式API的基础之上</li></ul><a href=#kubernetes-cluster-api><h2 id=kubernetes-cluster-api><span class=hanchor arialabel=Anchor># </span>Kubernetes Cluster API</h2></a><p><img src=https://obsidian.codeplayer.org//Assets/image_1667038756175_0.png width=auto alt=image.png></p><a href=#参与角色><h2 id=参与角色><span class=hanchor arialabel=Anchor># </span>参与角色</h2></a><ul><li>管理集群<ul><li>管理workload 集群的集群，用来存放Cluster API对象的地方</li></ul></li><li>Workload集群<ul><li>真正开放给用户用来运行应用的集群，由管理集群管理</li></ul></li><li>Infrastructure provider<ul><li>提供不同云的基础架构管理，包括计算节点，网络等。目前流行的公有云多与Cluster API集成了。</li></ul></li><li>Bootstrap provider<ul><li>证书生成</li><li>控制面组件安装和初始化，监控节点的创建</li><li>将主节点和计算节点加入集群</li></ul></li><li>Control plane<ul><li>Kubernetes控制平面组件</li></ul></li></ul><a href=#涉及模型><h2 id=涉及模型><span class=hanchor arialabel=Anchor># </span>涉及模型</h2></a><ul><li>Machine<ul><li>计算节点，用来描述可以运行Kubernetes组件的机器对象（注意与Kubernetes Node）的差异</li><li>一个新Machine被创建以后，对应的控制器会创建一个计算节点，安装好操作系统并更新Machine的状态</li><li>当一个Machine被删除后，对应的控制器会删除掉该节点并回收计算资源。</li><li>当Machine属性被更新以后（比如Kubernetes版本更新），对应的控制器会删除旧节点并创建新节点</li></ul></li><li>Machine Immutability (In-place Upgrade vs. Replace)<ul><li>不可变基础架构</li></ul></li><li>MachineDeployment<ul><li>提供针对Machine和MachineSet的声明式更新，类似于Kubernetes Deployment</li></ul></li><li>MachineSet<ul><li>维护一个稳定的机器集合，类似Kubernetes ReplicaSet</li></ul></li><li>MachineHealthCheck<ul><li>定义节点应该被标记为不可用的条件</li></ul></li></ul><a href=#日常运营中的节点问题归类><h2 id=日常运营中的节点问题归类><span class=hanchor arialabel=Anchor># </span>日常运营中的节点问题归类</h2></a><ul><li>可自动修复的问题<ul><li>计算节点down<ul><li>Ping不通</li><li>TCP probe失败</li><li>节点上的所有应用都不可达</li></ul></li></ul></li><li>不可自动修复的问题<ul><li>文件系统坏</li><li>磁盘阵列故障</li><li>网盘挂载问题</li><li>其他硬件故障</li><li>Kernel出错，core dumps</li></ul></li><li>其他问题<ul><li>软件Bug</li><li>进程锁死，或者memory/CPU竞争问题</li><li>Kubernetes组件出问题<ul><li>Kubelet/Kube-proxy/Docker/Salt</li></ul></li></ul></li></ul><a href=#故障监测和自动恢复><h2 id=故障监测和自动恢复><span class=hanchor arialabel=Anchor># </span>故障监测和自动恢复</h2></a><p>当创建Compute节点时，允许定义Liveness Probe</p><p>当livenessProbe失败时，ComputeNode的ProbePassed设置为false</p><p>在Prometheus中，已经有Node level的alert，抓取Prometheus中的alert</p><p><img src=https://obsidian.codeplayer.org//Assets/image_1667045860645_0.png width=auto alt=image.png></p><p>设定自动恢复规则</p><ul><li>大多数情况下，重启大法好（人人都是Restart operator)。</li><li>如果重启不行就重装。（reprovision）</li><li>重装不行就重修。（breakfix）</li></ul><a href=#cluster-autoscaler><h1 id=cluster-autoscaler><span class=hanchor arialabel=Anchor># </span>Cluster Autoscaler</h1></a><a href=#工作机制><h2 id=工作机制><span class=hanchor arialabel=Anchor># </span>工作机制</h2></a><ul><li>扩容<ul><li>由于资源不足，pod调度失败，即有pod一直处于Pending状态</li></ul></li><li>缩容<ul><li>node的资源利用率较低时，持续10分钟低于50%</li><li>此node上存在的pod都能被重新调度到其他node上运行</li></ul></li></ul><p><img src=https://obsidian.codeplayer.org//Assets/image_1667046078752_0.png width=auto alt=image.png></p><a href=#cluster-autoscaler架构><h2 id=cluster-autoscaler架构><span class=hanchor arialabel=Anchor># </span>Cluster AutoScaler架构</h2></a><ul><li>Autoscaler：核心模块，负责整体扩缩容功能</li><li>Estimator：负责评估计算扩容节点</li><li>Simulator：负责模拟调度，计算缩容节点</li><li>Cloud-Provider：与云交互进行节点的增删操作，每个支持CA的主流厂商都实现自己的plugin实现动态缩放</li></ul><p><img src=https://obsidian.codeplayer.org//Assets/image_1667046243581_0.png width=auto alt=image.png></p><a href=#cluster-autoscaler的扩展机制><h2 id=cluster-autoscaler的扩展机制><span class=hanchor arialabel=Anchor># </span>Cluster Autoscaler的扩展机制</h2></a><ul><li>为了自动创建和初始化Node，Cluster Autoscaler要求Node必须属于某个Node Group，比如<ul><li>GCE/GKE 中的 Managed instance groups（MIG）</li><li>AWS中的 Autoscaling Groups</li><li>Cluster API Node</li></ul></li><li>当集群中有多个Node Group 时，可以通过<code>--expander=&lt;option></code> 选项配置选择Node Group 的策略，支持如下四种方式<ul><li>random：随机选择</li><li>most-pods：选择容量最大（可以创建最多Pod）的Node Group</li><li>least-waste：以最小浪费原则选择，即选择有最少可用资源的 Node Group</li><li>price：选择最便宜的 Node Group</li></ul></li></ul><a href=#多租户集群管理><h1 id=多租户集群管理><span class=hanchor arialabel=Anchor># </span>多租户集群管理</h1></a><a href=#租户><h2 id=租户><span class=hanchor arialabel=Anchor># </span>租户</h2></a><p>租户是指一组拥有访问特定软件资源权限的用户集合，在多租户环境中，它还包括共享的应用、服务、数据和各项配置等</p><p>多租户集群必须将租户彼此隔离，以最大限度地减少租户与租户、租户与集群之间的影响</p><p>集群须在租户之间公平地分配集群资源。通过多租户共享集群资源，可以有效地降低集群管理成本，提高整体集群的资源利用率</p><a href=#认证-实现多租户的基础><h2 id=认证-实现多租户的基础><span class=hanchor arialabel=Anchor># </span>认证-实现多租户的基础</h2></a><p>租户管理首先需要识别访问的用户是谁，因此用户身份认证是多租户的基础</p><p>权限控制，如允许合法登录的用户访问、拒绝非法登录的用户访问或提供有限的匿名访问</p><p>Kubernetes可管理两类用户</p><p>用来标识和管理系统组件的ServiceAccount</p><p>外部用户的认证，需要通过Kubernetes的认证扩展来对接企业、供应商的认证服务，为用户验证、操作授权、资源隔离等提供基础</p><a href=#隔离><h2 id=隔离><span class=hanchor arialabel=Anchor># </span>隔离</h2></a><p>除认证、授权这些基础条件外，还要能够保证用户的工作负载彼此之间有尽可能安全的隔离，减少用户工作负载之间的影响。通常从权限、网络、数据三个方面对不同用户进行隔离</p><p>权限隔离</p><p>普通用户的容器默认不具有priviledged、sys admin、net admin等高级管理权限，以阻止对宿主机及其他用户的容器进行读取、写入等操作。</p><p>网络隔离</p><p>不同的Pod，运行在不同的Network Namespace中，拥有独立的网络协议栈。Pod之间只能通过容器开放的端口进行通信，不能通过其他方式进行访问。</p><p>数据隔离</p><p>容器之间利用Namespace进行隔离，在第2章中我们已经对不同的Namespace进行了详细描述。不同Pod的容器，运行在不同的MNT、UTS、PID、IPC Namespace上，相互之间无法访问对方的文件系统、进程、IPC等信息;同一个Pod的容器，其mnt、PID Namespace也不共享。</p><a href=#租户隔离手段><h2 id=租户隔离手段><span class=hanchor arialabel=Anchor># </span>租户隔离手段</h2></a><p>Namespace：Namespace属于且仅属于一个租户。权限定义∶定义内容包括命名空间中的Role与RoleBinding。这些资源表示目前租户在归属于自己的命名空间中定义了什么权限、授权给了哪些租户的成员。</p><p>Pod安全策略：特殊权限指集群级的特定资源定义——PodSecurityPolicy。它定义了一系列工作负载与基础设施之间、工作负载与工作负载之间的关联关系，并通过命名空间的RoleBinding完成授权。</p><p>网络策略：基础设施层面为保障租户网络的隔离机制提供了一系列默认策略，以及租户自己定制的用于租户应用彼此访问的策略。</p><p>Pod、Service、PersistentVolumeClaim等命名空间资源：这些定义表示租户的应用落地到Kubernetes中的实体。</p><p><img src=https://obsidian.codeplayer.org//Assets/image_1667048020921_0.png width=auto alt=image.png></p><a href=#权限隔离><h2 id=权限隔离><span class=hanchor arialabel=Anchor># </span>权限隔离</h2></a><ul><li>基于Namespace的权限隔离<ul><li>创建一个namespace-admin ClusterRole，拥有所有对象的所有权限</li><li>为用户开辟新namespace，并在该namespace创建rolebinding绑定namespace-admin ClusterRole，用户即可拥有当前namespace所有对象操作权限</li></ul></li><li>自动化解决方案<ul><li>当Namespace创建时，通过mutatingwebhook将namespace变形，将用户信息记录至namespace annotation</li><li>创建一个控制器，监控namespace，创建rolebinding为该用户绑定namespace-admin 的权限</li></ul></li></ul><p><img src=https://obsidian.codeplayer.org//Assets/image_1667048158659_0.png width=auto alt=image.png></p><a href=#quota管理><h2 id=quota管理><span class=hanchor arialabel=Anchor># </span>Quota管理</h2></a><p>开启ResourceQuota准入插件。</p><p>在用户namespace创建ResourceQuota对象进行限额配置。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ResourceQuota</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>high-qos-limit-requests</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>Spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>hard</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>limits.cpu</span><span class=p>:</span><span class=w> </span><span class=m>8</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>limits.memory</span><span class=p>:</span><span class=w> </span><span class=l>24Gi</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>pods</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>requests.cpu</span><span class=p>:</span><span class=w> </span><span class=m>4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>requests.memory</span><span class=p>:</span><span class=w> </span><span class=l>12Gi</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>scopes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>NotBestEffort</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p><img src=https://obsidian.codeplayer.org//Assets/image_1667048868059_0.png width=auto alt=image.png></p><a href=#节点资源隔离><h2 id=节点资源隔离><span class=hanchor arialabel=Anchor># </span>节点资源隔离</h2></a><p>通过为节点设置不同taint来识别不同租户的计算资源。</p><p>不同租户在创建Pod时，增加Toleration关键字，确保其能调度至某个taint的节点。</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script async src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://obsidian.codeplayer.org/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Patrick Zhao using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://obsidian.codeplayer.org/>Home</a></li><li><a href=https://github.com/PetrusZ>GitHub</a></li><li><a href=https://t.me/Petrus_Z>Telegram</a></li><li><a href=mailto:silencly07@gmail.com>Email</a></li></ul></footer></div><hr><script src=https://giscus.app/client.js data-repo=PetrusZ/ObsidianPublish data-repo-id=R_kgDOJTGL0A data-category=Announcements data-category-id=DIC_kwDOJTGL0M4CVohQ data-mapping=title data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=preferred_color_scheme data-lang=zh-CN crossorigin=anonymous async></script></div></body></html>