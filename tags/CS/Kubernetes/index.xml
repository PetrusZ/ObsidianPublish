<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>CS/Kubernetes on</title><link>https://obsidian.codeplayer.org/tags/CS/Kubernetes/</link><description>Recent content in CS/Kubernetes on</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><atom:link href="https://obsidian.codeplayer.org/tags/CS/Kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title>APIServer</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/APIServer/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/APIServer/</guid><description>API Server kube-apiserver是Kubernetes最重要的核心组件之一，主要提供以下的功能
提供集群管理的REST API接口，包括认证授权、数据校验以及集群状态变更等
提供其他模块之间的数据交互和通信的枢纽（其他模块通过API Server查询或修改数据，只有API Server才直接操作etcd）
访问控制概览 Kubernetes API的每个请求都会经过多阶段的访问控制之后才会被接受，这包括认证、授权以及准入控制（Admission Control）等。
访问控制细节 认证 开启TLS时，所有的请求都需要首先认证。Kubernetes支持多种认证机制，并支持同时开启多个认证插件（只要有一个认证通过即可）。如果认证成功，则用户的username会传入授权模块做进一步授权验证；而对于认证失败的请求则返回HTTP 401。
认证插件 X509证书 使用X509客户端证书只需要API Server启动时配置&amp;ndash;client-ca-file=SOMEFILE。在证书认证时，其CN域用作用户名，而组织机构域则用作group名。 静态Token文件 使用静态Token文件认证只需要API Server启动时配置&amp;ndash;token-auth-file=SOMEFILE。 该文件为csv格式，每行至少包括三列token,username,user id， token,user,uid,&amp;quot;group1,group2,group3” 引导Token 为了支持平滑地启动引导新的集群，Kubernetes 包含了一种动态管理的持有者令牌类型， 称作 启动引导令牌（BootstrapToken）。 这些令牌以 Secret 的形式保存在 kube-system 名字空间中，可以被动态管理和创建。 控制器管理器包含的 TokenCleaner 控制器能够在启动引导令牌过期时将其删除。 在使用kubeadm部署Kubernetes时，可通过kubeadm token list命令查询。 静态密码文件 需要API Server启动时配置&amp;ndash;basic-auth-file=SOMEFILE，文件格式为csv，每行至少三列password, user, uid，后面是可选的group名 password,user,uid,&amp;quot;group1,group2,group3” ServiceAccount ServiceAccount是Kubernetes自动生成的，并会自动挂载到容器的/run/secrets/kubernetes.</description></item><item><title>CNI</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/CNI/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/CNI/</guid><description>CNI Kubernetes 网络模型设计的基础原则是：
所有的 Pod 能够不通过 NAT就能相互访问。 所有的节点能够不通过NAT就能相互访问。 容器内看见的 IP地址和外部组件看到的容器 IP是一样的。 Kubernetes 的集群里，IP地址是以 Pod为单位进行分配的，每个 Pod都拥有一个独立的 IP地址。一个Pod内部的所有容器共享一个网络栈，即宿主机上的一个网络命名空间，包括它们的IP 地址、网络设备、配置等都是共享的。也就是说，Pod 里面的所有容器能通过localhost:port来连接对方。在Kubernetes中，提供了一个轻量的通用容器网络接口 CNI（Container Network Interface），专门用于设置和删除容器的网络连通性。容器运行时通过 CNI调用网络插件来完成容器的网络设置。</description></item><item><title>Controller Manager</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/Controller-Manager/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/Controller-Manager/</guid><description>控制器的工作流程 Informer 的内部机制 控制器的协同工作原理 通用 Controller Job Controller：处理job。
Pod AutoScaler：处理 pod 的自动缩容/扩容。
RelicaSet：依据 Replicaset Spec 创建 Pod。</description></item><item><title>CRI</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/CRI/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/CRI/</guid><description>CRI 容器运行时（Container Runtime），运行于Kubernetes（k8s）集群的每个节点中，负责容器的整个生命周期。其中 Docker 是目前应用最广的。随着容器云的发展，越来越多的容器运行时涌现。为了解决这些容器运行时和 Kubernetes 的集成问题，在 Kubernetes1.5版本中，社区推出了CRI（Container Runtime Interface，容器运行时接口）以支持更多的容器运行时。
CRI是 Kubernetes定义的一组 gRPC服务。kubelet作为客户端，基于gRPC框架，通过 Socket 和容器运行时通信。 它包括两类服务∶镜像服务（Image Service）和运行时服务（Runtime Service）。 镜像服务提供下载、检查和删除镜像的远程程序调用。 运行时服务 包含用于管理容器生命周期，以及与容器交互的调用（exeC/ attach/ port-forward）的远程程序调用。</description></item><item><title>CSI</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/CSI/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/CSI/</guid><description>容器运行时存储 除外挂存储卷外，容器启动后，运行时所需文件系统性能直接影响容器性能；
早期的 Docker 采用 Device Mapper 作为容器运行时存储驱动，因为 OverlayFS尚未合并进Kernel；
目前 Docker 和 containerd都默认以OverlayFS 作为运行时存储驱动；
OverlayFS目前已经有非常好的性能，与DeviceMapper 相比优 20%，与操作主机文件性能几乎一致
存储卷插件管理 Kubernetes支持以插件的形式来实现对不同存储的支持和扩展，这些扩展基于如下三种方式：
out-of-tree CSI插件 CSI 通过 RPC与存储驱动进行交互。</description></item><item><title>etcd</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/etcd/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/etcd/</guid><description>etcd Etcd是CoreOS基于Raft开发的分布式key-value存储，可用于服务发现、共享配置以及一致性保障（如数据库选主、分布式锁等）。
在分布式系统中，如何管理节点间的状态一直是一个难题，etcd像是专门为集群环境的服务发现和注册而设计，它提供了数据TTL失效、数据改变监视、多值、目录监听、分布式锁原子操作等功能，可以方便的跟踪并管理集群节点的状态。
键值对存储：将数据存储在分层组织的目录中，如同在标准文件系统中 监测变更：监测特定的键或目录以进行更改，并对值的更改做出反应 简单: curl可访问的用户的API（HTTP+JSON） 安全: 可选的SSL客户端证书认证 快速: 单实例每秒1000次写操作，2000+次读操作 可靠: 使用Raft算法保证一致性 主要功能 基本的key-value存储 监听机制 key的过期及续约机制，用于监控和服务发现 原子Compare And Swap和Compare And Delete，用于分布式锁和leader选举 使用场景 也可以用于键值对存储，应用程序可以读取和写入 etcd 中的数据 etcd 比较多的应用场景是用于服务注册与发现 基于监听机制的分布式异步系统 键值对存储 etcd 是一个键值存储的组件，其他的应用都是基于其键值存储的功能展开。</description></item><item><title>Headless</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/Headless/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/Headless/</guid><description>Headless和ClusterIP的区别 CoreDNS的作用：在K8S里，我们想要通过name来访问服务的方式就是在Deployment上面添加一层Service，这样我们就可以通过Service name来访问服务了，那其中的原理就是和CoreDNS有关，它将Service name解析成Cluster IP。
这样我们访问Cluster IP的时候就通过Cluster IP作负载均衡，把流量分布到各个POD上面。
K8s中资源的全局FQDN格式:
1 2 Service_NAME.NameSpace_NAME.Domain.LTD. Domain.LTD.=svc.cluster.local.　#这是默认k8s集群的域名。 ClusterIP ClusterIP的原理：一个Service可能对应多个EndPoint(Pod)，client访问的是Cluster IP，通过iptables规则转到Real Server，从而达到负载均衡的效果。</description></item><item><title>istio</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/istio/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/istio/</guid><description>问题 No tracing问题 没有正确设置meshConfig.defaultConfig.tracing.zipkin.address和meshConfig.defaultConfig.tracing.sampling istioctl install --set profile=demo --set meshConfig.defaultConfig.tracing.zipkin.address=jaeger-collector.istio-system:9411 --set meshConfig.defaultConfig.tracing.sampling=100 -y 重建、更新istio后，可能需要把sidecar重启/重建 jaeger暂时使用allinone即可，因为用es做后端存储太难配置 kiali的Graph为空或prometheus没有数据 prometheus.</description></item><item><title>K0S</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/K0S/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/K0S/</guid><description>安装K0S https://docs.k0sproject.io/v1.23.6+k0s.2/install/
curl -sSLf [https://get.k0s.sh](https://get.k0s.sh) | sudo sh sudo k0s install controller --single sudo k0s start sudo k0s status sudo k0s kubectl get nodes 卸载K0S sudo k0s stop sudo k0s reset reboot system 💡 A few small k0s fragments persist even after the reset (for example, iptables).</description></item><item><title>kube-proxy</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/kube-proxy/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/kube-proxy/</guid><description>kube-proxy 每台机器上都运行一个kube-proxy服务，它监听API server中service和endpoint的变化情况，并通过iptables等来为服务配置负载均衡（仅支持TCP和UDP）。
kube-proxy可以直接运行在物理机上，也可以以static pod或者DaemonSet的方式运行。
kube-proxy当前支持一下几种实现
userspace：最早的负载均衡方案，它在用户空间监听一个端口，所有服务通过iptables转发到这个端口，然后在其内部负载均衡到实际的Pod。该方式最主要的问题是效率低，有明显的性能瓶颈。 iptables：目前推荐的方案，完全以iptables规则的方式来实现service负载均衡。该方式最主要的问题是在服务多的时候产生太多的iptables规则，非增量式更新会引入一定的时延，大规模情况下有明显的性能问题 ipvs：为解决iptables模式的性能问题，v1.8新增了ipvs模式，采用增量式更新，并可以保证service更新期间连接保持不断开 winuserspace∶ 同userspace，但仅工作在windows上 Linux内核处理数据包：Netfilter框架 Netfilter和iptables iptables iptables支持的锚点 kube-proxy工作原理 Kubernetes iptables规则 IPVS IPVS支持的锚点和核心函数 域名服务 Kubernetes Service通过虚拟IP地址或者节点端口为用户应用提供访问入口</description></item><item><title>Kubelet</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/Kubelet/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/Kubelet/</guid><description>kubelet 架构 kubelet 管理 Pod 的核心流程 kubelet 每个节点上都运行一个kubelet 服务进程，默认监听 10250端口。
接收并执行 master 发来的指令； 管理 Pod及Pod 中的容器； 每个kubelet 进程会在API Server 上注册节点自身信息，定期向 master 节点汇报节点的资源使用情况，并通过 cAdvisor监控节点和容器的资源。 节点管理 节点管理主要是节点自注册和节点状态更新：</description></item><item><title>Service</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/Service/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/Service/</guid><description>Service对象 Service Selector Kubernetes允许将Pod对象通过标签（Label）进行标记，并通过Service Selector定义基于Pod标签的过滤规则，以便选择服务的上游应用实例 Ports Ports属性中定义了服务的端口、协议目标端口等信息 1 2 3 4 5 6 7 8 9 10 11 apiVersion:v1kind:Servicemetadata:name:nginx-serviceSpec:selector:app:nginxports:- protocol:TCPport:80targetPort:80 Endpoint对象 当Service的selector不为空时，Kubernetes EndpointController会侦听服务创建事件，创建与Service同名的Endpoint对象 selector能够选取的所有PodIP都会被配置到addresses属性中 如果此时selector所对应的filter查询不到对应的Pod，则addresses列表为空 默认配置下，如果此时对应的Pod为not ready状态，则对应的PodIP只会出现在subsets的notReadyAddresses属性中，这意味着对应的Pod还没准备好提供服务，不能作为流量转发的目标。 如果设置了PublishNotReadyAdddress为true，则无论Pod是否就绪都会被加入readyAddress list 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion:v1kind:Endpointmetadata:name:nginx-servicesubsets:- addresses:- ip:10.</description></item><item><title>基于Istio的高级流量管理</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/%E5%9F%BA%E4%BA%8EIstio%E7%9A%84%E9%AB%98%E7%BA%A7%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/%E5%9F%BA%E4%BA%8EIstio%E7%9A%84%E9%AB%98%E7%BA%A7%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86/</guid><description>微服务架构的演变 Evolution 从单块系统到微服务系统的演进 微服务架构的演进 典型的微服务业务场景 更完整的微服务架构 系统边界 微服务到服务网格还缺什么? Sidecar 的工作原理 Service Mesh 适应性</description></item><item><title>基于Kubernetes和Istio的安全保证</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/%E5%9F%BA%E4%BA%8EKubernetes%E5%92%8CIstio%E7%9A%84%E5%AE%89%E5%85%A8%E4%BF%9D%E8%AF%81/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/%E5%9F%BA%E4%BA%8EKubernetes%E5%92%8CIstio%E7%9A%84%E5%AE%89%E5%85%A8%E4%BF%9D%E8%AF%81/</guid><description>云原生语境下的安全保证 云原生语境下的安全保证 安全保证是贯穿软件整个生命周期的重要部分。
安全与效率有时候是相违背的。
如何将二者统一起来，提升整体效率是关键。
这需要我们将安全思想贯穿到软件开发运维的所有环节。
云原生层次模型 软件的生命周期∶开发-&amp;gt;分发→部署-&amp;gt;运行
开发环节的安全保证 SaaS 应用的 [[系统架构#12 factors|12-factor]] 设计原则的一些理念与云原生安全不谋而合。
传统的安全三元素 CIA（ConfidentialitVIntegrity和 AVailability），在云原生安全中被充分应用，如对工作负载的完整性保护，与I（Integrity）完整性保护相对应。
机密性（Confidentiality）指只有授权用户可以获取信息。 完整性（Integrity）指信息在输入和传输的过程中，不被非法授权修改和破坏，保证数据的一致性。 可用性（Availability）指保证合法用户对信息和资源的使用不会被不正当地拒绝。 **基础设施即代码（Infrastructure as Code，简称 laC）**也与云原生的实践紧密相关。</description></item><item><title>将应用迁移至Kubernetes平台</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/%E5%B0%86%E5%BA%94%E7%94%A8%E8%BF%81%E7%A7%BB%E8%87%B3Kubernetes%E5%B9%B3%E5%8F%B0/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/%E5%B0%86%E5%BA%94%E7%94%A8%E8%BF%81%E7%A7%BB%E8%87%B3Kubernetes%E5%B9%B3%E5%8F%B0/</guid><description>应用接入最佳实践 应用容器化 目标 稳定性、可用性、性能、安全
从多维度思考高可用的问题
应用容器化的思考 容器额外开销和风险 Log driver Blocking mode Non blocking mode 共用 kernel 所以 系统参数配置共享 进程数共享-Fork bomb fd 数共享 主机磁盘共享 容器化应用的资源监控 容器中看到的资源是主机资源</description></item><item><title>常用命令</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</guid><description>Kubectl Kubectl 命令和 kubeconfig kubectl 是一个 Kubernetes 的命令行工具，它允许Kubernetes 用户以命令行的方式与 Kubernetes 交互，其默认读取配置文件 ~/.kube/config。 kubectl 会将接收到的用户请求转化为 rest 调用以rest client 的形式与 apiserver 通讯。 apiserver 的地址，用户信息等配置在 kubeconfig。 常用命令 kubectl get po –oyaml -w kubectl 可查看对象。 -oyaml 输出详细信息为 yaml 格式。 -w watch 该对象的后续变化。 -owide 以详细列表的格式查看对象。 在终端通过stdin读取inline YAML 1 2 3 4 $ kubectl apply -f - &amp;lt;&amp;lt;EOF &amp;lt;-- insert YAML content here --&amp;gt; EOF OR</description></item><item><title>服务发现</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/</guid><description>服务发布 需要把服务发布至集群内部或者外部，服务的不同类型 ClusterlP(Headless) NodePort LoadBalancer ExternalName 证书管理和七层负载均衡的需求 需要gRPC负载均衡如何做? DNS需求 与上下游服务的关系 服务发布的挑战 kube-dns DNS TTL问题 Service ClusterIP只能对内 kube-proxy支持的iptables/ipvs规模有限 IPVS的性能和生产化问题 kube-proxy的drift问题 频繁的Pod变动（spec change，failover，crashloop）导致LB频繁变更 对外发布的Service需要与企业ELB即成 不支持gRPC 不支持自定义DNS和高级路由功能 Ingress Spec的成熟度?</description></item><item><title>架构基础</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/</guid><description>主要组件 Kubernetes 的主节点（Master Node） API服务器：API Server 这是 Kubernetes 控制面板中唯一带有用户可访问 API 以及用户可交互的组件。API 服务器会暴露一个 RESTful 的 Kubernetes API 并使用 JSON 格式的清单文件（manifest files）。 群的数据存储：Cluster Data Store Kubernetes 使用“etcd”。这是一个强大的、稳定的、高可用的键值存储，被Kubernetes 用于长久储存所有的 API 对象。 控制管理器：Controller Manager 被称为“kube-controller manager”，它运行着所有处理集群日常任务的控制器。包括了节点控制器、副本控制器、端点（endpoint）控制器以及服务账户等。 调度器：Scheduler 调度器会监控新建的 pods（一组或一个容器）并将其分配给节点。 Kubernetes 的工作节点（Worker Node） Kubelet 负责调度到对应节点的 Pod 的生命周期管理，执行任务并将 Pod 状态报告给主节点的渠道，通过容器运行时（拉取镜像、启动和停止容器等）来运行这些容器。它还会定期执行被请求的容器的健康探测程序。 Kube-proxy 它负责节点的网络，在主机上维护网络规则并执行连接转发。它还负责对正在服务的 pods 进行负载平衡。 etcd etcd 是 CoreOS 基于 Raft 开发的分布式 key-value 存储，可用于服务发现、共享配置以及一致性保障（如数据库选主、分布式锁等）。</description></item><item><title>深入理解</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/</guid><description>API 设计原则 所有 API 都应是声明式的
相对于命令式操作，声明式操作对于重复操作的效果是稳定的，这对于容易出现数据丢失或重复的分布式环境来说是很重要的。
声明式操作更易被用户使用，可以使系统向用户隐藏实现的细节，同时也保留了系统未来持续优化的可能性。
此外，声明式的 API 还隐含了所有的 API 对象都是名词性质的，例如 Service、Volume 这些 API 都是名词，这些名词描述了用户所
期望得到的一个目标对象。
API 对象是彼此互补而且可组合的
这实际上鼓励 API 对象尽量实现面向对象设计时的要求，即“高内聚，松耦合”，对业务相关的概念有一个合适的分解，提高分解出来的对象的可重用性。</description></item><item><title>深入理解Pod的生命周期</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Pod%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Pod%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/</guid><description>如何优雅的管理Pod的完整生命周期 Pod状态机 Pod Phase Pod Phase Pending Running Succeeded Failed Unknown kubectl get pod显示的状态信息是由podstatus 的conditions和phase计算出来的 查看pod细节 kubectl get pod $podname-oyaml 查看pod相关事件 kubectl describe pod Pod状态计算细节 如何确保Pod的高可用 避免容器进程被终止避免Pod被驱逐</description></item><item><title>生产化运维</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/%E7%94%9F%E4%BA%A7%E5%8C%96%E8%BF%90%E7%BB%B4/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/%E7%94%9F%E4%BA%A7%E5%8C%96%E8%BF%90%E7%BB%B4/</guid><description>镜像仓库 镜像仓库 镜像仓库（Docker Registry）负责存储、管理和分发镜像。 镜像仓库管理多个Repository，Repository通过命名来区分。每个Repository包含一个或多个镜像，镜像通过镜像名称和标签（Tag）来区分。 客户端拉取镜像时，要指定三要素： 镜像仓库：要从哪一个镜像仓库拉取镜像，通常通过DNS或IP地址来确定一个镜像仓库，如hub.docker.com Repository：组织名，如cncamp 镜像名称+标签：如nginx∶latest 公有镜像仓库优势
开放：任何开发者都可以上传、分享镜像到公有镜像仓库中。
便捷：开发者可以非常方便地搜索、拉取其他开发者的镜像，避免重复造轮子。
免运维：开发者只需要关注应用开发，不必关心镜像仓库的更新、升级、维护等。</description></item><item><title>生产化集群的管理</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%9A%84%E7%AE%A1%E7%90%86/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%9A%84%E7%AE%A1%E7%90%86/</guid><description>计算节点 生产化集群的考量 计算节点： 如何批量安装和升级计算节点的操作系统? 如何管理配置计算节点的网络信息? 如何管理不同SKU（StockKeeping Unit）的计算节点? 如何快速下架故障的计算节点? 如何快速扩缩集群的规模? 控制平面： 如何在主节点上下载、安装和升级控制平面组件及其所需的配置文件? 如何确保集群所需的其他插件，例如CoreDNS、监控系统等部署完成? 如何准备控制平面组件的各种安全证书? 如何快速升级或回滚控制平面组件的版本? 操作系统的选择 操作系统的评估与选择 通用操作系统 Ubuntu Centos Fedora 专为容器优化的操作系统 最小化操作系统 CoreOS RedHat Atomic Snappy Ubuntu Core RancherOS 操作系统的评估与选择 操作系统评估和选型的标准</description></item><item><title>调度</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/%E8%B0%83%E5%BA%A6/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/%E8%B0%83%E5%BA%A6/</guid><description>kube-scheduler kube-scheduler负责分配调度Pod 到集群内的节点上，它监听 kube-apiserver，查询还未分配
Node 的Pod，然后根据调度策略为这些 Pod 分配节点（更新 Pod 的 NodeName 字段）。
调度器需要充分考虑诸多的因素∶
公平调度； 资源高效利用； QoS； affinity和 anti-affinity； 数据本地化（data locality）； 内部负载干扰（inter-workload interference）； deadlines。 调度器 kube-scheduler 调度分为两个阶段，predicate 和 priority：</description></item><item><title>远程调试</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/%E8%BF%9C%E7%A8%8B%E8%B0%83%E8%AF%95/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/%E8%BF%9C%E7%A8%8B%E8%B0%83%E8%AF%95/</guid><description> 原理：https://learn.microsoft.com/zh-cn/visualstudio/bridge/overview-bridge-to-kubernetes
教程：https://learn.microsoft.com/zh-cn/visualstudio/bridge/bridge-to-kubernetes-vs-code
示例：https://learn.microsoft.com/zh-cn/visualstudio/bridge/bridge-to-kubernetes-sample
安装 安装vscode 安装vscode扩展Kubernetes和Bridge to Kubernetes 安装go扩展 配置 打开项目 选择kubernetes namespace 打开命令面板，运行命令“Bridge to Kubernetes: 配置” 选择需要重定向到本地的服务 输入本地运行的程序的端口号 选择或创建调试的启动配置（与本地开发的配置相同） 选择在隔离或非隔离模式下运行，隔离模式需要转发header kubernetes-route-as 在调试中选择刚才创建的配置启动 限制 要使 Bridge to Kubernetes 成功连接，一个 pod 只能有一个容器在该 pod 中运行。 目前，Bridge to Kubernetes pod 必须是 Linux 容器。 不支持 Windows 容器。 Bridge to Kubernetes 需要提升的权限才能在开发计算机上运行，以便编辑主机文件。 Bridge to Kubernetes 不能用于已启用 Azure Dev Spaces 的群集。</description></item><item><title>部署KubeSphere</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/%E9%83%A8%E7%BD%B2KubeSphere/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/%E9%83%A8%E7%BD%B2KubeSphere/</guid><description>安装KubeSphere 直接在k8s中安装 1 2 3 kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.3.0/kubesphere-installer.yaml kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.3.0/cluster-configuration.yaml 检查安装日志 kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l 'app in (ks-install, ks-installer)' -o jsonpath='{.</description></item><item><title>集群联邦和Istio多集群管理</title><link>https://obsidian.codeplayer.org/CS/Kubernetes/%E9%9B%86%E7%BE%A4%E8%81%94%E9%82%A6%E5%92%8CIstio%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/</link><pubDate>Sun, 09 Apr 2023 14:51:49 +0800</pubDate><guid>https://obsidian.codeplayer.org/CS/Kubernetes/%E9%9B%86%E7%BE%A4%E8%81%94%E9%82%A6%E5%92%8CIstio%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/</guid><description>集群治理的驱动力 分布式云是未来 成本优化（Cost Effective） 更好的弹性及灵活性（Elasticity&amp;amp;Flexibility） 避免厂商锁定（Avoid Vendor Lock-in） 第一时间获取云上的新功能（Innovation） 容灾（Resilience &amp;amp;Recovery） 数据保护及风险管理（Data Protection&amp;amp;Risk Management） 提升响应速度（NetworkPerformance Improvements） 分布式云的挑战 Kubernetes单集群承载能力有限 异构的基础设施 存量资源接入 配置变更及下发 跨地域、跨机房应用部署及管理 容灾与隔离性，异地多活 弹性调度及自动伸缩 监控告警 如何应对 通过Kubernetes 屏蔽底层基础设施，提供统一的接入层 多云架构 多集群+多云 多集群管控 统一的管控面 方便接入，降低使用门槛 跨地域的集群管理 集群联邦 集群联邦的必要性 单一集群的管理规模有上限 数据库存储</description></item></channel></rss>