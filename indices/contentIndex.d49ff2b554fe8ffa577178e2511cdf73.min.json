{"/%E5%92%96%E5%95%A1/%E5%92%96%E5%95%A1%E7%90%86%E8%AE%BA":{"title":"咖啡理论","content":"\n# 意式萃取理论\n\n![CleanShot 2022-11-29 at 17.09.31-2x.png](Assets/CleanShot_2022-11-29_at_17.09.31-2x_1669712988560_0.png)\n\n![CleanShot 2022-11-29 at 16.43.54-2x.png](Assets/CleanShot_2022-11-29_at_16.43.54-2x_1669711447206_0.png)\n\nErgun公式：计算水通过粉饼的压降。压降deltaP就是水从开始接触粉饼的面，到从粉饼出来的这个面结束，这段过程中因为粉饼的阻力，造成的水的压强的损失。\n\nu0是水的流速。\n\n![CleanShot 2022-11-29 at 16.44.53-2x.png](Assets/CleanShot_2022-11-29_at_16.44.53-2x_1669711503019_0.png)\n\n![CleanShot 2022-11-29 at 16.46.27-2x.png](Assets/CleanShot_2022-11-29_at_16.46.27-2x_1669711597473_0.png)\n\n![CleanShot 2022-11-29 at 16.48.20-2x.png](Assets/CleanShot_2022-11-29_at_16.48.20-2x_1669711707926_0.png)\n\n![CleanShot 2022-11-29 at 16.50.46-2x.png](Assets/CleanShot_2022-11-29_at_16.50.46-2x_1669711858130_0.png)\n\n![CleanShot 2022-11-29 at 16.57.58-2x.png](Assets/CleanShot_2022-11-29_at_16.57.58-2x_1669712493583_0.png)\n\n![CleanShot 2022-11-29 at 17.05.01-2x.png](Assets/CleanShot_2022-11-29_at_17.05.01-2x_1669712715373_0.png)\n\n# 咖啡冲煮的最根本原则\u0026底层逻辑\n\n* 是否使用滤纸\n\t* 所有带滤纸的冲煮，获得的咖啡液中的物质构成种类是一样的\n\t\t* 味道不同的原因：物质的比例不同\n\t* 不带滤纸的冲煮，还会有悬浊分散系的分子\n* 溶解速率\n\t* 咖啡萃取没有“最先萃取出来酸味物质、然后甜、最后苦”的说法\n\t* 是不同风味物质的溶解速率不同，都是同时开始被萃取的\n\t\t* 酸最容易溶解、萃取速率最快，其次甜，最后苦\n\t* 同一个豆子冲煮的咖啡偏苦偏酸的原因\n\t\t* 酸味物质都差不多一样多\n\t\t* 偏酸的，甜味苦味物质更少，酸味更突出\n\t\t* 偏苦的，甜味苦味物质更多，酸味没那么突出\n* 【调节风味的基本原则】\n\t* 调节豆子里面风味特征更难萃取的那一部分\n\t* 例如浅烘豆子，调节甜味物质的萃取\n\t\t* 突出酸味：降低萃取率的方法\n\t\t* 突出甜味：提高萃取率的方法\n\t* 例如深烘豆子，调节苦味物质的萃取\n\t\t* 突出甜味：降低萃取率的方法\n\t\t* 突出苦味：提高萃取率的方法\n* 萃取时需要避免，严重的萃取不足和萃取过度\n\t* 通过分段来控制总体萃取率\n* 想要控制酸，只能通过改变水质的方法\n* https://www.bilibili.com/video/BV1Y34y1k769\n\n","lastmodified":"2023-05-01T11:46:37.90458941Z","tags":["咖啡"]},"/%E5%92%96%E5%95%A1/%E5%92%96%E5%95%A1%E8%B1%86":{"title":"咖啡豆","content":"\n# 摩卡壶\n\n啡舍克里奥片意式拼配：深度烘焙。227g 48.4元，0.21元/g。危地马拉、哥斯达黎加、洪都拉斯。建议配合牛奶。\n\n菠剪咖啡成子日式深烘：深度烘焙。66.63元 250克 0.26元/克。埃塞俄比亚、哥伦比亚、巴西。建议配合牛奶。\n\n治光师北野拼配：深度烘焙。72.6元 300克 0.24元/克。哥伦比亚、曼特宁、危地马拉和秘鲁、青尼亚、云南水洗。均衡。\n\nM2M Casanove 意式拼配：深度烘焙。85.9元 500克 0.17元/克。埃塞俄比亚、哥伦比亚。苦味突出，没有酸味。美式和奶咖都不错。\n\n罗马假日意式拼配：中深烘焙。61元 454克 0.13元/克。巴西、哥伦比亚、乌手达。适合做奶咖。\n\n液体芝士SOE：中深烘焙。57.14元227克 0.25元/克。埃塞俄比亚、古姬罕贝拉。口感醇厚顺滑，口感低，无酸。配合提纯奶。\n\nM2M热带公路意式拼配：中度烘焙。129元 500克 0.26元/克。埃塞俄比亚、哥伦比亚。甜感高，苦感低，味道丰富，酸味突出。美式、奶咖都适合。\n\n牛奶生巧意式拼配：中深烘焙。45.5元 250克 0.18元/克。云南、哥伦比亚、埃塞俄比亚。醇厚度很高，油脂丰富，巧克力香，顺滑。\n\n","lastmodified":"2023-05-01T11:46:37.90458941Z","tags":["咖啡"]},"/%E5%92%96%E5%95%A1/%E6%91%A9%E5%8D%A1%E5%A3%B6":{"title":"摩卡壶","content":"\n# 摩卡壶基础理论\n\n先是小火，然后出液后改最小火，最后离火或铺满底部后离火。\n\n加热可选导热板，可以在关火后继续缓缓释放热量。\n\n低烘需要更多的咖啡液，深烘可以少一些，深烘可以在过热之前，提早结束萃取。同时萃取的液体越多，萃取率也就越高。\n\n粉越细，萃取压力越高，并没有发生通道效应，同时粉越细，萃取温度越高。\n\n但是高温萃取出来的，虽然萃取率高，味道并不是特别好。\n\n底部加开水，放滤纸（用水稍微浸湿），小火加热直到液体流出，然后关闭火源（如果用导热板）或最小火，10秒后离开热源。\n\n120g咖啡液，加入80-100g水\n\n---\n\n摩卡壶主要要避免过热，过热会导致非常苦。\n\n深烘豆子不要太满，太满会造成更高的温度\n\n如果出液太少，在萃取足够的液体之前开始喷射，有可能是咖啡豆研磨的太细了；另外还有可能是壶变热的太快，在出液后尝试使用更小火。\n\n小壶在出液后基本可以直接离火。\n\n# 摩卡壶萃取配方\n\n摩卡新手近2个月的尝试\n\n单阀三杯份，18g粉，意式研磨度轻压，加个滤纸，90g水出50g不到，美式或者拿铁都可\n\n双阀两杯份，20g粉，其他参数一致，拿铁更好喝一些。\n\n---\n\n16g咖啡豆，C40 7格（比意式还细一点），热水60g，通常咖啡粉的吸水量是粉重的两倍，得到咖啡液31.6g。做美式咖啡液跟水的比例是1:6。\n\n单阀三人份+卡斯炉中火\n细研磨 C40 7格\n小粉水比\n16g咖啡粉\n60g热水 70度左右 30g液缩咖啡\n加水加奶 1：6 自行调整\n追求高品质，要进行挑豆\n\n---\n\n18g咖啡豆，C40 10格（标准意式），水90-100g，得到60-70g咖啡液。\n\n比乐蒂双阀双杯份+户外一体瓦斯炉\n粗研磨 C4O 10格 \n18g咖啡粉\n100g热水\n60g浓缩咖啡（保留前中段） \n加水加奶到300ml（自行调整）\n\n---\n\n比乐蒂双阀双杯份+电陶炉P3档\n中等研磨 C40 8格\n1：6粉水比\n17g咖啡粉\n102g热水\n60g左右浓缩咖啡\n根据自己喜好加水/奶\n\n---\n\n16g粉量，磨豆机刻度30，水量75g左右\n\n出液30g左右，味道稍微有点酸，微甜，略苦","lastmodified":"2023-05-01T11:46:37.90458941Z","tags":["咖啡"]},"/%E5%92%96%E5%95%A1/%E6%B1%89%E5%8C%A0K6%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E":{"title":"汉匠K6使用说明","content":"\n# 刻度参考\n\n- 意式：0.5-1 圈\n- 摩卡壶：1-1.5 圈\n- 手冲、冷萃：1.5-2 圈\n- 法压：2.5 圈\n\n# 电钻\n\n电钻15扭矩以上，搭配 6.35mm 或14英制内六角套筒即可\n\n套筒搜索关键词：批头加长接杆电钻接头延长杆 6.35mm 磁性内六角1/4套筒自锁连接杆\n\n# 说明书\n\n![[Assets/4A2E0104-B3B0-4D1F-8EF2-F0FDD0DFE7C0_1_102_o.jpeg]]","lastmodified":"2023-05-01T11:46:37.90458941Z","tags":["咖啡","磨豆机"]},"/%E5%92%96%E5%95%A1/%E7%A3%A8%E8%B1%86%E6%9C%BA":{"title":"磨豆机","content":"\n# 磨豆机的影响因素\n\n颗粒形状，平刀片状，萃取率更高，溶解的一致性更好。鬼齿锥刀块状，萃取率相对低。需要根据形状调整参数\n\n粒径分布。分布越集中一致性更好，分布宽泛一点会有差异性味道。哪个更好取决于个人，没有绝对好坏\n\n细粉，两个作用，一是增加层次感丰富性，二是堵滤纸底增加萃取率。到底细粉好不好需要根据豆子等具体判断\n\n刀片直径，影响研磨速度，越大越快。发热家用可以基本忽略，商用发热越少越好\n\nek43是中国咖啡业的最大笑话，除非你是开咖啡店给别人磨豆子或者参赛要求0残粉，要不然那些钱买个同品质磨豆机还能剩很多","lastmodified":"2023-05-01T11:46:37.90458941Z","tags":["咖啡"]},"/%E5%92%96%E5%95%A1/%E8%83%B6%E5%9B%8A%E5%92%96%E5%95%A1":{"title":"胶囊咖啡","content":"\n* nespresso\n\t* 浓缩咖啡\n\t\t* volluto 沃鲁托\n\t* 灵感之源\n\t\t* arpeggio 阿佩奇欧\n\t* 大杯系列\n\t\t* Tokyo 东京唯沃托\n\t* 大师匠心\n\t\t* Ethiopia 埃塞俄比亚\n\t\t* Nicaragua 尼加拉瓜\n* illy\n\t* 中度烘焙\n\t* 深度烘焙\n* 明谦\n\t* 平衡70","lastmodified":"2023-05-01T11:46:37.90458941Z","tags":["咖啡"]},"/%E5%92%96%E5%95%A1/Understanding-Espresso":{"title":"Understanding Espresso","content":"\n# Dose (Episode #1)\n\n## Extraction theory\n\n从咖啡粉中提取可溶解物质\n\n![CleanShot 2022-12-31 at 22.49.46-2x.png](Assets/CleanShot_2022-12-31_at_22.49.46-2x_1672498192970_0.png)\n\n## How much work\n\n提取可溶解物质到杯子中是做功（work）\n\n为了提高萃取率必须更加努力做功\n\n用的咖啡粉越多，必须做的功也越多\n\n## The basket\n\n粉碗是最主要的决定粉量的因素\n\n粉碗标注大小的正负1g是应该使用的粉量大小\n\n如果使用的粉量过少，会在粉饼顶端和冲煮头之间产生一些空隙。这会造成粉饼凌乱、糊状，这样的粉饼无法告诉你任何关于萃取时发生了什么信息，同时这也非常难以清理。\n\n使用更多的粉量并没有任何更好的地方。传统中使用更多的粉量，是因为以前磨豆机没有定量功能，只能为了得到统一的粉量，而手动的填满粉碗。现在磨豆机有了定量功能，我们可以使用任何我们想要的粉量。\n\n## The coffee\n\n咖啡越是深烘，萃取时需要做的功越少，越容易得到可溶解物质。\n\n通常来说，更深烘的咖啡使用更多的粉量是可以的，而浅烘的咖啡需要更努力的做工萃取。使用大粉量需要做的功太多，需要不可能的做功量，总是会得到酸涩、不完全萃取的、薄弱的醇厚感。\n\n所以推荐浅烘使用小粉量。\n\n## What should we do\n\n在咖啡尝起来可以，但是还可以稍微调整一下的时候，应该稍稍增加0.5g粉量，而不是调整磨豆机刻度。使用的咖啡粉量越多，粉饼提供的阻力就越高，水越难通过粉饼，得到想要的粉水比需要的接触时间更长。这会使得萃取时间稍微延长，稍微多一点咖啡液，但是粉水比不变，这会使用可能最小的浪费，得到最美味的咖啡。相反，萃取时间太长时，也可以稍微减少粉量，使得流速更快。同时减少粉量，做的功也越少，这样即使流速稍微过快，萃取仍然很好。所以小调整是可以的，只要粉饼状态没问题，但如果差得很远，那就不要改变粉量，而且调整其他变量。\n\n## Caffeine\n\n粉量越多，喝到的咖啡因也就越多。\n\n每一次用的粉量越多，一天中制作咖啡次数就应该越少。通过减少粉量，可以增加一天中可以制作咖啡的次数，并且不会让你咖啡因过量。\n\n# Ratio (Episode #2)\n\n![CleanShot 2022-12-31 at 23.58.06-2x.png](Assets/CleanShot_2022-12-31_at_23.58.06-2x_1672502304195_0.png)\n\n水在咖啡制作中是一种溶剂，用来溶解固体物质，将咖啡粉中的好风味和其他的好物质，萃取出来以供我们最终能够品尝的。你是用的溶剂越多，你萃取得就越多，你穿过咖啡粉饼的水越多，你从中萃取出的固体物质就越多。这是一种很简单的方法，去增加或减少你的萃取量。\n\n有一个非常重要的注意事项，改变水量会同时改变两个输出量。当使用更多的液体，明显能增加萃取率，但同时也会降低咖啡的强度。\n\n用固定的比例作为开始，这是更加享受的浓缩咖啡风味比例，所以一般会使用1:2到1:2.2的比例区间。然后以这个固定和稳定的比例开始进行调整。\n\n从例如调节磨豆机开始尝试，达到想要的意式咖啡风味，而不是盲目混乱地尝试不同的比例。如果需要制作一杯浓缩，但不想改变磨豆机的设置，但想有轻微的调整，这是应用比例的最好机会，你会感到震惊，仅因为多了2-3毫升的咖啡液，就产生了这么多变化。\n\n如果需要多出多余2或3克的变化，那风味和口感的变化，离我想达到的意式咖啡风味会有很大变化，所以倾向于不要改变这么多，否则咖啡液会被稀释太多，所以倾向于改变粉量或磨豆机设置。但若想快速简单地做出调整，调整比例。\n\n# Brew Time (Episode #3)\n\n冲煮时间的定义是，从在咖啡机上按下开始键开始，到按下结束键结束，是整个冲煮过程的持续时间。建议的冲煮时间范围是25-30秒。\n\n还有一种定义是，从咖啡液开始出现，直到按下停止按钮。如果你开始调整预浸泡时间，无论是调短还是调长，在这种定义下冲煮时间是趋近和相似的。\n\n总结，对新手来说25-30秒的冲煮时间规则非常有帮助。但是，如果使用杠杆机，或有压力和流量剖析的机器，你可以破坏这个规则。\n\n# Grind Size (Episode #4)\n\n使用正确的研磨度来获得美味的咖啡，修复配方而不是改变。调整研磨度来获得一杯不错的咖啡，然后用配方中的其他变量来进行微调。\n\n尽可能的磨的更细，直到产生通道。产生通道的咖啡会比预期的更加薄弱、空洞一些，同时余味也会具有酸涩和苦涩的味道。\n\n在同一时间只改变一种跟流速有关的变量。如果要改变研磨度，那么就不要同时改变粉量。\n\n改变研磨度后，清理是有必要的。\n\n# Brew Temperature (Episode #5)\n\n一般来说温度越高，萃取率也就越高。温度是一个微调的变量。先调整粉量、比例、配方，然后再考虑调整温度。粉量和比例，比起温度，会对咖啡风味有更大的影响。\n\n一般来说，深烘85-90度，中烘88-92度，浅烘90-85度。\n\n# Pressure (Episode #6)\n\n预浸泡的定义，是从水进入粉碗的时刻开始，当机器达到冲煮压力巅峰结束。在这个阶段，致力于在高压的水通过咖啡粉之前，将所有的咖啡粉都均匀的浸泡。在理论上，长时间的预浸泡促进了萃取的均匀性，由此产生更美味的咖啡。\n\n9 bar是期望值，但用起来比较困难，可能会出现通道等。通过减少压力，可以让萃取变得简单一些。这是一个妥协，但是总体来说会得到一杯更好的浓缩。\n\n","lastmodified":"2023-05-01T11:46:37.90458941Z","tags":["咖啡"]},"/Apple/Devices":{"title":"Devices","content":"\n1. iPhone 14 Pro\n3. AirPods Pro\n4. Macbook Pro 2021 M1 Pro\n5. [[iPad]] Air 5\n6. Apple Watch S7","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["Apple"]},"/Apple/MacOS":{"title":"MacOS","content":"\n* 解除下载程序的限制\n\t* 进入 `/Applications` 目录，执行 `xattr -r -c \u003capp name\u003e`\n* 查看路由\n\t* `netstat -rn`\n* 查看某个网络接口由哪个程序创建\n\t* `ifconfig -v \u003cinterface\u003e | grep \"agent domain\"`\n* SSH到别的服务器后，中文乱码\n\t* 编辑 /etc/ssh_config，注释`SendEnv LANG LC_*`一行即可。\n* SSH自动unlink GPG sockets\n\t* 编辑 /etc/ssh_config，添加配置`StreamLocalBindUnlink yes`","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["Apple"]},"/Apple/iPad":{"title":"iPad","content":"\n* 型号\n\t* iPad Air 5\n* 购买日期\n\t* 2022-09-29\n* 无法锁屏问题\n\t* 因为通用控制\n* 学习工具\n\t* MarginNote 3\n\t* GoodNotes 5\n\t* Prodrafts\n\t* PDF Expert\n* 贴膜\n\t* 水凝膜\n* 笔尖\n\t* 洛小希2H","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["Apple"]},"/CS/%E5%AE%89%E5%85%A8/%E7%8E%A9%E8%BD%ACYubikey":{"title":"玩转YubiKey","content":"\n# 什么是YubiKey\n\n## 官方解释\n\n\u003e [!info] 官方解释\n\u003e The YubiKey is a form of 2 Factor Authentication (2FA) which works as an extra layer of security to your online accounts. With a YubiKey, you simply register it to your account, then when you log in, you must input your login credentials (username+password) and use your YubiKey (plug into USB-port or scan via NFC). Both login credentials and YubiKey are needed at login. This physical layer of protection prevents many account takeovers that can be done virtually.\n\u003e https://support.yubico.com/hc/en-us/articles/4404456942738-FAQ#what-is-a-yubikey-\n\nYubikey是2因子验证（2FA）的一种形式，可作为你的在线账户的一个额外的安全层。使用 YubiKey，你只需将其注册到你的帐户，然后当登录时，你必须输入你的登录凭据（用户名+密码）并使用你的 YubiKey（插入 USB 端口或通过 NFC 扫描）。登录凭证和Yubikey两者在登录时都会需要。这种物理保护层可以防止许多可以虚拟完成的帐户接管。\n\n## YubiKey支持的加密协议\n\n\u003e [!info] 官方文档\n\u003e https://support.yubico.com/hc/en-us/articles/4404456942738-FAQ#security-protocols-explained\n\n### OTP\n\nOne-time password，一次性密码。\n\nYubico OTP 是一种简单而强大的身份验证机制，开箱即用，受到 YubiKey 5 系列和 YubiKey FIPS 系列的支持。 Yubico OTP 可以用作双因素身份验证方案中的第二个因素，或者单独提供强大的单因素身份验证。\n\n### FIDO2\n\nFIDO2 是 FIDO U2F 的无密码进化。 FIDO2 的总体目标是提供一组扩展功能以涵盖其他用例，主要驱动因素是无密码登录流程。 U2F 模型仍然是 FIDO2 的基础，FIDO2 规范中提供了现有 U2F 部署的兼容性。\n\n### FIDO2 U2F\n\nFIDO2 Universal 2nd Factor\n\nU2F由Yubico和谷歌共同开发，在为谷歌员工成功部署后贡献给了FIDO联盟。该协议旨在作为第二个因素来加强现有的基于用户名/密码的登录流程。它建立在 Yubico 发明的可扩展公钥模型的基础上，其中为每个服务生成一个新的密钥对，并且可以支持无限数量的服务，同时保持它们之间的完全分离以保护隐私。\n\n### OpenPGP\n\nOpenPGP 是一个用于签名和加密的开放标准。它通过像 PKCS#11 这样的接口，使用存储在智能卡上的私钥来启用 RSA/ECC 密钥，支持加密、签名、认证等功能。\n\n### WebAuthn\n\nWebAuthn 是一项新的 W3C 全球标准，用于在所有领先的浏览器和平台支持的 Web 上进行安全身份验证。 WebAuthn 使用户可以轻松地选择身份验证器来保护他们的帐户，包括外部/便携式身份验证器（例如硬件安全密钥）和内置平台身份验证器（例如生物识别传感器）\n\n### OATH\n\nOATH 是一个组织，它指定了两个开放的认证标准：TOTP（Time-Based One-Time Password） 和 HOTP（HMAC-based One-Time Password）。要使用 TOTP 进行身份验证，用户输入一个 6-8 位代码，该代码每 30 秒更改一次。该代码是使用 HMAC(sharedSecret, timestamp) 生成的，其中时间戳每 30 秒更改一次。shared secret通常以二维码形式提供或预编程到硬件安全密钥中。\n\n### PIV\n\nYubikey 支持个人身份验证 (PIV 和 FIPS 201) 智能卡接口 (NIST SP 800-73)。根据智能卡上存储的私钥，通过 PKCS#11 一类的通用接口进行RSA 或者 ESS 的签名、加密、解密操作。\n\n## Yubikey的账户限制\n\n\u003e [!info] 官方文档\n\u003e https://support.yubico.com/hc/en-us/articles/4404456942738-FAQ#what-is-the-yubikey-s-account-limit-\n\n### OTP\n\n此应用程序可以持有两个凭据。然而，Yubico OTP 是放置在这个应用程序中最流行的凭证类型之一，可以注册无限数量的服务。\n\n### FIDO U2F\n\n与 Yubico OTP 类似，U2F 应用程序可以注册无限数量的服务\n\n### FIDO2\n\nYubiKey 5 在其 FIDO2 应用程序中最多可容纳 25 个常驻密钥。\n\n### PIV\n\nYubiKey 5的PIV（智能卡）应用程序有24个插槽，每个插槽可以容纳一个证书及其相应的私钥\n\n### OpenPGP\n\nYubiKey 5 的 OpenPGP 应用程序最多可以保存三个 OpenPGP 私钥，一个用于加密，一个用于签名，一个用于身份验证。\n\n# 使用说明\n\n## Yubikey 的使用方式\n\n-   第三方软件请求 Yubikey 并需要触碰（例如 FIDO U2F）\n-   将两个 Slot 作为模拟键盘输入（例如静态密码）\n-   通过 Yubico 指定的软件使用 NFC 或接口读取 Yubikey 内其他的数据（例如 Yubico Authenticator）\n\n## Yubikey 的金属片触碰方式\n\n-   轻触 1 下 在 Yubikey 亮灯时， 允许当前对于 Yubikey 的请求\n-   轻触 1 秒 激活 Slot 1 并输入该插槽的默认输出结果\n-   长触 3 秒 激活 Slot 2 并输入该插槽的默认输出结果\n\n## Yubikey 的灯光含义\n\n-   短闪烁： 有应用对 Yubikey 发送请求\n-   长亮： 作为模拟键盘正在向当前窗口输入插槽\n\n## 使用场景\n\n### 操作系统登录\n\nWindows、Linux和MacOS系统的登录验证。\n\n### 网页登录\n\n常见的支持网站有：\n\n- Apple\n- Bitwarden\n- Cloudflare\n- Dropbox\n- Facebook\n- Google\n- Github\n- GItlab\n- Twitter\n- Microsoft\n\n等等。\n\n### GPG密钥\n\n将GPG密钥存储到YubiKey里，更加安全并且便携。\n\n### 支持的密码管理器\n\n1Password，Keeper®，LastPass Premium，Bitwarden Premium。\n\n### Yubico Authenticator\n\nYubico Authenticator 是跨移动设备和桌面端的身份验证器应用。  \n与其他身份验证器应用程序（包括 Google Authenticator、Microsoft Authenticator ）兼容。\n\nGoogle Authenticator 身份验证应用程序只允许将凭证存储在本地设备上，避免了因为网络原因导致的凭证泄露，但是当设备无法使用时无法找回已经保存的凭证。\n\nMicrosoft Authenticator 身份验证应用程序允许用户将凭证存储保存到 OneDrive 上，并且允许多设备登录。\n\nYubico Authenticator 身份验证应用程序允许将唯一凭据存储在硬件支持的安全密钥上，无论您从移动设备到桌面设备，都可以随身携带。无需再在手机上存储敏感机密，避免您的帐户被他人接管。\n\nYubico Authenticator 应用程序创建凭据存储后将凭证存储在 YubiKey 上而不是手机上，这样即使手机无法使用仍然可以使用另一台设备查看凭证。\n\n## Yubikey 的备份\n\nYubico 官方建议拥有多个 YubiKey。这样一个密钥可以用作主密钥，另一个密钥可以用作备用密钥。拥有备用钥匙的重要性已得到充分证实，如果你丢失了主钥匙，在最需要的时候，你也不会失去访问重要帐户的权限。换句话说，有了备用钥匙，你就不必担心被锁在任何账户之外，也不需要经历漫长的恢复和身份验证过程来重新获得每个账户的访问权限。\n\n# 配置\n\nYubikey的具体配置可以查看这篇文章：[[CS/安全/YubiKey配置]]\n\n# 参考资料\n\n1. [Yubikey 工作原理](https://www.mengorg.cn/archives/yubikey-work)\n2. [YubiKey 5 NFC 安全密匙简介与使用](https://www.mengorg.cn/archives/yubikey-5-nfc)\n3. [Yubikey 使用手册](https://www.bookstack.cn/read/yubikey-handbook-chinese/README.md)\n4. [Yubikey使用一年总结](https://blog.blahgeek.com/yubikey-review/)\n5. [YubiKey 的若干种玩法](https://blog.dl444.net/2021/06/05/YubiKey/)","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["CS/安全","YubiKey"]},"/CS/%E5%AE%89%E5%85%A8/YubiKey%E9%85%8D%E7%BD%AE":{"title":"YubiKey配置","content":"\n# 初始化设置\n\n\u003e https://xiaoqiang.blog/yubikey-shiyong/\n\n设置PIV功能的PIN和PUK\n\n设置FIDO2功能的PIN\n\n设置OTP功能\n\n# 操作系统登录\n\n## Mac登陆配置\n\n\u003e https://support.yubico.com/hc/en-us/articles/360016649059-Using-Your-YubiKey-as-a-Smart-Card-in-macOS\n\n注意需要在Yubikey Manger的Applications \u003e PIV里，点击setup for macOS\n\n## Enable smartcard only on macOS\n\n开启card only验证后，只有最近使用过验证的yubikey能解锁macOS的启动过程中的登陆\n\n\u003e https://support.apple.com/zh-cn/guide/deployment/depfce8de48b/1/web/1.0\n\n### 使用基于机器的执行方案的仅智能卡认证\n\n`sudo defaults write /Library/Preferences/com.apple.security.smartcard enforceSmartCard -bool true`\n\n## Linux登陆配置\n\n\u003e https://support.yubico.com/hc/en-us/articles/360016649099-Ubuntu-Linux-Login-Guide-U2F\n\n## Windows登录配置\n\n\u003e https://support.yubico.com/hc/en-us/articles/360013708460-Yubico-Login-for-Windows-Configuration-Guide\n\n# GPG配置\n\n\u003e https://github.com/drduh/YubiKey-Guide\n\n## Configure Smartcard\n\nEnable KDF\n\nChange PIN\n\nSet information\n\n## Transfer keys\n\n# OTP\n\nOTP 功能开箱即用，一次一密，需要联网认证，官方有提供认证服务器，当然也可以自己搭建认证服务器。\n\n-   OTP: `KEY_ID+AES(AES_KEY, SECRET, COUNT++)`即生成的密码包含明文的KEY_ID和对称加密的SECRET和计数器。第一次使用前需要把KEY_ID，AES_KEY，SECRET提交至验证服务器（Yubico提供或者自己搭建），之后应用程序每次通过服务器验证密码的可靠性（解码后SECRET对应、COUNT增大（防止重放攻击））。\n-   Static: 静态密码。顾名思义，每次生成固定的一串密码。\n-   Challenge-Response: `HMAC(SECRET, INPUT)`即可以通过HID接口给定一个输入，输入HMAC的计算结果。输入需要本地代码实现。\n-   HOTP: `HMAC(SECRET, COUNTER++)`算法与Challenge-Response类似，然而使用累加计数器代替了输入，并且HTOP是一个[标准协议](https://twofactorauth.org/)，许多网站和设备都兼容该标准。\n\n每个Yubikey都有两个slot，出厂时默认OTP配置在slot 1，短按操作即可触发OTP认证; slot 2可配置 `静态密码` `Challenge-Response` `HOTP` 中的一种，长按(2-5s)可触发。\n\n# U2F\n\nYubiKey中该功能开箱即用，无需复杂配置。\n\n只要根据网站提示开启二次验证(2FA)并绑定你的YubiKey, 下次登录账户时，你首先需要像往常一样输入用户名和密码，然后根据提示用你的YubiKey进行二次验证。这样即使你的帐号密码被别人知道了，只要key在你手里，那么你的帐号依然是安全的。\n\n## Apple\n\n## Bitwarden\n\n\u003e https://bitwarden.com/help/setup-two-step-login-yubikey/\n\n\u003e https://github.com/dani-garcia/vaultwarden/wiki/Enabling-Yubikey-OTP-authentication\n\n## Cloudflare\n\n## Google\n\n## Github\n\n## Microsoft\n\n# 变更opengpg email\n\n\u003e https://support.apple.com/zh-cn/guide/deployment/depfce8de48b/1/web/1.0\n\n载入主密钥的私钥\n\n`gpg --import /path/to/offline/master/key/MASTERKEY.priv.asc`\n\n编辑主密钥\n\n```bash\ngpg --edit-key MASTERKEY\n    adduid # Fill out form\n    uid n # where n is the new uid number\n    primary # To set the new e-mail as the primary e-mail address for the key\n    save\n```\n\n分别导出主密钥的公钥和私钥备份，并妥善保存\n\n```bash\ngpg -a --export MASTERKEY \u003e /path/to/offline/master/key/MASTERKEY.pub.asc\ngpg -a --export-secret-key MASTERKEY \u003e /path/to/offline/master/key/MASTERKEY.priv.asc\n```\n\n删除主密钥的私钥\n\n```bash\ngpg --delete-secret-key MASTERKEY\n```\n\n上传并导入公钥\n\n```bash\ngpg --send-key $KEYID\ngpg --keyserver pgp.mit.edu --send-key $KEYID\ngpg --keyserver keys.gnupg.net --send-key $KEYID\ngpg --keyserver hkps://keyserver.ubuntu.com:443 --send-key $KEYID\n```","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["CS/安全","YubiKey"]},"/CS/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93":{"title":"数据结构与算法总结","content":"\n# 数据结构与算法总结\n\n# 数据结构\n\n20个最常用的、最基础数据结构与算法：**数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie 树；10 个算法：递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法。**\n\n**我们学习数据结构和算法，要学习它的由来、特性、适用的场景以及它能解决的问题。**\n\n## 数组\n\n数组可以说是最基础、最简单的数据结构了。数组用一块连续的内存空间，来存储相同类型的一组数据，**最大的特点就是支持随机访问，但插入、删除操作也因此变得比较低效，平均情况时间复杂度为 O(n)**。在平时的业务开发中，我们可以直接使用编程语言提供的容器类，但是，如果是特别底层的开发，直接使用数组可能会更合适。\n\n## 链表\n\n链表跟数组一样，也是非常基础、非常常用的数据结构。不过链表要比数组稍微复杂，从普通的单链表衍生出来好几种链表结构，比如双向链表、循环链表、双向循环链表。\n\n和数组相比，链表更适合插入、删除操作频繁的场景，查询的时间复杂度较高。不过，在具体软件开发中，要对数组和链表的各种性能进行对比，综合来选择使用两者中的哪一个。\n\n写出正确链表代码的六个技巧。分别是理解指针或引用的含义、警惕指针丢失和内存泄漏、利用哨兵简化实现难度、重点留意边界条件处理，以及举例画图、辅助思考，还有多写多练。\n\n写链表代码是最考验逻辑思维能力的。因为，链表代码到处都是指针的操作、边界条件的处理，稍有不慎就容易产生 Bug。链表代码写得好坏，可以看出一个人写代码是否够细心，考虑问题是否全面，思维是否缜密。所以，这也是很多面试官喜欢让人手写链表代码的原因。所以，这一节讲到的东西，你一定要自己写代码实现一下，才有效果。\n\n## **栈**\n\n栈是一种操作受限的数据结构，只支持入栈和出栈操作。后进先出是它最大的特点。栈既可以通过数组实现，也可以通过链表来实现。**不管基于数组还是链表，入栈、出栈的时间复杂度都为 O(1)。**除此之外，我们还讲了一种支持动态扩容的顺序栈，你需要重点掌握它的均摊时间复杂度分析方法。\n\n## **队列**\n\n队列最大的特点就是先进先出，主要的两个操作是入队和出队。跟栈一样，它既可以用数组来实现，也可以用链表来实现。用数组实现的叫顺序队列，用链表实现的叫链式队列。特别是长得像一个环的循环队列。在数组实现队列的时候，会有数据搬移操作，要想解决数据搬移的问题，我们就需要像环一样的循环队列。\n\n循环队列是我们这节的重点。要想写出没有 bug 的循环队列实现代码，关键要确定好队空和队满的判定条件，具体的代码你要能写出来。\n\n## **跳表**\n\n跳表使用空间换时间的设计思路，通过构建多级索引来提高查询的效率，实现了基于链表的“二分查找”。**跳表是一种动态数据结构，支持快速地插入、删除、查找操作，时间复杂度都是 O(logn)**。\n\n**跳表的空间复杂度是 O(n)**。不过，跳表的实现非常灵活，可以通过改变索引构建策略，有效平衡执行效率和内存消耗。虽然跳表的代码实现并不简单，但是作为一种动态数据结构，比起红黑树来说，实现要简单多了。所以很多时候，我们为了代码的简单、易读，比起红黑树，我们更倾向用跳表。\n\n## **散列表**\n\n散列表来源于数组，它借助散列函数对数组这种数据结构进行扩展，利用的是数组支持按照下标随机访问元素的特性。散列表两个核心问题是散列函数设计和散列冲突解决。散列冲突有两种常用的解决方法，开放寻址法和链表法。散列函数设计的好坏决定了散列冲突的概率，也就决定散列表的性能。\n\n---\n\n如何设计一个工业级的散列表，以及如何应对各种异常情况，防止在极端情况下，散列表的性能退化过于严重。我分了三部分来讲解这些内容，分别是：如何设计散列函数，如何根据装载因子动态扩容，以及如何选择散列冲突解决方法。\n\n关于散列函数的设计，我们要尽可能让散列后的值随机且均匀分布，这样会尽可能地减少散列冲突，即便冲突之后，分配到每个槽内的数据也比较均匀。除此之外，散列函数的设计也不能太复杂，太复杂就会太耗时间，也会影响散列表的性能。\n\n关于散列冲突解决方法的选择，我对比了开放寻址法和链表法两种方法的优劣和适应的场景。大部分情况下，链表法更加普适。而且，我们还可以通过将链表法中的链表改造成其他动态查找数据结构，比如红黑树，来避免散列表时间复杂度退化成 O(n)，抵御散列碰撞攻击。但是，对于小规模数据、装载因子不高的散列表，比较适合用开放寻址法。\n\n对于动态散列表来说，不管我们如何设计散列函数，选择什么样的散列冲突解决方法。随着数据的不断增加，散列表总会出现装载因子过高的情况。这个时候，我们就需要启动动态扩容。\n\n---\n\n为什么散列表和链表经常一块使用？\n\n散列表这种数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的。也就说，它无法支持按照某种顺序快速地遍历数据。如果希望按照顺序遍历散列表中的数据，那我们需要将散列表中的数据拷贝到数组中，然后排序，再遍历。\n\n因为散列表是动态数据结构，不停地有数据的插入、删除，所以每当我们希望按顺序遍历散列表中的数据的时候，都需要先排序，那效率势必会很低。为了解决这个问题，我们将散列表和链表（或者跳表）结合在一起使用。\n\n## **二叉树**\n\n树。关于树，有几个比较常用的概念你需要掌握，那就是：根节点、叶子节点、父节点、子节点、兄弟节点，还有节点的高度、深度、层数，以及树的高度。\n\n我们平时最常用的树就是二叉树。二叉树的每个节点最多有两个子节点，分别是左子节点和右子节点。二叉树中，有两种比较特殊的树，分别是满二叉树和完全二叉树。满二叉树又是完全二叉树的一种特殊情况。\n\n二叉树既可以用链式存储，也可以用数组顺序存储。数组顺序存储的方式比较适合完全二叉树，其他类型的二叉树用数组存储会比较浪费存储空间。除此之外，二叉树里非常重要的操作就是前、中、后序遍历操作，遍历的时间复杂度是 O(n)，你需要理解并能用递归代码来实现。\n\n---\n\n二叉查找树是一种特殊的二叉树，它支持快速地查找、插入、删除操作。\n\n二叉查找树中，每个节点的值都大于左子树节点的值，小于右子树节点的值。不过，这只是针对没有重复数据的情况。对于存在重复数据的二叉查找树，我介绍了两种构建方法，一种是让每个节点存储多个值相同的数据；另一种是，每个节点中存储一个数据。针对这种情况，我们只需要稍加改造原来的插入、删除、查找操作即可。\n\n在二叉查找树中，查找、插入、删除等很多操作的时间复杂度都跟树的高度成正比。两个极端情况的时间复杂度分别是 O(n) 和 O(logn)，分别对应二叉树退化成链表的情况和完全二叉树。\n\n为了避免时间复杂度的退化，针对二叉查找树，我们又设计了一种更加复杂的树，平衡二叉查找树，时间复杂度可以做到稳定的 O(logn)。\n\n## **红黑树**\n\n**我们学习数据结构和算法，要学习它的由来、特性、适用的场景以及它能解决的问题。对于红黑树，也不例外。你如果能搞懂这几个问题，其实就已经足够了。**\n\n红黑树是一种平衡二叉查找树。它是为了解决普通二叉查找树在数据更新的过程中，复杂度退化的问题而产生的。红黑树的高度近似 log2n，所以它是近似平衡，插入、删除、查找操作的时间复杂度都是 O(logn)。\n\n因为红黑树是一种性能非常稳定的二叉查找树，所以，在工程中，但凡是用到动态插入、删除、查找数据的场景，都可以用到它。不过，它实现起来比较复杂，如果自己写代码实现，难度会有些高，这个时候，我们其实更倾向用跳表来替代它。\n\n## **堆**\n\n堆是一种完全二叉树。它最大的特性是：每个节点的值都大于等于（或小于等于）其子树节点的值。因此，堆被分成了两类，大顶堆和小顶堆。\n\n堆中比较重要的两个操作是插入一个数据和删除堆顶元素。这两个操作都要用到堆化。插入一个数据的时候，我们把新插入的数据放到数组的最后，然后从下往上堆化；删除堆顶数据的时候，我们把数组中的最后一个元素放到堆顶，然后从上往下堆化。这两个操作时间复杂度都是 O(logn)。\n\n除此之外，我们还讲了堆的一个经典应用，堆排序。堆排序包含两个过程，建堆和排序。我们将下标从 n/2 到 1 的节点，依次进行从上到下的堆化操作，然后就可以将数组中的数据组织成堆这种数据结构。接下来，我们迭代地将堆顶的元素放到堆的末尾，并将堆的大小减一，然后再堆化，重复这个过程，直到堆中只剩下一个元素，整个数组中的数据就都有序排列了。\n\n### 堆的应用\n\n优先级队列、求 Top K 问题和求中位数问题。\n\n优先级队列是一种特殊的队列，优先级高的数据先出队，而不再像普通的队列那样，先进先出。实际上，堆就可以看作优先级队列，只是称谓不一样罢了。求 Top K 问题又可以分为针对静态数据和针对动态数据，只需要利用一个堆，就可以做到非常高效率地查询 Top K 的数据。求中位数实际上还有很多变形，比如求 99 百分位数据、90 百分位数据等，处理的思路都是一样的，即利用两个堆，一个大顶堆，一个小顶堆，随着数据的动态添加，动态调整两个堆中的数据，最后大顶堆的堆顶元素就是要求的数据。\n\n## **图**\n\n关于图，你需要理解这样几个概念：无向图、有向图、带权图、顶点、边、度、入度、出度。除此之外，我们还学习了图的两个主要的存储方式：邻接矩阵和邻接表。\n\n邻接矩阵存储方法的缺点是比较浪费空间，但是优点是查询效率高，而且方便矩阵运算。邻接表存储方法中每个顶点都对应一个链表，存储与其相连接的其他顶点。尽管邻接表的存储方式比较节省存储空间，但链表不方便查找，所以查询效率没有邻接矩阵存储方式高。针对这个问题，邻接表还有改进升级版，即将链表换成更加高效的动态数据结构，比如平衡二叉查找树、跳表、散列表等。\n\n## Trie 树\n\nTrie 树是一种解决字符串快速匹配问题的数据结构。如果用来构建 Trie 树的这一组字符串中，前缀重复的情况不是很多，那 Trie 树这种数据结构总体上来讲是比较费内存的，是一种空间换时间的解决问题思路。\n\n尽管比较耗费内存，但是对内存不敏感或者内存消耗在接受范围内的情况下，在 Trie 树中做字符串匹配还是非常高效的，时间复杂度是 O(k)，k 表示要匹配的字符串的长度。\n\n但是，Trie 树的优势并不在于，用它来做动态集合数据的查找，因为，这个工作完全可以用更加合适的散列表或者红黑树来替代。Trie 树最有优势的是查找前缀匹配的字符串，比如搜索引擎中的关键词提示功能这个场景，就比较适合用它来解决，也是 Trie 树比较经典的应用场景。\n\n# 算法\n\n## 递归\n\n递归是一种非常高效、简洁的编码技巧。只要是满足“三个条件”的问题就可以通过递归代码来解决。\n\n不过递归代码也比较难写、难理解。编写递归代码的关键就是不要把自己绕进去，正确姿势是写出递推公式，找出终止条件，然后再翻译成递归代码。\n\n递归代码虽然简洁高效，但是，递归代码也有很多弊端。比如，堆栈溢出、重复计算、函数调用耗时多、空间复杂度高等，所以，在编写递归代码的时候，一定要控制好这些副作用。\n\n## **排序**\n\n![Untitled](Assets/alg_sum.png)\n\n这三种时间复杂度为 O(n2) 的排序算法中，冒泡排序、选择排序，可能就纯粹停留在理论的层面了，学习的目的也只是为了开拓思维，实际开发中应用并不多，但是插入排序还是挺有用的。后面讲排序优化的时候，我会讲到，有些编程语言中的排序函数的实现原理会用到插入排序算法。\n\n这三种排序算法，实现代码都非常简单，对于小规模数据的排序，用起来非常高效。但是在大规模数据排序的时候，这个时间复杂度还是稍微有点高，所以我们更倾向于用下面要讲的时间复杂度为 O(nlogn) 的排序算法。\n\n---\n\n归并排序和快速排序是两种稍微复杂的排序算法，它们用的都是分治的思想，代码都通过递归来实现，过程非常相似。理解归并排序的重点是理解递推公式和 merge() 合并函数。同理，理解快排的重点也是理解递推公式，还有 partition() 分区函数。\n\n归并排序算法是一种在任何情况下时间复杂度都比较稳定的排序算法，这也使它存在致命的缺点，即归并排序不是原地排序算法，空间复杂度比较高，是 O(n)。正因为此，它也没有快排应用广泛。\n\n快速排序算法虽然最坏情况下的时间复杂度是 O(n2)，但是平均情况下时间复杂度都是 O(nlogn)。不仅如此，快速排序算法时间复杂度退化到 O(n2) 的概率非常小，我们可以通过合理地选择 pivot 来避免这种情况。\n\n## **线性排序**\n\n3 种线性时间复杂度的排序算法，有桶排序、计数排序、基数排序。它们对要排序的数据都有比较苛刻的要求，应用不是非常广泛。但是如果数据特征比较符合这些排序算法的要求，应用这些算法，会非常高效，线性时间复杂度可以达到 O(n)。\n\n桶排序和计数排序的排序思想是非常相似的，都是针对范围不大的数据，将数据划分成不同的桶来实现排序。基数排序要求数据可以划分成高低位，位之间有递进关系。比较两个数，我们只需要比较高位，高位相同的再比较低位。而且每一位的数据范围不能太大，因为基数排序算法需要借助桶排序或者计数排序来完成每一个位的排序工作。\n\n## **排序优化**\n\n![Untitled](Assets/alg_sum_1.png)\n\n大部分排序函数都是采用 O(nlogn) 排序算法来实现，但是为了尽可能地提高性能，会做很多优化。还着重讲了快速排序的一些优化策略，比如合理选择分区点、避免递归太深等等。\n\n## **二分查找**\n\n针对有序数据的高效查找算法，二分查找，它的时间复杂度是 O(logn)。\n\n二分查找的核心思想理解起来非常简单，有点类似分治思想。即每次都通过跟区间中的中间元素对比，将待查找的区间缩小为一半，直到找到要查找的元素，或者区间被缩小为 0。但是二分查找的代码实现比较容易写错。你需要着重掌握它的三个容易出错的地方：循环退出条件、mid 的取值，low 和 high 的更新。\n\n二分查找虽然性能比较优秀，但应用场景也比较有限。底层必须依赖数组，并且还要求数据是有序的。对于较小规模的数据查找，我们直接使用顺序遍历就可以了，二分查找的优势并不明显。二分查找更适合处理静态数据，也就是没有频繁的数据插入、删除操作。\n\n---\n\n![Untitled](Assets/alg_sum_2.png)\n\n凡是用二分查找能解决的，绝大部分我们更倾向于用散列表或者二叉查找树。即便是二分查找在内存使用上更节省，但是毕竟内存如此紧缺的情况并不多。那二分查找真的没什么用处了吗？\n\n实际上，之前讲的求“值等于给定值”的二分查找确实不怎么会被用到，二分查找更适合用在“近似”查找问题，在这类问题上，二分查找的优势更加明显。比如几种变体问题，用其他数据结构，比如散列表、二叉树，就比较难实现了。\n\n变体的二分查找算法写起来非常烧脑，很容易因为细节处理不好而产生 Bug，这些容易出错的细节有：**终止条件、区间上下界更新方法、返回值选择。**\n\n## **哈希算法**\n\n哈希算法的四个应用场景。\n\n第一个应用是唯一标识，哈希算法可以对大数据做信息摘要，通过一个较短的二进制编码来表示很大的数据。\n\n第二个应用是用于校验数据的完整性和正确性。\n\n第三个应用是安全加密，我们讲到任何哈希算法都会出现散列冲突，但是这个冲突概率非常小。越是复杂哈希算法越难破解，但同样计算时间也就越长。所以，选择哈希算法的时候，要权衡安全性和计算时间来决定用哪种哈希算法。\n\n第四个应用是散列函数，这个我们前面讲散列表的时候已经详细地讲过，它对哈希算法的要求非常特别，更加看重的是散列的平均性和哈希算法的执行效率。\n\n---\n\n三种哈希算法在分布式系统中的应用，它们分别是：负载均衡、数据分片、分布式存储。\n\n在负载均衡应用中，利用哈希算法替代映射表，可以实现一个会话粘滞的负载均衡策略。在数据分片应用中，通过哈希算法对处理的海量数据进行分片，多机分布式处理，可以突破单机资源的限制。在分布式存储应用中，利用一致性哈希算法，可以解决缓存等分布式系统的扩容、缩容导致数据大量搬移的难题。\n\n## **深度和广度优先搜索**\n\n广度优先搜索和深度优先搜索是图上的两种最常用、最基本的搜索算法，比起其他高级的搜索算法，比如 A*、IDA* 等，要简单粗暴，没有什么优化，所以，也被叫作暴力搜索算法。所以，这两种搜索算法仅适用于状态空间不大，也就是说图不大的搜索。\n\n广度优先搜索，通俗的理解就是，地毯式层层推进，从起始顶点开始，依次往外遍历。广度优先搜索需要借助队列来实现，遍历得到的路径就是，起始顶点到终止顶点的最短路径。深度优先搜索用的是回溯思想，非常适合用递归实现。换种说法，深度优先搜索是借助栈来实现的。在执行效率方面，深度优先和广度优先搜索的时间复杂度都是 O(E)，空间复杂度是 O(V)。\n\n## **贪心算法**\n\n实际上，贪心算法适用的场景比较有限。这种算法思想更多的是指导设计基础算法。比如最小生成树算法、单源最短路径算法，这些算法都用到了贪心算法。**从我个人的学习经验来讲，不要刻意去记忆贪心算法的原理，多练习才是最有效的学习方法。**\n\n贪心算法的最难的一块是如何将要解决的问题抽象成贪心算法模型，只要这一步搞定之后，贪心算法的编码一般都很简单。贪心算法解决问题的正确性虽然很多时候都看起来是显而易见的，但是要严谨地证明算法能够得到最优解，并不是件容易的事。所以，很多时候，我们只需要多举几个例子，看一下贪心算法的解决方案是否真的能得到最优解就可以了。\n\n## **分治算法**\n\n分治算法用四个字概括就是“分而治之”，将原问题划分成 n 个规模较小而结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。这个思想非常简单、好理解。\n\n今天我们讲了两种分治算法的典型的应用场景，一个是用来指导编码，降低问题求解的时间复杂度，另一个是解决海量数据处理问题。比如 MapReduce 本质上就是利用了分治思想。\n\n## **回溯算法**\n\n回溯算法的思想非常简单，大部分情况下，都是用来解决广义的搜索问题，也就是，从一组可能的解中，选择出一个满足要求的解。回溯算法非常适合用递归来实现，在实现的过程中，剪枝操作是提高回溯效率的一种技巧。利用剪枝，我们并不需要穷举搜索所有的情况，从而提高搜索效率。\n\n尽管回溯算法的原理非常简单，但是却可以解决很多问题，比如我们开头提到的深度优先搜索、八皇后、0-1 背包问题、图的着色、旅行商问题、数独、全排列、正则表达式匹配等等。如果感兴趣的话，你可以自己搜索研究一下，最好还能用代码实现一下。如果这几个问题都能实现的话，你基本就掌握了回溯算法。\n\n## **动态规划**\n\n大部分动态规划能解决的问题，都可以通过回溯算法来解决，只不过回溯算法解决起来效率比较低，时间复杂度是指数级的。动态规划算法，在执行效率方面，要高很多。尽管执行效率提高了，但是动态规划的空间复杂度也提高了，所以，很多时候，我们会说，动态规划是一种空间换时间的算法思想。\n\n---\n\n我首先讲了什么样的问题适合用动态规划解决。这些问题可以总结概括为“一个模型三个特征”。其中，“一个模型”指的是，问题可以抽象成分阶段决策最优解模型。“三个特征”指的是最优子结构、无后效性和重复子问题。\n\n然后，我讲了两种动态规划的解题思路。它们分别是状态转移表法和状态转移方程法。其中，状态转移表法解题思路大致可以概括为，**回溯算法实现 - 定义状态 - 画递归树 - 找重复子问题 - 画状态转移表 - 根据递推关系填表 - 将填表过程翻译成代码**。状态转移方程法的大致思路可以概括为，**找最优子结构 - 写状态转移方程 - 将状态转移方程翻译成代码**。\n\n最后，我们对比了之前讲过的四种算法思想。贪心、回溯、动态规划可以解决的问题模型类似，都可以抽象成多阶段决策最优解模型。尽管分治算法也能解决最优问题，但是大部分问题的背景都不适合抽象成多阶段决策模型。","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["CS"]},"/CS/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84":{"title":"算法与数据结构","content":"\n# 算法与数据结构\n\n# 前言\n\n想要学习数据结构与算法，**首先要掌握一个数据结构与算法中最重要的概念——复杂度分析。**\n\n这个概念究竟有多重要呢？可以这么说，它几乎占了数据结构和算法这门课的半壁江山，是数据结构和算法学习的精髓。\n\n数据结构和算法解决的是如何更省、更快地存储和处理数据的问题，因此，我们就需要一个考量效率和资源消耗的方法，这就是复杂度分析方法。所以，如果你只掌握了数据结构和算法的特点、用法，但是没有学会复杂度分析，那就相当于只知道操作口诀，而没掌握心法。只有把心法了然于胸，才能做到无招胜有招！\n\n![Untitled](Assets/alg.png)\n\n**我们学习数据结构和算法，要学习它的由来、特性、适用的场景以及它能解决的问题。**\n\n作为初学者，或者一个非算法工程师来说，需要掌握的20个最常用的、最基础数据结构与算法：**数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie 树；10 个算法：递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法。**\n\n## 一些可以让你事半功倍的学习技巧\n\n### 边学边练，适度刷题\n\n建议你每周花 1～2 个小时的时间，集中把学习的数据结构和算法，全都自己写出来，用代码实现一遍。这样一定会比单纯地看或者听的效果要好很多！\n\n### 多问、多思考、多互动\n\n学习最好的方法是，找到几个人一起学习，一块儿讨论切磋，有问题及时寻求老师答疑。\n\n### 打怪升级学习法\n\n学习的过程中，我们碰到最大的问题就是，坚持不下来。 是的，很多基础课程学起来都非常枯燥。为此，我自己总结了一套“打怪升级学习法”。\n\n游戏你肯定玩过吧？为什么很多看起来非常简单又没有乐趣的游戏，你会玩得不亦乐乎呢？这是因为，当你努力打到一定级别之后，每天看着自己的经验值、战斗力在慢慢提高，那种每天都在一点一点成长的成就感就不由自主地产生了。\n\n所以，**我们在枯燥的学习过程中，也可以给自己设立一个切实可行的目标**，就像打怪升级一样。\n\n### 知识需要沉淀，不要想试图一下子掌握所有\n\n在学习的过程中，一定会碰到“拦路虎”。如果哪个知识点没有怎么学懂，不要着急，这是正常的。因为，想听一遍、看一遍就把所有知识掌握，这肯定是不可能的。**学习知识的过程是反复迭代、不断沉淀的过程。**\n\n# 复杂度分析\n\n这里有段非常简单的代码，求 1,2,3...n 的累加和。现在，我们就一块来估算一下这段代码的执行时间。\n\n```c\nint cal(int n) {\n   int sum = 0;\n   int i = 1;\n   for (; i \u003c= n; ++i) {\n     sum = sum + i;\n   }\n   return sum;\n }\n```\n\n从 CPU 的角度来看，这段代码的每一行都执行着类似的操作：读数据-运算-写数据。尽管每行代码对应的 CPU 执行的个数、执行的时间都不一样，但是，我们这里只是粗略估计，所以可以假设每行代码执行的时间都一样，为 unit_time。在这个假设的基础之上，这段代码的总执行时间是多少呢？\n\n第 2、3 行代码分别需要 1 个 unit_time 的执行时间，第 4、5 行都运行了 n 遍，所以需要 2n*unit_time 的执行时间，所以这段代码总的执行时间就是 T(n)=(2n+2)*unit_time。可以看出来，所有代码的执行时间 T(n) 与每行代码的执行次数成正比。\n\n尽管我们不知道 unit_time 的具体值，但是通过这两段代码执行时间的推导过程，我们可以得到一个非常重要的规律，那就是，所有代码的执行时间 T(n) 与每行代码的执行次数 f(n) 成正比。\n\n我们可以把这个规律总结成一个公式。\n\n![Untitled](Assets/alg_1.png)\n\nT(n) 我们已经讲过了，它表示代码执行的时间；n 表示数据规模的大小；f(n) 表示每行代码执行的次数总和。因为这是一个公式，所以用 f(n) 来表示。公式中的 O，表示代码的执行时间 T(n) 与 f(n) 表达式成正比。\n\n所以，第一个例子中的 T(n) = O(2n+2)。这就是大 O 时间复杂度表示法。大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示**代码执行时间随数据规模增长的变化趋势**，所以，也叫作**渐进时间复杂度**（asymptotic time complexity），简称**时间复杂度**。\n\n## 时间复杂度分析\n\n### 只关注循环执行次数最多的一段代码\n\n大 O 这种复杂度表示方法只是表示一种变化趋势。我们通常会忽略掉公式中的常量、低阶、系数，只需要记录一个最大阶的量级就可以了。所以，**我们在分析一个算法、一段代码的时间复杂度的时候，也只关注循环执行次数最多的那一段代码就可以了**。这段核心代码执行次数的 n 的量级，就是整段要分析代码的时间复杂度。\n\n### 加法法则：总复杂度等于量级最大的那段代码的复杂度\n\n**总的时间复杂度就等于量级最大的那段代码的时间复杂度。**那我们将这个规律抽象成公式就是：\n\n如果 T1(n)=O(f(n))，T2(n)=O(g(n))；那么 T(n)=T1(n)+T2(n)=max(O(f(n)), O(g(n))) =O(max(f(n), g(n))).\n\n### 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积\n\n如果 T1(n)=O(f(n))，T2(n)=O(g(n))；那么 T(n)=T1(n)*T2(n)=O(f(n))*O(g(n))=O(f(n)*g(n)).也就是说，假设 T1(n) = O(n)，T2(n) = O(n2)，则 T1(n) * T2(n) = O(n3)。\n\n落实到具体的代码上，我们可以把乘法法则看成是**嵌套循环。**\n\n## 几种常见时间复杂度实例分析\n\n![Untitled](Assets/alg_2.png)\n\n对于刚罗列的复杂度量级，我们可以粗略地分为两类，多项式量级和非多项式量级。其中，非多项式量级只有两个：O(2n) 和 O(n!)。\n\n我们把时间复杂度为非多项式量级的算法问题叫作 NP（Non-Deterministic Polynomial，非确定多项式）问题。\n\n当数据规模 n 越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法其实是非常低效的算法。\n\n### O(1)\n\n只要代码的执行时间不随 n 的增大而增长，这样代码的时间复杂度我们都记作 O(1)。或者说，一般情况下，**只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)**。\n\n### O(logn)、O(nlogn)\n\n对数阶时间复杂度非常常见，同时也是最难分析的一种时间复杂度。\n\n```c\ni=1;\nwhile (i \u003c= n) {\n   i = i * 2;\n}\n```\n\n根据我们前面讲的复杂度分析方法，第三行代码是循环执行次数最多的。所以，我们只要能计算出这行代码被执行了多少次，就能知道整段代码的时间复杂度。\n\n从代码中可以看出，变量 i 的值从 1 开始取，每循环一次就乘以 2。当大于 n 时，循环结束。还记得我们高中学过的等比数列吗？实际上，变量 i 的取值就是一个等比数列。如果我把它一个一个列出来，就应该是这个样子的：\n\n![Untitled](Assets/alg_3.png)\n\n所以，我们只要知道 x 值是多少，就知道这行代码执行的次数了。通过 2x=n 求解 x 这个问题我们想高中应该就学过了，我就不多说了。x=log2n，所以，这段代码的时间复杂度就是 O(log2n)。\n\n实际上，不管是以 2 为底、以 3 为底，还是以 10 为底，我们可以把所有对数阶的时间复杂度都记为 O(logn)。为什么呢？\n\n我们知道，对数之间是可以互相转换的，log3n 就等于 log32 * log2n，所以 O(log3n) = O(C * log2n)，其中 C=log32 是一个常量。基于我们前面的一个理论：在采用大 O 标记复杂度的时候，可以忽略系数，即 O(Cf(n)) = O(f(n))。所以，O(log2n) 就等于 O(log3n)。因此，在对数阶时间复杂度的表示方法里，我们忽略对数的“底”，统一表示为 O(logn)。\n\n### O(m+n)、O(m*n)\n\n我们再来讲一种跟前面都不一样的时间复杂度，代码的复杂度**由两个数据的规模**来决定。\n\n```c\nint cal(int m, int n) {\n  int sum_1 = 0;\n  int i = 1;\n  for (; i \u003c m; ++i) {\n    sum_1 = sum_1 + i;\n  }\n\n  int sum_2 = 0;\n  int j = 1;\n  for (; j \u003c n; ++j) {\n    sum_2 = sum_2 + j;\n  }\n\n  return sum_1 + sum_2;\n}\n```\n\n从代码中可以看出，m 和 n 是表示两个数据规模。我们无法事先评估 m 和 n 谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个。所以，上面代码的时间复杂度就是 O(m+n)。\n\n针对这种情况，原来的加法法则就不正确了，我们需要将加法规则改为：T1(m) + T2(n) = O(f(m) + g(n))。但是乘法法则继续有效：T1(m)*T2(n) = O(f(m) * f(n))。\n\n## 空间复杂度分析\n\n前面我讲过，时间复杂度的全称是渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系。类比一下，空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系。\n\n```c\nvoid print(int n) {\n  int i = 0;\n  int[] a = new int[n];\n  for (i; i \u003cn; ++i) {\n    a[i] = i * i;\n  }\n\n  for (i = n-1; i \u003e= 0; --i) {\n    print out a[i]\n  }\n}\n```\n\n跟时间复杂度分析一样，我们可以看到，第 2 行代码中，我们申请了一个空间存储变量 i，但是它是常量阶的，跟数据规模 n 没有关系，所以我们可以忽略。第 3 行申请了一个大小为 n 的 int 类型数组，除此之外，剩下的代码都没有占用更多的空间，所以整段代码的空间复杂度就是 O(n)。\n\n我们常见的空间复杂度就是 O(1)、O(n)、O(n2 )，像 O(logn)、O(nlogn) 这样的对数阶复杂度平时都用不到。而且，空间复杂度分析比时间复杂度分析要简单很多。所以，对于空间复杂度，掌握刚我说的这些内容已经足够了。\n\n## **最好、最坏、平均、均摊时间复杂度**\n\n### 最好、最坏情况时间复杂度\n\n```c\n// n表示数组array的长度\nint find(int[] array, int n, int x) {\n  int i = 0;\n  int pos = -1;\n  for (; i \u003c n; ++i) {\n    if (array[i] == x) {\n       pos = i;\n       break;\n    }\n  }\n  return pos;\n}\n```\n\n因为，要查找的变量 x 可能出现在数组的任意位置。如果数组中第一个元素正好是要查找的变量 x，那就不需要继续遍历剩下的 n-1 个数据了，那时间复杂度就是 O(1)。但如果数组中不存在变量 x，那我们就需要把整个数组都遍历一遍，时间复杂度就成了 O(n)。所以，不同的情况下，这段代码的时间复杂度是不一样的。\n\n为了表示代码在不同情况下的不同时间复杂度，我们需要引入三个概念：最好情况时间复杂度、最坏情况时间复杂度和平均情况时间复杂度。\n\n顾名思义，**最好情况时间复杂度就是，在最理想的情况下，执行这段代码的时间复杂度**。就像我们刚刚讲到的，在最理想的情况下，要查找的变量 x 正好是数组的第一个元素，这个时候对应的时间复杂度就是最好情况时间复杂度。\n\n同理，**最坏情况时间复杂度就是，在最糟糕的情况下，执行这段代码的时间复杂度**。就像刚举的那个例子，如果数组中没有要查找的变量 x，我们需要把整个数组都遍历一遍才行，所以这种最糟糕情况下对应的时间复杂度就是最坏情况时间复杂度。\n\n### 平均情况时间复杂度\n\n我们都知道，最好情况时间复杂度和最坏情况时间复杂度对应的都是极端情况下的代码复杂度，发生的概率其实并不大。为了更好地表示平均情况下的复杂度，我们需要引入另一个概念：平均情况时间复杂度，后面我简称为平均时间复杂度。\n\n平均时间复杂度又该怎么分析呢？我还是借助刚才查找变量 x 的例子来给你解释。要查找的变量 x 在数组中的位置，有 n+1 种情况：在数组的 0～n-1 位置中和不在数组中。我们把每种情况下，查找需要遍历的元素个数累加起来，然后再除以 n+1，就可以得到需要遍历的元素个数的平均值，即：\n\n![Untitled](Assets/alg_4.png)\n\n我们知道，时间复杂度的大 O 标记法中，可以省略掉系数、低阶、常量，所以，咱们把刚刚这个公式简化之后，得到的平均时间复杂度就是 O(n)。\n\n这个结论虽然是正确的，但是计算过程稍微有点儿问题。究竟是什么问题呢？我们刚讲的这 n+1 种情况，出现的概率并不是一样的。我带你具体分析一下。（这里要稍微用到一点儿概率论的知识，不过非常简单，你不用担心。）\n\n我们知道，要查找的变量 x，要么在数组里，要么就不在数组里。这两种情况对应的概率统计起来很麻烦，为了方便你理解，我们假设在数组中与不在数组中的概率都为 1/2。另外，要查找的数据出现在 0～n-1 这 n 个位置的概率也是一样的，为 1/n。所以，根据概率乘法法则，要查找的数据出现在 0～n-1 中任意位置的概率就是 1/(2n)。\n\n因此，前面的推导过程中存在的最大问题就是，没有将各种情况发生的概率考虑进去。如果我们把每种情况发生的概率也考虑进去，那平均时间复杂度的计算过程就变成了这样：\n\n![Untitled](Assets/alg_5.png)\n\n这个值就是概率论中的**加权平均值**，也叫作**期望值**，所以平均时间复杂度的全称应该叫**加权平均时间复杂度**或者**期望时间复杂度**。\n\n引入概率之后，前面那段代码的加权平均值为 (3n+1)/4。用大 O 表示法来表示，去掉系数和常量，这段代码的加权平均时间复杂度仍然是 O(n)。\n\n实际上，在大多数情况下，我们并不需要区分最好、最坏、平均情况时间复杂度三种情况。很多时候，我们使用一个复杂度就可以满足需求了。只有同一块代码在不同的情况下，时间复杂度有量级的差距，我们才会使用这三种复杂度表示法来区分。\n\n### 均摊时间复杂度\n\n```c\n// array表示一个长度为n的数组\n // 代码中的array.length就等于n\n int[] array = new int[n];\n int count = 0;\n\n void insert(int val) {\n    if (count == array.length) {\n       int sum = 0;\n       for (int i = 0; i \u003c array.length; ++i) {\n          sum = sum + array[i];\n       }\n       array[0] = sum;\n       count = 1;\n    }\n\n    array[count] = val;\n    ++count;\n }\n```\n\n这段代码实现了一个往数组中插入数据的功能。当数组满了之后，也就是代码中的 count == array.length 时，我们用 for 循环遍历数组求和，并清空数组，将求和之后的 sum 值放到数组的第一个位置，然后再将新的数据插入。但如果数组一开始就有空闲空间，则直接将数据插入数组。\n\n最理想的情况下，数组中有空闲空间，我们只需要将数据插入到数组下标为 count 的位置就可以了，所以最好情况时间复杂度为 O(1)。最坏的情况下，数组中没有空闲空间了，我们需要先做一次数组的遍历求和，然后再将数据插入，所以最坏情况时间复杂度为 O(n)。\n\n那平均时间复杂度是多少呢？答案是 O(1)。我们还是可以通过前面讲的概率论的方法来分析。\n\n假设数组的长度是 n，根据数据插入的位置的不同，我们可以分为 n 种情况，每种情况的时间复杂度是 O(1)。除此之外，还有一种“额外”的情况，就是在数组没有空闲空间时插入一个数据，这个时候的时间复杂度是 O(n)。而且，这 n+1 种情况发生的概率一样，都是 1/(n+1)。所以，根据加权平均的计算方法，我们求得的平均时间复杂度就是：\n\n![Untitled](Assets/alg_6.png)\n\n这个例子里的平均复杂度分析其实并不需要这么复杂，不需要引入概率论的知识。我们先来对比一下这个 insert() 的例子和前面那个 find() 的例子，你就会发现这两者有很大差别。\n\n首先，find() 函数在极端情况下，复杂度才为 O(1)。但 insert() 在大部分情况下，时间复杂度都为 O(1)。只有个别情况下，复杂度才比较高，为 O(n)。这是 insert()第一个区别于 find() 的地方。\n\n我们再来看第二个不同的地方。对于 insert() 函数来说，O(1) 时间复杂度的插入和 O(n) 时间复杂度的插入，出现的频率是非常有规律的，而且有一定的前后时序关系，一般都是一个 O(n) 插入之后，紧跟着 n-1 个 O(1) 的插入操作，循环往复。\n\n针对这种特殊的场景，我们引入了一种更加简单的分析方法：**摊还分析法**，通过摊还分析得到的时间复杂度我们起了一个名字，叫**均摊时间复杂度**。\n\n那究竟如何使用摊还分析法来分析算法的均摊时间复杂度呢？每一次 O(n) 的插入操作，都会跟着 n-1 次 O(1) 的插入操作，所以把耗时多的那次操作均摊到接下来的 n-1 次耗时少的操作上，均摊下来，这一组连续的操作的均摊时间复杂度就是 O(1)。这就是均摊分析的大致思路。\n\n对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一块儿分析，看是否能将较高时间复杂度那次操作的耗时，平摊到其他那些时间复杂度比较低的操作上。而且，在能够应用均摊时间复杂度分析的场合，一般均摊时间复杂度就等于最好情况时间复杂度。\n\n**均摊时间复杂度就是一种特殊的平均时间复杂度**，我们没必要花太多精力去区分它们。你最应该掌握的是它的分析方法，摊还分析。\n\n# 数据结构\n\n## **数组**\n\n数组（Array）是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。正因为如此也使得数组具有随机访问的特性。\n\n低效的“插入”和“删除”：插入和删除的平均时间复杂度均为O(n)。\n\n容器能否完全替代数组？\n\n容器最大的优势就是可以将很多数组操作的细节封装起来。比如前面提到的数组插入、删除数据时需要搬移其他数据等。另外，它还有一个优势，就是支持动态扩容。\n\n不过，这里需要注意一点，因为扩容操作涉及内存申请和数据搬移，是比较耗时的。所以，如果事先能确定需要存储的数据大小，最好在创建容器的时候事先指定数据大小。\n\n总结一下，对于业务开发，直接使用容器就足够了，省时省力。毕竟损耗一丢丢性能，完全不会影响到系统整体的性能。但如果你是做一些非常底层的开发，比如开发网络框架，性能的优化需要做到极致，这个时候数组就会优于容器，成为首选。\n\n## **链表**\n\n底层的存储结构上来看，数组需要一块连续的内存空间来存储，而链表恰恰相反，它并不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起来使用。\n\n![Untitled](Assets/alg_7.png)\n\n链表结构五花八门，今天介绍三种最常见的链表结构，它们分别是：单链表、双向链表和循环链表。\n\n![Untitled](Assets/alg_8.png)\n\n在进行数组的插入、删除操作时，为了保持内存数据的连续性，需要做大量的数据搬移，所以时间复杂度是 O(n)。而在链表中插入或者删除一个数据，我们并不需要为了保持内存的连续性而搬移结点，因为链表的存储空间本身就不是连续的。针对链表的插入和删除操作，我们只需要考虑相邻结点的指针改变，所以对应的时间复杂度是 O(1)。\n\n但是，有利就有弊。链表要想随机访问第 k 个元素，就没有数组那么高效了。因为链表中的数据并非连续存储的，所以无法像数组那样，根据首地址和下标，通过寻址公式就能直接计算出对应的内存地址，而是需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点。\n\n**循环链表是一种特殊的单链表。**\n\n![Untitled](Assets/alg_9.png)\n\n单向链表只有一个方向，结点只有一个后继指针 next 指向后面的结点。而**双向链表**，顾名思义，它支持两个方向，每个结点不止有一个后继指针 next 指向后面的结点，还有一个前驱指针 prev 指向前面的结点。\n\n![Untitled](Assets/alg_10.png)\n\n这里有一个更加重要的知识点需要你掌握，那就是用空间换时间的设计思想。当内存空间充足的时候，如果我们更加追求代码的执行速度，我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构。相反，如果内存比较紧缺，比如代码跑在手机或者单片机上，这个时候，就要反过来用时间换空间的设计思路。\n\n对于执行较慢的程序，可以通过消耗更多的内存（空间换时间）来进行优化；而消耗过多内存的程序，可以通过消耗更多的时间（时间换空间）来降低内存的消耗。\n\n了解了循环链表和双向链表，如果把这两种链表整合在一起就是一个新的版本：**双向循环链表**。\n\n![Untitled](Assets/alg_11.png)\n\n### 链表 VS 数组性能大比拼\n\n![Untitled](Assets/alg_12.png)\n\n数组简单易用，在实现上使用的是连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对 CPU 缓存不友好，没办法有效预读。\n\n链表中的每个结点都需要消耗额外的存储空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍。而且，对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片，如果是 Java 语言，就有可能会导致频繁的 GC（Garbage Collection，垃圾回收）。\n\n和数组相比，链表更适合插入、删除操作频繁的场景，查询的时间复杂度较高。不过，在具体软件开发中，要对数组和链表的各种性能进行对比，综合来选择使用两者中的哪一个。\n\n### 几个写链表代码技巧\n\n- **技巧一：理解指针或引用的含义**\n\n    将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。\n\n- **技巧二：警惕指针丢失和内存泄漏**\n\n    插入结点时，一定要注意操作的顺序。删除链表结点时，也一定要记得手动释放内存空间。\n\n- **技巧三：利用哨兵简化实现难度**\n\n    针对链表的插入、删除操作，需要对插入第一个结点和删除最后一个结点的情况进行特殊处理。\n\n    如果我们引入哨兵结点，在任何时候，不管链表是不是空，head 指针都会一直指向这个哨兵结点。我们也把这种有哨兵结点的链表叫**带头链表**。相反，没有哨兵结点的链表就叫作不带头链表。\n\n    哨兵结点是不存储数据的。因为哨兵结点一直存在，所以插入第一个结点和插入其他结点，删除最后一个结点和删除其他结点，都可以统一为相同的代码实现逻辑了。\n\n    ![Untitled](Assets/alg_13.png)\n\n- **技巧四：重点留意边界条件处理**\n    - 如果链表为空时，代码是否能正常工作？\n    - 如果链表只包含一个结点时，代码是否能正常工作？\n    - 如果链表只包含两个结点时，代码是否能正常工作？\n    - 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？\n\n    不光光是写链表代码，你在写任何代码时，也千万不要只是实现业务正常情况下的功能就好了，一定要多想想，你的代码在运行的时候，可能会遇到哪些边界情况或者异常情况。遇到了应该如何应对，这样写出来的代码才够健壮！\n\n- **技巧五：举例画图，辅助思考**\n\n    对于稍微复杂的链表操作，比如前面我们提到的单链表反转，指针一会儿指这，一会儿指那，一会儿就被绕晕了。总感觉脑容量不够，想不清楚。所以这个时候就要使用大招了，举例法和画图法。\n\n    你可以找一个具体的例子，把它画在纸上，释放一些脑容量，留更多的给逻辑思考，这样就会感觉到思路清晰很多。比如往单链表中插入一个数据这样一个操作，我一般都是把各种情况都举一个例子，画出插入前和插入后的链表变化，如图所示：\n\n    ![Untitled](Assets/alg_14.png)\n\n- **技巧六：多写多练，没有捷径**\n\n    我精选了 5 个常见的链表操作。你只要把这几个操作都能写熟练，不熟就多写几遍，我保证你之后再也不会害怕写链表代码。\n\n    - 单链表反转\n    - 链表中环的检测\n    - 两个有序的链表合并\n    - 删除链表倒数第 n 个结点\n    - 求链表的中间结点\n\n## **栈**\n\n关于“栈”，我有一个非常贴切的例子，就是一摞叠在一起的盘子。我们平时放盘子的时候，都是从下往上一个一个放；取的时候，我们也是从上往下一个一个地依次取，不能从中间任意抽出。**后进者先出，先进者后出，这就是典型的“栈”结构。**\n\n![Untitled](Assets/alg_15.png)\n\n从栈的操作特性上来看，**栈是一种“操作受限”的线性表，只允许在一端插入和删除数据**。\n\n从功能上来说，数组或链表确实可以替代栈，但你要知道，特定的数据结构是对特定场景的抽象，而且，数组或链表暴露了太多的操作接口，操作上的确灵活自由，但使用时就比较不可控，自然也就更容易出错。\n\n**当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，这时我们就应该首选“栈”这种数据结构。**\n\n栈既可以用数组来实现，也可以用链表来实现。用数组实现的栈，我们叫作**顺序栈**，用链表实现的栈，我们叫作**链式栈**。\n\n### 栈在表达式求值中的应用\n\n编译器通过两个栈来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。我们从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较。\n\n如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。\n\n![Untitled](Assets/alg_16.png)\n\n### 栈在括号匹配中的应用\n\n我们假设表达式中只包含三种括号，圆括号 ()、方括号[]和花括号{}，并且它们可以任意嵌套。比如，{[] ()[{}]}或[{()}([])]等都为合法格式，而{[}()]或[({)]为不合法的格式。那我现在给你一个包含三种括号的表达式字符串，如何检查它是否合法呢？\n\n这里也可以用栈来解决。我们用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。\n\n当所有的括号都扫描完成之后，如果栈为空，则说明字符串为合法格式；否则，说明有未匹配的左括号，为非法格式。\n\n## **队列**\n\n队列这个概念非常好理解。你可以把它想象成排队买票，先来的先买，后来的人只能站末尾，不允许插队。**先进者先出，这就是典型的“队列”。**\n\n队列跟栈一样，也是一种操作受限的线性表数据结构。\n\n跟栈一样，队列可以用数组来实现，也可以用链表来实现。用数组实现的栈叫作顺序栈，用链表实现的栈叫作链式栈。同样，用数组实现的队列叫作**顺序队列**，用链表实现的队列叫作**链式队列**。\n\n在实现上，对于栈来说，我们只需要一个**栈顶指针**就可以了。但是队列需要两个指针：一个是 head 指针，指向队头；一个是 tail 指针，指向队尾。\n\n![Untitled](Assets/alg_17.png)\n\n### 循环队列\n\n要想写出没有 bug 的循环队列的实现代码，我个人觉得，最关键的是，**确定好队空和队满的判定条件。**\n\n队列为空的判断条件仍然是 head == tail。但队列满的判断条件就稍微有点复杂了。我画了一张队列满的图，你可以看一下，试着总结一下规律。\n\n![Untitled](Assets/alg_18.png)\n\n就像我图中画的队满的情况，tail=3，head=4，n=8，所以总结一下规律就是：(3+1)%8=4。多画几张队满的图，你就会发现，当队满时，**(tail+1)%n=head**。\n\n你有没有发现，当队列满时，图中的 tail 指向的位置实际上是没有存储数据的。所以，循环队列会浪费一个数组的存储空间。\n\n### **二叉树基础**\n\n### 树（Tree）\n\n下面这幅图，A 节点就是 B 节点的**父节点**，B 节点是 A 节点的**子节点**。B、C、D 这三个节点的父节点是同一个节点，所以它们之间互称为**兄弟节点**。我们把没有父节点的节点叫做**根节点**，也就是图中的节点 E。我们把没有子节点的节点叫做**叶子节点**或者**叶节点**，比如图中的 G、H、I、J、K、L 都是叶子节点。\n\n![Untitled](Assets/alg_19.png)\n\n关于“树”，还有三个比较相似的概念：高度（Height）、深度（Depth）、层（Level）。它们的定义是这样的：\n\n![Untitled](Assets/alg_20.png)\n\n这三个概念的定义比较容易混淆，描述起来也比较空洞。我举个例子说明一下，你一看应该就能明白。\n\n![Untitled](Assets/alg_21.png)\n\n记这几个概念，我还有一个小窍门，就是类比“高度”“深度”“层”这几个名词在生活中的含义。\n\n在我们的生活中，“高度”这个概念，其实就是从下往上度量，比如我们要度量第 10 层楼的高度、第 13 层楼的高度，起点都是地面。所以，树这种数据结构的高度也是一样，从最底层开始计数，并且计数的起点是 0。\n\n“深度”这个概念在生活中是从上往下度量的，比如水中鱼的深度，是从水平面开始度量的。所以，树这种数据结构的深度也是类似的，从根结点开始度量，并且计数起点也是 0。\n\n“层数”跟深度的计算类似，不过，计数起点是 1，也就是说根节点位于第 1 层。\n\n### 二叉树（Binary Tree）\n\n二叉树，顾名思义，每个节点最多有两个“叉”，也就是两个子节点，分别是左子节点和右子节点。不过，二叉树并不要求每个节点都有两个子节点，有的节点只有左子节点，有的节点只有右子节点。\n\n![Untitled](Assets/alg_22.png)\n\n编号 2 的二叉树中，叶子节点全都在最底层，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫做**满二叉树**。\n\n编号 3 的二叉树中，叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大，这种二叉树叫做**完全二叉树**。\n\n要理解完全二叉树定义的由来，我们需要先了解，如何**表示（或者存储）一棵二叉树**？\n\n想要存储一棵二叉树，我们有两种方法，一种是基于指针或者引用的二叉链式存储法，一种是基于数组的顺序存储法。\n\n我们先来看比较简单、直观的**链式存储法**。从图中你应该可以很清楚地看到，每个节点有三个字段，其中一个存储数据，另外两个是指向左右子节点的指针。我们只要拎住根节点，就可以通过左右子节点的指针，把整棵树都串起来。这种存储方式我们比较常用。大部分二叉树代码都是通过这种结构来实现的。\n\n![Untitled](Assets/alg_23.png)\n\n我们再来看，基于数组的**顺序存储法**。我们把根节点存储在下标 i = 1 的位置，那左子节点存储在下标 2 * i = 2 的位置，右子节点存储在 2 * i + 1 = 3 的位置。以此类推，B 节点的左子节点存储在 2 * i = 2 * 2 = 4 的位置，右子节点存储在 2 * i + 1 = 2 * 2 + 1 = 5 的位置。\n\n![Untitled](Assets/alg_24.png)\n\n如果节点 X 存储在数组中下标为 i 的位置，下标为 2 * i 的位置存储的就是左子节点，下标为 2 * i + 1 的位置存储的就是右子节点。反过来，下标为 i/2 的位置存储就是它的父节点。通过这种方式，我们只要知道根节点存储的位置（一般情况下，为了方便计算子节点，根节点会存储在下标为 1 的位置），这样就可以通过下标计算，把整棵树都串起来。\n\n刚刚举的例子是一棵完全二叉树，所以仅仅“浪费”了一个下标为 0 的存储位置。如果是非完全二叉树，其实会浪费比较多的数组存储空间。\n\n![Untitled](Assets/alg_25.png)\n\n所以，如果某棵二叉树是一棵完全二叉树，那用数组存储无疑是最节省内存的一种方式。因为数组的存储方式并不需要像链式存储法那样，要存储额外的左右子节点的指针。这也是为什么完全二叉树会单独拎出来的原因，也是为什么完全二叉树要求最后一层的子节点都靠左的原因。\n\n### 二叉树的遍历\n\n如何将所有节点都遍历打印出来呢？经典的方法有三种，前序遍历、中序遍历和后序遍历。其中，前、中、后序，表示的是节点与它的左右子树节点遍历打印的先后顺序。\n\n- 前序遍历是指，对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树。\n- 中序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树。\n- 后序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身。\n\n![Untitled](Assets/alg_26.png)\n\n**实际上，二叉树的前、中、后序遍历就是一个递归的过程。**比如，前序遍历，其实就是先打印根节点，然后再递归地打印左子树，最后递归地打印右子树。\n\n```java\n前序遍历的递推公式：\npreOrder(r) = print r-\u003epreOrder(r-\u003eleft)-\u003epreOrder(r-\u003eright)\n\n中序遍历的递推公式：\ninOrder(r) = inOrder(r-\u003eleft)-\u003eprint r-\u003einOrder(r-\u003eright)\n\n后序遍历的递推公式：\npostOrder(r) = postOrder(r-\u003eleft)-\u003epostOrder(r-\u003eright)-\u003eprint r\n```\n\n```java\nvoid preOrder(Node* root) {\n  if (root == null) return;\n  print root // 此处为伪代码，表示打印root节点\n  preOrder(root-\u003eleft);\n  preOrder(root-\u003eright);\n}\n\nvoid inOrder(Node* root) {\n  if (root == null) return;\n  inOrder(root-\u003eleft);\n  print root // 此处为伪代码，表示打印root节点\n  inOrder(root-\u003eright);\n}\n\nvoid postOrder(Node* root) {\n  if (root == null) return;\n  postOrder(root-\u003eleft);\n  postOrder(root-\u003eright);\n  print root // 此处为伪代码，表示打印root节点\n}\n```\n\n从前面画的前、中、后序遍历的顺序图，可以看出来，每个节点最多会被访问两次，所以遍历操作的时间复杂度，跟节点的个数 n 成正比，也就是说二叉树遍历的时间复杂度是 O(n)。\n\n### 二叉查找树（Binary Search Tree）\n\n**二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值。**\n\n1. **二叉查找树的查找操作**\n\n    我们先取根节点，如果它等于我们要查找的数据，那就返回。如果要查找的数据比根节点的值小，那就在左子树中递归查找；如果要查找的数据比根节点的值大，那就在右子树中递归查找。\n\n    ![Untitled](Assets/alg_27.png)\n\n    ```java\n    public class BinarySearchTree {\n      private Node tree;\n\n      public Node find(int data) {\n        Node p = tree;\n        while (p != null) {\n          if (data \u003c p.data) p = p.left;\n          else if (data \u003e p.data) p = p.right;\n          else return p;\n        }\n        return null;\n      }\n\n      public static class Node {\n        private int data;\n        private Node left;\n        private Node right;\n\n        public Node(int data) {\n          this.data = data;\n        }\n      }\n    }\n    ```\n\n2. **二叉查找树的插入操作**\n\n    二叉查找树的插入过程有点类似查找操作。新插入的数据一般都是在叶子节点上，所以我们只需要从根节点开始，依次比较要插入的数据和节点的大小关系。\n\n    如果要插入的数据比节点的数据大，并且节点的右子树为空，就将新数据直接插到右子节点的位置；如果不为空，就再递归遍历右子树，查找插入位置。同理，如果要插入的数据比节点数值小，并且节点的左子树为空，就将新数据插入到左子节点的位置；如果不为空，就再递归遍历左子树，查找插入位置。\n\n    ![Untitled](Assets/alg_28.png)\n\n    ```java\n    public void insert(int data) {\n      if (tree == null) {\n        tree = new Node(data);\n        return;\n      }\n\n      Node p = tree;\n      while (p != null) {\n        if (data \u003e p.data) {\n          if (p.right == null) {\n            p.right = new Node(data);\n            return;\n          }\n          p = p.right;\n        } else { // data \u003c p.data\n          if (p.left == null) {\n            p.left = new Node(data);\n            return;\n          }\n          p = p.left;\n        }\n      }\n    }\n    ```\n\n3. **二叉查找树的删除操作**\n\n    针对要删除节点的子节点个数的不同，我们需要分三种情况来处理。\n\n    第一种情况是，如果要删除的节点没有子节点，我们只需要直接将父节点中，指向要删除节点的指针置为 null。比如图中的删除节点 55。\n\n    第二种情况是，如果要删除的节点只有一个子节点（只有左子节点或者右子节点），我们只需要更新父节点中，指向要删除节点的指针，让它指向要删除节点的子节点就可以了。比如图中的删除节点 13。\n\n    第三种情况是，如果要删除的节点有两个子节点，这就比较复杂了。我们需要找到这个节点的右子树中的最小节点，把它替换到要删除的节点上。然后再删除掉这个最小节点，因为最小节点肯定没有左子节点（如果有左子结点，那就不是最小节点了），所以，我们可以应用上面两条规则来删除这个最小节点。比如图中的删除节点 18。\n\n    ![Untitled](Assets/alg_29.png)\n\n    ```java\n    public void delete(int data) {\n      Node p = tree; // p指向要删除的节点，初始化指向根节点\n      Node pp = null; // pp记录的是p的父节点\n      while (p != null \u0026\u0026 p.data != data) {\n        pp = p;\n        if (data \u003e p.data) p = p.right;\n        else p = p.left;\n      }\n      if (p == null) return; // 没有找到\n\n      // 要删除的节点有两个子节点\n      if (p.left != null \u0026\u0026 p.right != null) { // 查找右子树中最小节点\n        Node minP = p.right;\n        Node minPP = p; // minPP表示minP的父节点\n        while (minP.left != null) {\n          minPP = minP;\n          minP = minP.left;\n        }\n        p.data = minP.data; // 将minP的数据替换到p中\n        p = minP; // 下面就变成了删除minP了\n        pp = minPP;\n      }\n\n      // 删除节点是叶子节点或者仅有一个子节点\n      Node child; // p的子节点\n      if (p.left != null) child = p.left;\n      else if (p.right != null) child = p.right;\n      else child = null;\n\n      if (pp == null) tree = child; // 删除的是根节点\n      else if (pp.left == p) pp.left = child;\n      else pp.right = child;\n    }\n    ```\n\n4. **二叉查找树的其他操作**\n\n    除了插入、删除、查找操作之外，二叉查找树中还可以支持**快速地查找最大节点和最小节点、前驱节点和后继节点**。\n\n    二叉查找树除了支持上面几个操作之外，还有一个重要的特性，就是**中序遍历二叉查找树，可以输出有序的数据序列，时间复杂度是 O(n)**，非常高效。因此，二叉查找树也叫作二叉排序树。\n\n\n### 支持重复数据的二叉查找树\n\n前面我们讲的二叉查找树的操作，针对的都是不存在键值相同的情况。那如果存储的两个对象键值相同，这种情况该怎么处理呢？有两种解决方法。\n\n第一种方法比较容易。二叉查找树中每一个节点不仅会存储一个数据，因此我们通过链表和支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上。\n\n第二种方法比较不好理解，不过更加优雅。\n\n每个节点仍然只存储一个数据。在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的右子树，也就是说，把这个新插入的数据当作大于这个节点的值来处理。\n\n![Untitled](Assets/alg_30.png)\n\n当要查找数据的时候，遇到值相同的节点，我们并不停止查找操作，而是继续在右子树中查找，直到遇到叶子节点，才停止。这样就可以把键值等于要查找值的所有节点都找出来。\n\n![Untitled](Assets/alg_31.png)\n\n对于删除操作，我们也需要先查找到每个要删除的节点，然后再按前面讲的删除操作的方法，依次删除。\n\n![Untitled](Assets/alg_32.png)\n\n### 二叉查找树的时间复杂度\n\n不管操作是插入、删除还是查找，时间复杂度其实都跟树的高度成正比，也就是 O(height)。\n\n在二叉查找树中，查找、插入、删除等很多操作的时间复杂度都跟树的高度成正比。两个极端情况的时间复杂度分别是 O(n) 和 O(logn)，分别对应二叉树退化成链表的情况和完全二叉树。\n\n## 堆\n\n### 如何理解“堆”？\n\n堆是一种特殊的树。什么样的树才是堆。我罗列了两点要求，只要满足这两点，它就是一个堆。\n\n- 堆是一个完全二叉树；\n- 堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。\n\n对于每个节点的值都大于等于子树中每个节点值的堆，我们叫做“大顶堆”。对于每个节点的值都小于等于子树中每个节点值的堆，我们叫做“小顶堆”。\n\n### 如何实现一个堆？\n\n要实现一个堆，我们先要知道，**堆都支持哪些操作**以及**如何存储一个堆**。\n\n之前讲过，完全二叉树比较适合用数组来存储。用数组来存储完全二叉树是非常节省存储空间的。因为我们不需要存储左右子节点的指针，单纯地通过数组的下标，就可以找到一个节点的左右子节点和父节点。所以，堆适合用数组来存储。\n\n1. **往堆中插入一个元素**\n\n    往堆中插入一个元素后，我们需要继续满足堆的两个特性。如果我们把新插入的元素放到堆的最后，就不符合堆的特性了。于是，我们就需要进行调整，让其重新满足堆的特性，这个过程我们起了一个名字，就叫做**堆化**（heapify）。\n\n    堆化实际上有两种，从下往上和从上往下。这里我先讲**从下往上**的堆化方法。\n\n    ![Untitled](Assets/alg_33.png)\n\n    堆化非常简单，就是顺着节点所在的路径，向上或者向下，对比，然后交换。我们可以让新插入的节点与父节点对比大小。如果不满足子节点小于等于父节点的大小关系，我们就互换两个节点。一直重复这个过程，直到父子节点之间满足刚说的那种大小关系。\n\n    ![Untitled](Assets/alg_34.png)\n\n    ```java\n    public class Heap {\n      private int[] a; // 数组，从下标1开始存储数据\n      private int n;  // 堆可以存储的最大数据个数\n      private int count; // 堆中已经存储的数据个数\n\n      public Heap(int capacity) {\n        a = new int[capacity + 1];\n        n = capacity;\n        count = 0;\n      }\n\n      public void insert(int data) {\n        if (count \u003e= n) return; // 堆满了\n        ++count;\n        a[count] = data;\n        int i = count;\n        while (i/2 \u003e 0 \u0026\u0026 a[i] \u003e a[i/2]) { // 自下往上堆化\n          swap(a, i, i/2); // swap()函数作用：交换下标为i和i/2的两个元素\n          i = i/2;\n        }\n      }\n     }\n    ```\n\n2. **删除堆顶元素**\n\n    从堆的定义的第二条中，任何节点的值都大于等于（或小于等于）子树节点的值，我们可以发现，堆顶元素存储的就是堆中数据的最大值或者最小值。\n\n    我们把最后一个节点放到堆顶，然后利用同样的父子节点对比方法。对于不满足父子节点大小关系的，互换两个节点，并且重复进行这个过程，直到父子节点之间满足大小关系为止。这就是**从上往下的堆化方法**。\n\n    因为我们移除的是数组中的最后一个元素，而在堆化的过程中，都是交换操作，不会出现数组中的“空洞”，所以这种方法堆化之后的结果，肯定满足完全二叉树的特性。\n\n    ![Untitled](Assets/alg_35.png)\n\n    ```java\n    public void removeMax() {\n      if (count == 0) return -1; // 堆中没有数据\n      a[1] = a[count];\n      --count;\n      heapify(a, count, 1);\n    }\n\n    private void heapify(int[] a, int n, int i) { // 自上往下堆化\n      while (true) {\n        int maxPos = i;\n        if (i*2 \u003c= n \u0026\u0026 a[i] \u003c a[i*2]) maxPos = i*2;\n        if (i*2+1 \u003c= n \u0026\u0026 a[maxPos] \u003c a[i*2+1]) maxPos = i*2+1;\n        if (maxPos == i) break;\n        swap(a, i, maxPos);\n        i = maxPos;\n      }\n    }\n    ```\n\n    我们知道，一个包含 n 个节点的完全二叉树，树的高度不会超过 log2n。堆化的过程是顺着节点所在路径比较交换的，所以堆化的时间复杂度跟树的高度成正比，也就是 O(logn)。插入数据和删除堆顶元素的主要逻辑就是堆化，所以，往堆中插入一个元素和删除堆顶元素的时间复杂度都是 O(logn)。\n\n\n### 如何基于堆实现排序？\n\n我们借助于堆这种数据结构实现的排序算法，就叫做堆排序。这种排序方法的时间复杂度非常稳定，是 O(nlogn)，并且它还是原地排序算法。\n\n我们可以把堆排序的过程大致分解成两个大的步骤，**建堆**和**排序**。\n\n1. **建堆**\n\n    我们首先将数组原地建成一个堆。所谓“原地”就是，不借助另一个数组，就在原数组上操作。建堆的过程，有两种思路。\n\n    第一种是借助我们前面讲的，在堆中插入一个元素的思路。尽管数组中包含 n 个数据，但是我们可以假设，起初堆中只包含一个数据，就是下标为 1 的数据。然后，我们调用前面讲的插入操作，将下标从 2 到 n 的数据依次插入到堆中。这样我们就将包含 n 个数据的数组，组织成了堆。\n\n    第二种实现思路，跟第一种截然相反，也是我这里要详细讲的。第一种建堆思路的处理过程是从前往后处理数组数据，并且每个数据插入堆中时，都是从下往上堆化。而第二种实现思路，是从后往前处理数组，并且每个数据都是从上往下堆化。\n\n    因为叶子节点往下堆化只能自己跟自己比较，所以我们直接从最后一个非叶子节点开始，依次堆化就行了。\n\n    ![Untitled](Assets/alg_36.png)\n\n    ![Untitled](Assets/alg_37.png)\n\n    ```java\n    private static void buildHeap(int[] a, int n) {\n      for (int i = n/2; i \u003e= 1; --i) {\n        heapify(a, n, i);\n      }\n    }\n\n    private static void heapify(int[] a, int n, int i) {\n      while (true) {\n        int maxPos = i;\n        if (i*2 \u003c= n \u0026\u0026 a[i] \u003c a[i*2]) maxPos = i*2;\n        if (i*2+1 \u003c= n \u0026\u0026 a[maxPos] \u003c a[i*2+1]) maxPos = i*2+1;\n        if (maxPos == i) break;\n        swap(a, i, maxPos);\n        i = maxPos;\n      }\n    }\n    ```\n\n    在这段代码中，我们对下标从 2n 开始到 1 的数据进行堆化，下标是 2n+1 到 n 的节点是叶子节点，我们不需要堆化。实际上，对于完全二叉树来说，下标从 2n+1 到 n 的节点都是叶子节点。\n\n    **建堆的时间复杂度是 O(n)。**\n\n2. **排序**\n\n    建堆结束之后，数组中的数据已经是按照大顶堆的特性来组织的。数组中的第一个元素就是堆顶，也就是最大的元素。我们把它跟最后一个元素交换，那最大元素就放到了下标为 n 的位置。\n\n    这个过程有点类似上面讲的“删除堆顶元素”的操作，当堆顶元素移除之后，我们把下标为 n 的元素放到堆顶，然后再通过堆化的方法，将剩下的 n−1 个元素重新构建成堆。堆化完成之后，我们再取堆顶的元素，放到下标是 n−1 的位置，一直重复这个过程，直到最后堆中只剩下标为 1 的一个元素，排序工作就完成了。\n\n    ![Untitled](Assets/alg_38.png)\n\n    ```java\n    // n表示数据的个数，数组a中的数据从下标1到n的位置。\n    public static void sort(int[] a, int n) {\n      buildHeap(a, n);\n      int k = n;\n      while (k \u003e 1) {\n        swap(a, 1, k);\n        --k;\n        heapify(a, k, 1);\n      }\n    }\n    ```\n\n    整个堆排序的过程，都只需要极个别临时存储空间，所以堆排序是原地排序算法**。堆排序包括建堆和排序两个操作，建堆过程的时间复杂度是 O(n)，排序过程的时间复杂度是 O(nlogn)，所以，堆排序整体的时间复杂度是 O(nlogn)。**\n\n    堆排序不是稳定的排序算法，因为在排序的过程，存在将堆的最后一个节点跟堆顶节点互换的操作，所以就有可能改变值相同数据的原始相对顺序。\n\n\n我这里要稍微解释一下，在前面的讲解以及代码中，我都假设，堆中的数据是从数组下标为 1 的位置开始存储。那如果从 0 开始存储，实际上处理思路是没有任何变化的，唯一变化的，可能就是，代码实现的时候，计算子节点和父节点的下标的公式改变了。\n\n如果节点的下标是 i，那左子节点的下标就是 2∗i+1，右子节点的下标就是 2∗i+2，父节点的下标就是 2i−1。\n\n### 在实际开发中，为什么快速排序要比堆排序性能好？\n\n- **第一点，堆排序数据访问的方式没有快速排序友好。**\n\n    对于快速排序来说，数据是顺序访问的。而对于堆排序来说，数据是跳着访问的。 比如，堆排序中，最重要的一个操作就是数据的堆化。比如下面这个例子，对堆顶节点进行堆化，会依次访问数组下标是 1，2，4，8 的元素，而不是像快速排序那样，局部顺序访问，所以，这样对 CPU 缓存是不友好的。\n\n- **第二点，对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序。**\n\n    我们在讲排序的时候，提过两个概念，有序度和逆序度。对于基于比较的排序算法来说，整个排序过程就是由两个基本的操作组成的，比较和交换（或移动）。快速排序数据交换的次数不会比逆序度多。\n\n    但是堆排序的第一步是建堆，建堆的过程会打乱数据原有的相对先后顺序，导致原数据的有序度降低。比如，对于一组已经有序的数据来说，经过建堆之后，数据反而变得更无序了。\n\n\n### 堆的应用\n\n优先级队列、求 Top K 问题和求中位数问题。\n\n优先级队列是一种特殊的队列，优先级高的数据先出队，而不再像普通的队列那样，先进先出。实际上，堆就可以看作优先级队列，只是称谓不一样罢了。求 Top K 问题又可以分为针对静态数据和针对动态数据，只需要利用一个堆，就可以做到非常高效率地查询 Top K 的数据。求中位数实际上还有很多变形，比如求 99 百分位数据、90 百分位数据等，处理的思路都是一样的，即利用两个堆，一个大顶堆，一个小顶堆，随着数据的动态添加，动态调整两个堆中的数据，最后大顶堆的堆顶元素就是要求的数据。\n\n## 图\n\n### 如何理解“图”？\n\n图中的元素我们就叫做**顶点**（vertex）。从我画的图中可以看出来，图中的一个顶点可以与任意其他顶点建立连接关系。我们把这种建立的关系叫做**边**（edge）。\n\n![Untitled](Assets/alg_39.png)\n\n我们生活中就有很多符合图这种结构的例子。比如，开篇问题中讲到的社交网络，就是一个非常典型的图结构。\n\n我们就拿微信举例子吧。我们可以把每个用户看作一个顶点。如果两个用户之间互加好友，那就在两者之间建立一条边。所以，整个微信的好友关系就可以用一张图来表示。其中，每个用户有多少个好友，对应到图中，就叫做顶点的**度**（degree），就是跟顶点相连接的边的条数。\n\n实际上，微博的社交关系跟微信还有点不一样，或者说更加复杂一点。微博允许单向关注，也就是说，用户 A 关注了用户 B，但用户 B 可以不关注用户 A。那我们如何用图来表示这种单向的社交关系呢？\n\n我们可以把刚刚讲的图结构稍微改造一下，引入边的“方向”的概念。\n\n如果用户 A 关注了用户 B，我们就在图中画一条从 A 到 B 的带箭头的边，来表示边的方向。如果用户 A 和用户 B 互相关注了，那我们就画一条从 A 指向 B 的边，再画一条从 B 指向 A 的边。我们把这种边有方向的图叫做“有向图”。以此类推，我们把边没有方向的图就叫做“无向图”。\n\n![Untitled](Assets/alg_40.png)\n\n我们刚刚讲过，无向图中有“度”这个概念，表示一个顶点有多少条边。在有向图中，我们把度分为**入度**（In-degree）和**出度**（Out-degree）。\n\n顶点的入度，表示有多少条边指向这个顶点；顶点的出度，表示有多少条边是以这个顶点为起点指向其他顶点。对应到微博的例子，入度就表示有多少粉丝，出度就表示关注了多少人。\n\n前面讲到了微信、微博、无向图、有向图，现在我们再来看另一种社交软件：QQ。\n\nQQ 中的社交关系要更复杂一点。不知道你有没有留意过 QQ 亲密度这样一个功能。QQ 不仅记录了用户之间的好友关系，还记录了两个用户之间的亲密度，如果两个用户经常往来，那亲密度就比较高；如果不经常往来，亲密度就比较低。如何在图中记录这种好友关系的亲密度呢？\n\n这里就要用到另一种图，**带权图**（weighted graph）。在带权图中，每条边都有一个权重（weight），我们可以通过这个权重来表示 QQ 好友间的亲密度。\n\n![Untitled](Assets/alg_41.png)\n\n### 邻接矩阵存储方法\n\n图最直观的一种存储方法就是，**邻接矩阵**（Adjacency Matrix）。\n\n邻接矩阵的底层依赖一个二维数组。对于无向图来说，如果顶点 i 与顶点 j 之间有边，我们就将 A[i][j]和 A[j][i]标记为 1；对于有向图来说，如果顶点 i 到顶点 j 之间，有一条箭头从顶点 i 指向顶点 j 的边，那我们就将 A[i][j]标记为 1。同理，如果有一条箭头从顶点 j 指向顶点 i 的边，我们就将 A[j][i]标记为 1。对于带权图，数组中就存储相应的权重。\n\n![Untitled](Assets/alg_42.png)\n\n用邻接矩阵来表示一个图，虽然简单、直观，但是比较浪费存储空间。\n\n### 邻接表存储方法\n\n针对上面邻接矩阵比较浪费内存空间的问题，我们来看另外一种图的存储方法，**邻接表**（Adjacency List）。\n\n每个顶点对应一条链表，链表中存储的是与这个顶点相连接的其他顶点。另外我需要说明一下，图中画的是一个有向图的邻接表存储方式，每个顶点对应的链表里面，存储的是指向的顶点。对于无向图来说，也是类似的，不过，每个顶点的链表中存储的，是跟这个顶点有边相连的顶点。\n\n![Untitled](Assets/alg_43.png)\n\n邻接矩阵存储起来比较浪费空间，但是使用起来比较节省时间。相反，邻接表存储起来比较节省空间，但是使用起来就比较耗时间。\n\n如果我们要确定，是否存在一条从顶点 2 到顶点 4 的边，那我们就要遍历顶点 2 对应的那条链表，看链表中是否存在顶点 4。而且，我们前面也讲过，链表的存储方式对缓存不友好。所以，比起邻接矩阵的存储方式，在邻接表中查询两个顶点之间的关系就没那么高效了。不过，如果链过长，我们也可以将邻接表中的链表改成平衡二叉查找树、红黑树、跳表等。\n\n## **Trie树**\n\n### 什么是“Trie 树”？\n\nTrie 树，也叫“字典树”。顾名思义，它是一个树形结构。它是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。\n\n我们有 6 个字符串，它们分别是：how，hi，her，hello，so，see。我们希望在里面多次查找某个字符串是否存在。如果每次查找，都是拿要查找的字符串跟这 6 个字符串依次进行字符串匹配，那效率就比较低，有没有更高效的方法呢？\n\n这个时候，我们就可以先对这 6 个字符串做一下预处理，组织成 Trie 树的结构，之后每次查找，都是在 Trie 树中进行匹配查找。**Trie 树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起。**最后构造出来的就是下面这个图中的样子。\n\n![Untitled](Assets/alg_44.png)\n\n其中，根节点不包含任何信息。每个节点表示一个字符串中的字符，从根节点到红色节点的一条路径表示一个字符串（注意：红色节点并不都是叶子节点）。\n\nTrie 树构造过程的每一步，都相当于往 Trie 树中插入一个字符串。当所有字符串都插入完成之后，Trie 树就构造好了。\n\n![Untitled](Assets/alg_45.png)\n\n![Untitled](Assets/alg_46.png)\n\n### Trie 树真的很耗内存吗？\n\n“Trie 树是非常耗内存的，用的是一种空间换时间的思路”。\n\n刚刚我们在讲 Trie 树的实现的时候，讲到用数组来存储一个节点的子节点的指针。如果字符串中包含从 a 到 z 这 26 个字符，那每个节点都要存储一个长度为 26 的数组，并且每个数组元素要存储一个 8 字节指针（或者是 4 字节，这个大小跟 CPU、操作系统、编译器等有关）。而且，即便一个节点只有很少的子节点，远小于 26 个，比如 3、4 个，我们也要维护一个长度为 26 的数组。\n\n我们前面讲过，Trie 树的本质是避免重复存储一组字符串的相同前缀子串，但是现在每个字符（对应一个节点）的存储远远大于 1 个字节。按照我们上面举的例子，数组长度为 26，每个元素是 8 字节，那每个节点就会额外需要 26*8=208 个字节。而且这还是只包含 26 个字符的情况。\n\n如果字符串中不仅包含小写字母，还包含大写字母、数字、甚至是中文，那需要的存储空间就更多了。所以，也就是说，在某些情况下，Trie 树不一定会节省存储空间。在重复的前缀并不多的情况下，Trie 树不但不能节省内存，还有可能会浪费更多的内存。\n\n我们可以稍微牺牲一点查询的效率，将每个节点中的数组换成其他数据结构，来存储一个节点的子节点指针。用哪种数据结构呢？我们的选择其实有很多，比如有序数组、跳表、散列表、红黑树等。\n\n### Trie 树与散列表、红黑树的比较\n\n在一组字符串中查找字符串，Trie 树实际上表现得并不好。它对要处理的字符串有极其严苛的要求。\n\n第一，字符串中包含的字符集不能太大。我们前面讲到，如果字符集太大，那存储空间可能就会浪费很多。即便可以优化，但也要付出牺牲查询、插入效率的代价。\n\n第二，要求字符串的前缀重合比较多，不然空间消耗会变大很多。\n\n第三，如果要用 Trie 树解决问题，那我们就要自己从零开始实现一个 Trie 树，还要保证没有 bug，这个在工程上是将简单问题复杂化，除非必须，一般不建议这样做。\n\n第四，我们知道，通过指针串起来的数据块是不连续的，而 Trie 树中用到了指针，所以，对缓存并不友好，性能上会打个折扣。\n\n综合这几点，针对在一组字符串中查找字符串的问题，我们在工程中，更倾向于用散列表或者红黑树。因为这两种数据结构，我们都不需要自己去实现，直接利用编程语言中提供的现成类库就行了。\n\nTrie 树只是不适合精确匹配查找，这种问题更适合用散列表或者红黑树来解决。Trie 树比较适合的是查找前缀匹配的字符串。\n\n对于用多模式串匹配实现敏感词过滤功能，还可以用AC自动机实现。\n\n# 算法\n\n## **递归**\n\n从我自己学习数据结构和算法的经历来看，我个人觉得，有两个最难理解的知识点，一个是动态规划，另一个就是递归。\n\n基本上，所有的递归问题都可以用递推公式来表示。比如，周末你带着女朋友去电影院看电影，女朋友问你，咱们现在坐在第几排啊？电影院里面太黑了，看不清，没法数，现在你怎么办？于是你就问前面一排的人他是第几排，你想只要在他的数字上加一，就知道自己在哪一排了。但是，前面的人也看不清啊，所以他也问他前面的人。就这样一排一排往前问，直到问到第一排的人，说我在第一排，然后再这样一排一排再把数字传回来。直到你前面的人告诉你他在哪一排，于是你就知道答案了。\n\n这就是一个非常标准的递归求解问题的分解过程，去的过程叫“递”，回来的过程叫“归”。基本上，所有的递归问题都可以用递推公式来表示。刚刚这个生活中的例子，我们用递推公式将它表示出来就是这样的：\n\n```\nf(n)=f(n-1)+1 其中，f(1)=1\n```\n\nf(n) 表示你想知道自己在哪一排，f(n-1) 表示前面一排所在的排数，f(1)=1 表示第一排的人知道自己在第一排。有了这个递推公式，我们就可以很轻松地将它改为递归代码，如下：\n\n```c\nint f(int n) {\n  if (n == 1) return 1;\n  return f(n-1) + 1;\n}\n```\n\n### 递归需要满足的三个条件\n\n1. **一个问题的解可以分解为几个子问题的解**\n\n    何为子问题？子问题就是数据规模更小的问题。\n\n2. **这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样**\n\n    比如电影院那个例子，你求解“自己在哪一排”的思路，和前面一排人求解“自己在哪一排”的思路，是一模一样的。\n\n3. **存在递归终止条件**\n\n    把问题分解为子问题，把子问题再分解为子子问题，一层一层分解下去，不能存在无限循环，这就需要有终止条件。\n\n\n### 如何编写递归代码？\n\n写递归代码最关键的是**写出递推公式，找到终止条件**，剩下将递推公式转化为代码就很简单了。\n\n假如这里有 n 个台阶，每次你可以跨 1 个台阶或者 2 个台阶，请问走这 n 个台阶有多少种走法？如果有 7 个台阶，你可以 2，2，2，1 这样子上去，也可以 1，2，1，1，2 这样子上去，总之走法有很多，那如何用编程求得总共有多少种走法呢？\n\n我们仔细想下，实际上，可以根据第一步的走法把所有走法分为两类，第一类是第一步走了 1 个台阶，另一类是第一步走了 2 个台阶。所以 n 个台阶的走法就等于先走 1 阶后，n-1 个台阶的走法 加上先走 2 阶后，n-2 个台阶的走法。用公式表示就是：\n\n```c\nf(n) = f(n-1)+f(n-2)\n```\n\n有了递推公式，递归代码基本上就完成了一半。我们再来看下终止条件。当有一个台阶时，我们不需要再继续递归，就只有一种走法。所以 f(1)=1。而f(2)=2 表示走 2 个台阶，有两种走法，一步走完或者分两步来走。\n\n所以，递归终止条件就是 f(1)=1，f(2)=2。我们把递归终止条件和刚刚得到的递推公式放到一起就是这样的：\n\n```c\nf(1) = 1;\nf(2) = 2;\nf(n) = f(n-1)+f(n-2)\n```\n\n有了这个公式，我们转化成递归代码就简单多了。最终的递归代码是这样的：\n\n```c\nint f(int n) {\n  if (n == 1) return 1;\n  if (n == 2) return 2;\n  return f(n-1) + f(n-2);\n}\n```\n\n**写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。**\n\n**编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。**\n\n### 递归代码要警惕堆栈溢出\n\n在实际的软件开发中，编写递归代码时，我们会遇到很多问题，比如堆栈溢出。而堆栈溢出会造成系统性崩溃，后果会非常严重。如何避免出现堆栈溢出呢？\n\n我们可以通过在代码中限制递归调用的最大深度的方式来解决这个问题。递归调用超过一定深度（比如 1000）之后，我们就不继续往下再递归了，直接返回报错。\n\n但这种做法并不能完全解决问题，因为最大允许的递归深度跟当前线程剩余的栈空间大小有关，事先无法计算。如果实时计算，代码过于复杂，就会影响代码的可读性。所以，如果最大深度比较小，比如 10、50，就可以用这种方法，否则这种方法并不是很实用。\n\n### 递归代码要警惕重复计算\n\n除此之外，使用递归时还会出现重复计算的问题。刚才我讲的第二个递归代码的例子，如果我们把整个递归过程分解一下的话，那就是这样的：\n\n![Untitled](Assets/alg_47.png)\n\n从图中，我们可以直观地看到，想要计算 f(5)，需要先计算 f(4) 和 f(3)，而计算 f(4) 还需要计算 f(3)，因此，f(3) 就被计算了很多次，这就是重复计算问题。\n\n为了避免重复计算，我们可以通过一个数据结构（比如散列表）来保存已经求解过的 f(k)。当递归调用到 f(k) 时，先看下是否已经求解过了。如果是，则直接从散列表中取值返回，不需要重复计算，这样就能避免刚讲的问题了。\n\n在时间效率上，递归代码里多了很多函数调用，当这些函数调用的数量较大时，就会积聚成一个可观的时间成本。在空间复杂度上，因为递归调用一次就会在内存栈中保存一次现场数据，所以在分析递归代码空间复杂度时，需要额外考虑这部分的开销，比如我们前面讲到的电影院递归代码，空间复杂度并不是 O(1)，而是 O(n)。\n\n## **排序**\n\n排序算法太多了，这里只讲众多排序算法中的一小撮，也是最经典的、最常用的：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序。按照时间复杂度把它们分成了三类。\n\n![Untitled](Assets/alg_48.png)\n\n### 如何分析一个“排序算法”？\n\n- **排序算法的执行效率**\n    1. **最好情况、最坏情况、平均情况时间复杂度**\n\n        我们在分析排序算法的时间复杂度时，要分别给出最好情况、最坏情况、平均情况下的时间复杂度。除此之外，你还要说出最好、最坏时间复杂度对应的要排序的原始数据是什么样的。\n\n        为什么要区分这三种时间复杂度呢？第一，有些排序算法会区分，为了好对比，所以我们最好都做一下区分。第二，对于要排序的数据，有的接近有序，有的完全无序。有序度不同的数据，对于排序的执行时间肯定是有影响的，我们要知道排序算法在不同数据下的性能表现。\n\n    2. **时间复杂度的系数、常数 、低阶**\n\n        我们知道，时间复杂度反映的是数据规模 n 很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低阶。但是实际的软件开发中，我们排序的可能是 10 个、100 个、1000 个这样规模很小的数据，所以，在对同一阶时间复杂度的排序算法性能对比的时候，我们就要把系数、常数、低阶也考虑进来。\n\n    3. **比较次数和交换（或移动）次数**\n\n        基于比较的排序算法的执行过程，会涉及两种操作，一种是元素比较大小，另一种是元素交换或移动。所以，如果我们在分析排序算法的执行效率的时候，应该把比较次数和交换（或移动）次数也考虑进去。\n\n- **排序算法的内存消耗**\n\n    算法的内存消耗可以通过空间复杂度来衡量，排序算法也不例外。不过，针对排序算法的空间复杂度，我们还引入了一个新的概念，原地排序（Sorted in place）。原地排序算法，就是特指空间复杂度是 O(1) 的排序算法。\n\n- **排序算法的稳定性**\n\n    仅仅用执行效率和内存消耗来衡量排序算法的好坏是不够的。针对排序算法，我们还有一个重要的度量指标，**稳定性**。这个概念是说，如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。\n\n\n### 冒泡排序（Bubble Sort）\n\n冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作。\n\n现在，结合分析排序算法的三个方面，有三个问题。\n\n- **第一，冒泡排序是原地排序算法吗？**\n\n    冒泡的过程只涉及相邻数据的交换操作，只需要常量级的临时空间，所以它的空间复杂度为 O(1)，是一个原地排序算法。\n\n- **第二，冒泡排序是稳定的排序算法吗？**\n\n    在冒泡排序中，只有交换才可以改变两个元素的前后顺序。为了保证冒泡排序算法的稳定性，当有相邻的两个元素大小相等的时候，我们不做交换，相同大小的数据在排序前后不会改变顺序，所以冒泡排序是稳定的排序算法。\n\n- **第三，冒泡排序的时间复杂度是多少？**\n\n最好情况下，要排序的数据已经是有序的了，我们只需要进行一次冒泡操作，就可以结束了，所以最好情况时间复杂度是 O(n)。而最坏的情况是，要排序的数据刚好是倒序排列的，我们需要进行 n 次冒泡操作，所以最坏情况时间复杂度为 O(n2)。\n\n![Untitled](Assets/alg_49.png)\n\n最好、最坏情况下的时间复杂度很容易分析，那平均情况下的时间复杂是多少呢？我们前面讲过，平均时间复杂度就是加权平均期望时间复杂度，分析的时候要结合概率论的知识。\n\n对于包含 n 个数据的数组，这 n 个数据就有 n! 种排列方式。不同的排列方式，冒泡排序执行的时间肯定是不同的。比如我们前面举的那两个例子，其中一个要进行 6 次冒泡，而另一个只需要 4 次。如果用概率论方法定量分析平均时间复杂度，涉及的数学推理和计算就会很复杂。我这里还有一种思路，通过“**有序度**”和“**逆序度**”这两个概念来进行分析。\n\n**有序度**是数组中具有有序关系的元素对的个数。有序元素对用数学表达式表示就是这样：\n\n```c\n有序元素对：a[i] \u003c= a[j], 如果i \u003c j。\n```\n\n![Untitled](Assets/alg_50.png)\n\n同理，对于一个倒序排列的数组，比如 6，5，4，3，2，1，有序度是 0；对于一个完全有序的数组，比如 1，2，3，4，5，6，有序度就是 **n*(n-1)/2**，也就是 15。我们把这种完全有序的数组的有序度叫作**满有序度**。\n\n逆序度的定义正好跟有序度相反（默认从小到大为有序）。关于这三个概念，我们还可以得到一个公式：**逆序度 = 满有序度 - 有序度**。我们排序的过程就是一种增加有序度，减少逆序度的过程，最后达到满有序度，就说明排序完成了。\n\n我还是拿前面举的那个冒泡排序的例子来说明。要排序的数组的初始状态是 4，5，6，3，2，1 ，其中，有序元素对有 (4，5) (4，6)(5，6)，所以有序度是 3。n=6，所以排序完成之后终态的满有序度为 n*(n-1)/2=15。\n\n冒泡排序包含两个操作原子，比较和交换。每交换一次，有序度就加 1。不管算法怎么改进，交换次数总是确定的，即为**逆序度，也就是n*(n-1)/2–初始有序度**。此例中就是 15–3=12，要进行 12 次交换操作。\n\n对于包含 n 个数据的数组进行冒泡排序，平均交换次数是多少呢？最坏情况下，初始状态的有序度是 0，所以要进行 n*(n-1)/2 次交换。最好情况下，初始状态的有序度是 n*(n-1)/2，就不需要进行交换。我们可以取个中间值 n*(n-1)/4，来表示初始有序度既不是很高也不是很低的平均情况。\n\n换句话说，平均情况下，需要 n*(n-1)/4 次交换操作，比较操作肯定要比交换操作多，而复杂度的上限是 O(n2)，所以平均情况下的时间复杂度就是 O(n2)。\n\n### 插入排序（Insertion Sort）\n\n一个有序的数组，我们往里面添加一个新的数据后，如何继续保持数据有序呢？很简单，我们只要遍历数组，找到数据应该插入的位置将其插入即可。\n\n![Untitled](Assets/alg_51.png)\n\n这是一个动态排序的过程，即动态地往有序集合中添加数据，我们可以通过这种方法保持集合中的数据一直有序。而对于一组静态数据，我们也可以借鉴上面讲的插入方法，来进行排序，于是就有了插入排序算法。\n\n那插入排序具体是如何借助上面的思想来实现排序的呢？\n\n首先，我们将数组中的数据分为两个区间，**已排序区间**和**未排序区间**。初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。\n\n如图所示，要排序的数据是 4，5，6，1，3，2，其中左侧为已排序区间，右侧是未排序区间。\n\n![Untitled](Assets/alg_52.png)\n\n插入排序也包含两种操作，一种是元素的比较，一种是元素的移动。当我们需要将一个数据 a 插入到已排序区间时，需要拿 a 与已排序区间的元素依次比较大小，找到合适的插入位置。找到插入点之后，我们还需要将插入点之后的元素顺序往后移动一位，这样才能腾出位置给元素 a 插入。\n\n对于不同的查找插入点方法（从头到尾、从尾到头），元素的比较次数是有区别的。但对于一个给定的初始序列，移动操作的次数总是固定的，就等于逆序度。\n\n- **第一，插入排序是原地排序算法吗？**\n\n    从实现过程可以很明显地看出，插入排序算法的运行并不需要额外的存储空间，所以空间复杂度是 O(1)，也就是说，这是一个原地排序算法。\n\n- **第二，插入排序是稳定的排序算法吗？**\n\n    在插入排序中，对于值相同的元素，我们可以选择将后面出现的元素，插入到前面出现元素的后面，这样就可以保持原有的前后顺序不变，所以插入排序是稳定的排序算法。\n\n- **第三，插入排序的时间复杂度是多少？**\n\n    对于插入排序来说，每次插入操作都相当于在数组中插入一个数据，循环执行 n 次插入操作，所以平均时间复杂度为 O(n2)。\n\n\n### 选择排序（Selection Sort）\n\n选择排序算法的实现思路有点类似插入排序，也分已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。\n\n![Untitled](Assets/alg_53.png)\n\n首先，选择排序空间复杂度为 O(1)，是一种原地排序算法。选择排序的最好情况时间复杂度、最坏情况和平均情况时间复杂度都为 O(n2)。\n\n那选择排序是稳定的排序算法吗？答案是否定的，选择排序是一种不稳定的排序算法。从前面的那张图中，可以看出来，选择排序每次都要找剩余未排序元素中的最小值，并和前面的元素交换位置，这样破坏了稳定性。\n\n### 为什么插入排序要比冒泡排序更受欢迎呢？\n\n冒泡排序不管怎么优化，元素交换的次数是一个固定值，是原始数据的逆序度。插入排序是同样的，不管怎么优化，元素移动的次数也等于原始数据的逆序度。\n\n但是，从代码实现上来看，冒泡排序的数据交换要比插入排序的数据移动要复杂，冒泡排序需要 3 个赋值操作，而插入排序只需要 1 个。我们来看这段操作：\n\n```c\n// 冒泡排序中数据的交换操作：\nif (a[j] \u003e a[j+1]) { // 交换\n   int tmp = a[j];\n   a[j] = a[j+1];\n   a[j+1] = tmp;\n   flag = true;\n}\n\n// 插入排序中数据的移动操作：\nif (a[j] \u003e value) {\n  a[j+1] = a[j];  // 数据移动\n} else {\n  break;\n}\n```\n\n我们把执行一个赋值语句的时间粗略地计为单位时间（unit_time），然后分别用冒泡排序和插入排序对同一个逆序度是 K 的数组进行排序。用冒泡排序，需要 K 次交换操作，每次需要 3 个赋值语句，所以交换操作总耗时就是 3*K 单位时间。而插入排序中数据移动操作只需要 K 个单位时间。\n\n![Untitled](Assets/alg_54.png)\n\n### 归并排序\n\n归并排序的核心思想还是蛮简单的。如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。\n\n![Untitled](Assets/alg_55.png)\n\n归并排序使用的就是**分治思想**。分治，顾名思义，就是分而治之，将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了。\n\n分治思想跟我们前面讲的递归思想很像。是的，分治算法一般都是用递归来实现的。**分治是一种解决问题的处理思想，递归是一种编程技巧**，这两者并不冲突。\n\n写递归代码的技巧就是，分析得出递推公式，然后找到终止条件，最后将递推公式翻译成递归代码。所以，要想写出归并排序的代码，我们先写出归并排序的递推公式。\n\n```c\n递推公式：\nmerge_sort(p…r) = merge(merge_sort(p…q), merge_sort(q+1…r))\n\n终止条件：\np \u003e= r 不用再继续分解\n```\n\n```c\n// 归并排序算法, A是数组，n表示数组大小\nmerge_sort(A, n) {\n  merge_sort_c(A, 0, n-1)\n}\n\n// 递归调用函数\nmerge_sort_c(A, p, r) {\n  // 递归终止条件\n  if p \u003e= r  then return\n\n  // 取p到r之间的中间位置q\n  q = (p+r) / 2\n  // 分治递归\n  merge_sort_c(A, p, q)\n  merge_sort_c(A, q+1, r)\n  // 将A[p...q]和A[q+1...r]合并为A[p...r]\n  merge(A[p...r], A[p...q], A[q+1...r])\n}\n```\n\n你可能已经发现了，merge(A[p...r], A[p...q], A[q+1...r]) 这个函数的作用就是，将已经有序的 A[p...q]和 A[q+1....r]合并成一个有序的数组，并且放入 A[p....r]。那这个过程具体该如何做呢？\n\n如图所示，我们申请一个临时数组 tmp，大小与 A[p...r]相同。我们用两个游标 i 和 j，分别指向 A[p...q]和 A[q+1...r]的第一个元素。比较这两个元素 A[i]和 A[j]，如果 A[i]\u003c=A[j]，我们就把 A[i]放入到临时数组 tmp，并且 i 后移一位，否则将 A[j]放入到数组 tmp，j 后移一位。\n\n继续上述比较过程，直到其中一个子数组中的所有数据都放入临时数组中，再把另一个数组中的数据依次加入到临时数组的末尾，这个时候，临时数组中存储的就是两个子数组合并之后的结果了。最后再把临时数组 tmp 中的数据拷贝到原数组 A[p...r]中。\n\n![Untitled](Assets/alg_56.png)\n\n```c\nmerge(A[p...r], A[p...q], A[q+1...r]) {\n  var i := p，j := q+1，k := 0 // 初始化变量i, j, k\n  var tmp := new array[0...r-p] // 申请一个大小跟A[p...r]一样的临时数组\n  while i\u003c=q AND j\u003c=r do {\n    if A[i] \u003c= A[j] {\n      tmp[k++] = A[i++] // i++等于i:=i+1\n    } else {\n      tmp[k++] = A[j++]\n    }\n  }\n\n  // 判断哪个子数组中有剩余的数据\n  var start := i，end := q\n  if j\u003c=r then start := j, end:=r\n\n  // 将剩余的数据拷贝到临时数组tmp\n  while start \u003c= end do {\n    tmp[k++] = A[start++]\n  }\n\n  // 将tmp中的数组拷贝回A[p...r]\n  for i:=0 to r-p do {\n    A[p+i] = tmp[i]\n  }\n}\n```\n\n- **归并排序的性能分析**\n    - **第一，归并排序是稳定的排序算法吗？**\n\n        结合我前面画的那张图和归并排序的伪代码，你应该能发现，归并排序稳不稳定关键要看 merge() 函数，也就是两个有序子数组合并成一个有序数组的那部分代码。\n\n        在合并的过程中，如果 A[p...q]和 A[q+1...r]之间有值相同的元素，那我们可以像伪代码中那样，先把 A[p...q]中的元素放入 tmp 数组。这样就保证了值相同的元素，在合并前后的先后顺序不变。所以，归并排序是一个稳定的排序算法。\n\n    - **第二，归并排序的时间复杂度是多少？**\n\n        在递归那一节我们讲过，递归的适用场景是，一个问题 a 可以分解为多个子问题 b、c，那求解问题 a 就可以分解为求解问题 b、c。问题 b、c 解决之后，我们再把 b、c 的结果合并成 a 的结果。\n\n        如果我们定义求解问题 a 的时间是 T(a)，求解问题 b、c 的时间分别是 T(b) 和 T( c)，那我们就可以得到这样的递推关系式：\n\n        ```c\n        T(a) = T(b) + T(c) + K\n        ```\n\n        其中 K 等于将两个子问题 b、c 的结果合并成问题 a 的结果所消耗的时间。\n\n        从刚刚的分析，我们可以得到一个重要的结论：**不仅递归求解的问题可以写成递推公式，递归代码的时间复杂度也可以写成递推公式。**\n\n        套用这个公式，我们来分析一下归并排序的时间复杂度。我们假设对 n 个元素进行归并排序需要的时间是 T(n)，那分解成两个子数组排序的时间都是 T(n/2)。我们知道，merge() 函数合并两个有序子数组的时间复杂度是 O(n)。所以，套用前面的公式，归并排序的时间复杂度的计算公式就是：\n\n        ```c\n        T(1) = C；   n=1时，只需要常量级的执行时间，所以表示为C。\n        T(n) = 2*T(n/2) + n； n\u003e1\n\n        T(n) = 2*T(n/2) + n\n             = 2*(2*T(n/4) + n/2) + n = 4*T(n/4) + 2*n\n             = 4*(2*T(n/8) + n/4) + 2*n = 8*T(n/8) + 3*n\n             = 8*(2*T(n/16) + n/8) + 3*n = 16*T(n/16) + 4*n\n             ......\n             = 2^k * T(n/2^k) + k * n\n             ......\n        ```\n\n        通过这样一步一步分解推导，我们可以得到 T(n) = 2^kT(n/2^k)+kn。当 T(n/2^k)=T(1) 时，也就是 n/2^k=1，我们得到 k=log2n 。我们将 k 值代入上面的公式，得到 T(n)=Cn+nlog2n 。如果我们用大 O 标记法来表示的话，T(n) 就等于 O(nlogn)。**所以归并排序的时间复杂度是 O(nlogn)。**\n\n        从我们的原理分析和伪代码可以看出，归并排序的执行效率与要排序的原始数组的有序程度无关，所以其时间复杂度是非常稳定的，不管是最好情况、最坏情况，还是平均情况，时间复杂度都是 O(nlogn)。\n\n    - **第三，归并排序的空间复杂度是多少？**\n\n        归并排序的时间复杂度任何情况下都是 O(nlogn)，看起来非常优秀。（即便是快速排序，最坏情况下，时间复杂度也是 O(n2)。）但是，归并排序并没有像快排那样，应用广泛，因为它有一个致命的“弱点”，那就是归并排序不是原地排序算法。\n\n        在任意时刻，CPU 只会有一个函数在执行，也就只会有一个临时的内存空间在使用。临时内存空间最大也不会超过 n 个数据的大小，所以空间复杂度是 O(n)。\n\n\n### 快速排序\n\n快排的思想是这样的：如果要排序数组中下标从 p 到 r 之间的一组数据，我们选择 p 到 r 之间的任意一个数据作为 pivot（分区点）。\n\n我们遍历 p 到 r 之间的数据，将小于 pivot 的放到左边，将大于 pivot 的放到右边，将 pivot 放到中间。经过这一步骤之后，数组 p 到 r 之间的数据就被分成了三个部分，前面 p 到 q-1 之间都是小于 pivot 的，中间是 pivot，后面的 q+1 到 r 之间是大于 pivot 的。\n\n![Untitled](Assets/alg_57.png)\n\n根据分治、递归的处理思想，我们可以用递归排序下标从 p 到 q-1 之间的数据和下标从 q+1 到 r 之间的数据，直到区间缩小为 1，就说明所有的数据都有序了。\n\n如果我们用递推公式来将上面的过程写出来的话，就是这样：\n\n```c\n递推公式：\nquick_sort(p…r) = quick_sort(p…q-1) + quick_sort(q+1… r)\n\n终止条件：\np \u003e= r\n```\n\n我将递推公式转化成递归代码。跟归并排序一样，我还是用伪代码来实现，你可以翻译成你熟悉的任何语言。\n\n```c\n// 快速排序，A是数组，n表示数组的大小\nquick_sort(A, n) {\n  quick_sort_c(A, 0, n-1)\n}\n// 快速排序递归函数，p,r为下标\nquick_sort_c(A, p, r) {\n  if p \u003e= r then return\n\n  q = partition(A, p, r) // 获取分区点\n  quick_sort_c(A, p, q-1)\n  quick_sort_c(A, q+1, r)\n}\n```\n\n原地分区函数的实现思路非常巧妙，我写成了伪代码，我们一起来看一下。\n\n```c\npartition(A, p, r) {\n  pivot := A[r]\n  i := p\n  for j := p to r-1 do {\n    if A[j] \u003c pivot {\n      swap A[i] with A[j]\n      i := i+1\n    }\n  }\n  swap A[i] with A[r]\n  return i\n```\n\n我们通过游标 i 把 A[p...r-1]分成两部分。A[p...i-1]的元素都是小于 pivot 的，我们暂且叫它“已处理区间”，A[i...r-1]是“未处理区间”。我们每次都从未处理的区间 A[i...r-1]中取一个元素 A[j]，与 pivot 对比，如果小于 pivot，则将其加入到已处理区间的尾部，也就是 A[i]的位置。\n\n![Untitled](Assets/alg_58.png)\n\n因为分区的过程涉及交换操作，如果数组中有两个相同的元素，比如序列 6，8，7，6，3，5，9，4，在经过第一次分区操作之后，两个 6 的相对先后顺序就会改变。所以，**快速排序并不是一个稳定的排序算法。**\n\n到此，快速排序的原理你应该也掌握了。现在，我再来看另外一个问题：快排和归并用的都是分治思想，递推公式和递归代码也非常相似，那它们的区别在哪里呢？\n\n![Untitled](Assets/alg_59.png)\n\n可以发现，归并排序的处理过程是由下到上的，先处理子问题，然后再合并。而快排正好相反，它的处理过程是由上到下的，先分区，然后再处理子问题。\n\n- **快速排序的性能分析**\n\n快排也是用递归来实现的。对于递归代码的时间复杂度，我前面总结的公式，这里也还是适用的。如果每次分区操作，都能正好把数组分成大小接近相等的两个小区间，那快排的时间复杂度递推求解公式跟归并是相同的。所以，快排的时间复杂度也是 O(nlogn)。\n\n```c\nT(1) = C；   n=1时，只需要常量级的执行时间，所以表示为C。\nT(n) = 2*T(n/2) + n； n\u003e1\n```\n\n但是，公式成立的前提是每次分区操作，我们选择的 pivot 都很合适，正好能将大区间对等地一分为二。但实际上这种情况是很难实现的。\n\n举一个比较极端的例子。如果数组中的数据原来已经是有序的了，比如 1，3，5，6，8。如果我们每次选择最后一个元素作为 pivot，那每次分区得到的两个区间都是不均等的。我们需要进行大约 n 次分区操作，才能完成快排的整个过程。每次分区我们平均要扫描大约 n/2 个元素，这种情况下，快排的时间复杂度就从 O(nlogn) 退化成了 O(n2)。\n\nT(n) 在大部分情况下的时间复杂度都可以做到 O(nlogn)，只有在极端情况下，才会退化到 O(n2)。而且，我们也有很多方法将这个概率降到很低。\n\n## **线性排序**\n\n三种时间复杂度是 O(n) 的排序算法：桶排序、计数排序、基数排序。因为这些排序算法的时间复杂度是线性的，所以我们把这类排序算法叫作线性排序（Linear sort）。之所以能做到线性的时间复杂度，主要原因是，这三个算法是非基于比较的排序算法，都不涉及元素之间的比较操作。\n\n### 桶排序（Bucket sort）\n\n桶排序，顾名思义，会用到“桶”，核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。\n\n![Untitled](Assets/alg_60.png)\n\n如果要排序的数据有 n 个，我们把它们均匀地划分到 m 个桶内，每个桶里就有 k=n/m 个元素。每个桶内部使用快速排序，时间复杂度为 O(k * logk)。m 个桶排序的时间复杂度就是 O(m * k * logk)，因为 k=n/m，所以整个桶排序的时间复杂度就是 O(n*log(n/m))。当桶的个数 m 接近数据个数 n 时，log(n/m) 就是一个非常小的常量，这个时候桶排序的时间复杂度接近 O(n)。\n\n**桶排序看起来很优秀，那它是不是可以替代我们之前讲的排序算法呢？**\n\n答案当然是否定的。实际上，桶排序对要排序数据的要求是非常苛刻的。\n\n首先，要排序的数据需要很容易就能划分成 m 个桶，并且，桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序。\n\n其次，数据在各个桶之间的分布是比较均匀的。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不平均，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为 O(nlogn) 的排序算法了。\n\n**桶排序比较适合用在外部排序中。**所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。\n\n### 计数排序（Counting sort）\n\n**计数排序其实是桶排序的一种特殊情况。**当要排序的 n 个数据，所处的范围并不大的时候，比如最大值是 k，我们就可以把数据划分成 k 个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。因为只涉及扫描遍历操作，所以时间复杂度是 O(n)。\n\n**计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。**\n\n### 基数排序（Radix sort）\n\n假设我们有 10 万个手机号码，希望将这 10 万个手机号码从小到大排序，你有什么比较快速的排序方法呢？\n\n刚刚这个问题里有这样的规律：假设要比较两个手机号码 a，b 的大小，如果在前面几位中，a 手机号码已经比 b 手机号码大了，那后面的几位就不用看了。\n\n借助稳定排序算法，这里有一个巧妙的实现思路。先按照最后一位来排序手机号码，然后，再按照倒数第二位重新排序，以此类推，最后按照第一位重新排序。经过 11 次排序之后，手机号码就都有序了。\n\n手机号码稍微有点长，画图比较不容易看清楚，我用字符串排序的例子，画了一张基数排序的过程分解图。\n\n![Untitled](Assets/alg_61.png)\n\n根据每一位来排序，我们可以用刚讲过的桶排序或者计数排序，它们的时间复杂度可以做到 O(n)。如果要排序的数据有 k 位，那我们就需要 k 次桶排序或者计数排序，总的时间复杂度是 O(k*n)。当 k 不大的时候，比如手机号码排序的例子，k 最大就是 11，所以基数排序的时间复杂度就近似于 O(n)。\n\n有时候要排序的数据并不都是等长的，但我们可以把所有的单词补齐到相同长度，位数不够的可以在后面补“0”，因为根据ASCII 值，所有字母都大于“0”，所以补“0”不会影响到原有的大小顺序。这样就可以继续用基数排序了。\n\n总结一下，**基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到 O(n) 了。**\n\n## **排序优化**\n\n### **如何选择合适的排序算法？**\n\n![Untitled](Assets/alg_62.png)\n\n线性排序算法的时间复杂度比较低，适用场景比较特殊。所以如果要写一个通用的排序函数，不能选择线性排序算法。\n\n如果对小规模数据进行排序，可以选择时间复杂度是 O(n2) 的算法；如果对大规模数据进行排序，时间复杂度是 O(nlogn) 的算法更加高效。所以，为了兼顾任意规模数据的排序，一般都会首选时间复杂度是 O(nlogn) 的排序算法来实现排序函数。\n\n时间复杂度是 O(nlogn) 的排序算法不止一个，我们已经讲过的有归并排序、快速排序，后面讲堆的时候我们还会讲到堆排序。堆排序和快速排序都有比较多的应用，比如 Java 语言采用堆排序实现排序函数，C 语言使用快速排序实现排序函数。\n\n### **如何优化快速排序？**\n\n如果数据原来就是有序的或者接近有序的，每次分区点都选择最后一个数据，那快速排序算法就会变得非常糟糕，时间复杂度就会退化为 O(n2)。实际上，**这种 O(n2) 时间复杂度出现的主要原因还是因为我们分区点选得不够合理。**\n\n最理想的分区点是：**被分区点分开的两个分区中，数据的数量差不多。**\n\n这里介绍两个比较常用、比较简单的分区算法。\n\n1. **三数取中法**\n\n    我们从区间的首、尾、中间，分别取出一个数，然后对比大小，取这 3 个数的中间值作为分区点。这样每间隔某个固定的长度，取数据出来比较，将中间值作为分区点的分区算法，肯定要比单纯取某一个数据更好。但是，如果要排序的数组比较大，那“三数取中”可能就不够了，可能要“五数取中”或者“十数取中”。\n\n2. **随机法**\n\n    随机法就是每次从要排序的区间中，随机选择一个元素作为分区点。这种方法并不能保证每次分区点都选的比较好，但是从概率的角度来看，也不大可能会出现每次分区点都选得很差的情况，所以平均情况下，这样选的分区点是比较好的。时间复杂度退化为最糟糕的 O(n2) 的情况，出现的可能性不大。\n\n\n我们知道，快速排序是用递归来实现的。我们在递归那一节讲过，递归要警惕堆栈溢出。为了避免快速排序里，递归过深而堆栈过小，导致堆栈溢出，我们有两种解决办法：第一种是限制递归深度。一旦递归过深，超过了我们事先设定的阈值，就停止递归。第二种是通过在堆上模拟实现一个函数调用栈，手动模拟递归压栈、出栈的过程，这样就没有了系统栈大小的限制。\n\n## **二分查找**\n\n二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为 0。\n\n### **O(logn) 惊人的查找速度**\n\n我们假设数据大小是 n，每次查找后数据都会缩小为原来的一半，也就是会除以 2。最坏情况下，直到查找区间被缩小为空，才停止。\n\n![Untitled](Assets/alg_63.png)\n\n可以看出来，这是一个等比数列。其中 n/2k=1 时，k 的值就是总共缩小的次数。而每一次缩小操作只涉及两个数据的大小比较，所以，经过了 k 次区间缩小操作，时间复杂度就是 O(k)。通过 n/2k=1，我们可以求得 k=log2n，所以时间复杂度就是 O(logn)。\n\n### **二分查找的递归与非递归实现**\n\n最简单的情况就是有序数组中不存在重复元素。\n\n```java\npublic int bsearch(int[] a, int n, int value) {\n  int low = 0;\n  int high = n - 1;\n\n  while (low \u003c= high) {\n    int mid = (low + high) / 2;\n    if (a[mid] == value) {\n      return mid;\n    } else if (a[mid] \u003c value) {\n      low = mid + 1;\n    } else {\n      high = mid - 1;\n    }\n  }\n\n  return -1;\n}\n```\n\n着重强调一下容易出错的 3 个地方。\n\n1. **循环退出条件**\n\n    注意是 low\u003c=high，而不是 low\u003chigh。\n\n2. **mid 的取值**\n\n    实际上，mid=(low+high)/2 这种写法是有问题的。因为如果 low 和 high 比较大的话，两者之和就有可能会溢出。改进的方法是将 mid 的计算方式写成 low+(high-low)/2。更进一步，如果要将性能优化到极致的话，我们可以将这里的除以 2 操作转化成位运算 low+((high-low)\u003e\u003e1)。因为相比除法运算来说，计算机处理位运算要快得多。\n\n3. **low 和 high 的更新**\n\n    low=mid+1，high=mid-1。注意这里的 +1 和 -1，如果直接写成 low=mid 或者 high=mid，就可能会发生死循环。比如，当 high=3，low=3 时，如果 a[3]不等于 value，就会导致一直循环不退出。\n\n\n---\n\n实际上，二分查找除了用循环来实现，还可以用递归来实现，过程也非常简单。\n\n```java\n// 二分查找的递归实现\npublic int bsearch(int[] a, int n, int val) {\n  return bsearchInternally(a, 0, n - 1, val);\n}\n\nprivate int bsearchInternally(int[] a, int low, int high, int value) {\n  if (low \u003e high) return -1;\n\n  int mid =  low + ((high - low) \u003e\u003e 1);\n  if (a[mid] == value) {\n    return mid;\n  } else if (a[mid] \u003c value) {\n    return bsearchInternally(a, mid+1, high, value);\n  } else {\n    return bsearchInternally(a, low, mid-1, value);\n  }\n}\n```\n\n### **二分查找应用场景的局限性**\n\n- 首先，二分查找依赖的是顺序表结构，简单点说就是数组，并不能依赖链表，因为其不支持随机访问。\n- 其次，二分查找针对的是有序数据。\n\n    二分查找只能用在插入、删除操作不频繁，一次排序多次查找的场景中。针对动态变化的数据集合，二分查找将不再适用。对于动态变化的数据集合，更适合使用散列表或二叉查找树。\n\n- 再次，数据量太小不适合二分查找。\n\n    如果要处理的数据量很小，完全没有必要用二分查找，顺序遍历就足够了。不过，这里有一个例外。如果数据之间的比较操作非常耗时，不管数据量大小，我都推荐使用二分查找。我们需要尽可能地减少比较次数，而比较次数的减少会大大提高性能，这个时候二分查找就比顺序遍历更有优势。\n\n- 最后，数据量太大也不适合二分查找。\n\n    二分查找的底层需要依赖数组这种数据结构，而数组为了支持随机访问的特性，要求内存空间连续，对内存的要求比较苛刻。注意这里的“连续”二字，也就是说，即便有 2GB 的内存空间剩余，但是如果这剩余的 2GB 内存空间都是零散的，没有连续的 1GB 大小的内存空间，那照样无法申请一个 1GB 大小的数组。而我们的二分查找是作用在数组这种数据结构之上的，所以太大的数据用数组存储就比较吃力了，也就不能用二分查找了。\n\n\n### **二分查找的变形问题**\n\n![Untitled](Assets/alg_64.png)\n\n- **变体一：查找第一个值等于给定值的元素**\n\n    ```java\n    public int bsearch(int[] a, int n, int value) {\n      int low = 0;\n      int high = n - 1;\n      while (low \u003c= high) {\n        int mid =  low + ((high - low) \u003e\u003e 1);\n        if (a[mid] \u003e value) {\n          high = mid - 1;\n        } else if (a[mid] \u003c value) {\n          low = mid + 1;\n        } else {\n          if ((mid == 0) || (a[mid - 1] != value)) return mid;\n          else high = mid - 1;\n        }\n      }\n      return -1;\n    }\n    ```\n\n    a[mid]跟要查找的 value 的大小关系有三种情况：大于、小于、等于。对于 a[mid]\u003evalue 的情况，我们需要更新 high= mid-1；对于 a[mid]=value 的时候应该如何处理呢？\n\n    如果我们查找的是任意一个值等于给定值的元素，当 a[mid]等于要查找的值时，a[mid]就是我们要找的元素。但是，如果我们求解的是第一个值等于给定值的元素，当 a[mid]等于要查找的值时，我们就需要确认一下这个 a[mid]是不是第一个值等于给定值的元素。\n\n    我们重点看第 11 行代码。如果 mid 等于 0，那这个元素已经是数组的第一个元素，那它肯定是我们要找的；如果 mid 不等于 0，但 a[mid]的前一个元素 a[mid-1]不等于 value，那也说明 a[mid]就是我们要找的第一个值等于给定值的元素。\n\n    如果经过检查之后发现 a[mid]前面的一个元素 a[mid-1]也等于 value，那说明此时的 a[mid]肯定不是我们要查找的第一个值等于给定值的元素。那我们就更新 high=mid-1，因为要找的元素肯定出现在[low, mid-1]之间。\n\n- **变体二：查找最后一个值等于给定值的元素**\n\n    ```java\n    public int bsearch(int[] a, int n, int value) {\n      int low = 0;\n      int high = n - 1;\n      while (low \u003c= high) {\n        int mid =  low + ((high - low) \u003e\u003e 1);\n        if (a[mid] \u003e value) {\n          high = mid - 1;\n        } else if (a[mid] \u003c value) {\n          low = mid + 1;\n        } else {\n          if ((mid == n - 1) || (a[mid + 1] != value)) return mid;\n          else low = mid + 1;\n        }\n      }\n      return -1;\n    }\n    ```\n\n    我们还是重点看第 11 行代码。如果 a[mid]这个元素已经是数组中的最后一个元素了，那它肯定是我们要找的；如果 a[mid]的后一个元素 a[mid+1]不等于 value，那也说明 a[mid]就是我们要找的最后一个值等于给定值的元素。\n\n    如果我们经过检查之后，发现 a[mid]后面的一个元素 a[mid+1]也等于 value，那说明当前的这个 a[mid]并不是最后一个值等于给定值的元素。我们就更新 low=mid+1，因为要找的元素肯定出现在[mid+1, high]之间。\n\n- **变体三：查找第一个大于等于给定值的元素**\n\n    ```java\n    public int bsearch(int[] a, int n, int value) {\n      int low = 0;\n      int high = n - 1;\n      while (low \u003c= high) {\n        int mid =  low + ((high - low) \u003e\u003e 1);\n        if (a[mid] \u003e= value) {\n          if ((mid == 0) || (a[mid - 1] \u003c value)) return mid;\n          else high = mid - 1;\n        } else {\n          low = mid + 1;\n        }\n      }\n      return -1;\n    }\n    ```\n\n    如果 a[mid]小于要查找的值 value，那要查找的值肯定在[mid+1, high]之间，所以，我们更新 low=mid+1。\n\n    对于 a[mid]大于等于给定值 value 的情况，我们要先看下这个 a[mid]是不是我们要找的第一个值大于等于给定值的元素。如果 a[mid]前面已经没有元素，或者前面一个元素小于要查找的值 value，那 a[mid]就是我们要找的元素。这段逻辑对应的代码是第 7 行。\n\n    如果 a[mid-1]也大于等于要查找的值 value，那说明要查找的元素在[low, mid-1]之间，所以，我们将 high 更新为 mid-1。\n\n- **变体四：查找最后一个小于等于给定值的元素**\n\n    ```java\n    public int bsearch7(int[] a, int n, int value) {\n      int low = 0;\n      int high = n - 1;\n      while (low \u003c= high) {\n        int mid =  low + ((high - low) \u003e\u003e 1);\n        if (a[mid] \u003e value) {\n          high = mid - 1;\n        } else {\n          if ((mid == n - 1) || (a[mid + 1] \u003e value)) return mid;\n          else low = mid + 1;\n        }\n      }\n      return -1;\n    }\n    ```\n\n\n变体的二分查找算法写起来非常烧脑，很容易因为细节处理不好而产生 Bug，这些容易出错的细节有：**终止条件、区间上下界更新方法、返回值选择**。\n\n## **跳表**\n\nRedis 中的有序集合（Sorted Set）就是用跳表来实现的。\n\n### **如何理解“跳表”？**\n\n对于一个单链表来讲，即便链表中存储的数据是有序的，如果我们要想在其中查找某个数据，也只能从头到尾遍历链表。这样查找效率就会很低，时间复杂度会很高，是 O(n)。\n\n那怎么来提高查找效率呢？如果像图中那样，对链表建立一级“索引”，查找起来是不是就会更快一些呢？每两个结点提取一个结点到上一级，我们把抽出来的那一级叫做索引或索引层。加来一层索引之后，查找一个结点需要遍历的结点个数减少了，也就是说查找效率提高了。\n\n![Untitled](Assets/alg_65.png)\n\n前面讲的这种链表加多级索引的结构，就是跳表。\n\n### **用跳表查询到底有多快？**\n\n按照我们刚才讲的，每两个结点会抽出一个结点作为上一级索引的结点，那第一级索引的结点个数大约就是 n/2，第二级索引的结点个数大约就是 n/4，第三级索引的结点个数大约就是 n/8，依次类推，也就是说，**第 k 级索引的结点个数是第 k-1 级索引的结点个数的 1/2，那第 k级索引结点的个数就是 n/(2k)。**\n\n假设索引有 h 级，最高级的索引有 2 个结点。通过上面的公式，我们可以得到 n/(2h)=2，从而求得 h=log2n-1。如果包含原始链表这一层，整个跳表的高度就是 log2n。我们在跳表中查询某个数据的时候，如果每一层都要遍历 m 个结点，那在跳表中查询一个数据的时间复杂度就是 O(m*logn)。\n\n那这个 m 的值是多少呢？按照前面这种索引结构，我们每一级索引都最多只需要遍历 3 个结点，也就是说 m=3。\n\n假设我们要查找的数据是 x，在第 k 级索引中，我们遍历到 y 结点之后，发现 x 大于 y，小于后面的结点 z，所以我们通过 y 的 down 指针，从第 k 级索引下降到第 k-1 级索引。在第 k-1 级索引中，y 和 z 之间只有 3 个结点（包含 y 和 z），所以，我们在 K-1 级索引中最多只需要遍历 3 个结点，依次类推，每一级索引都最多只需要遍历 3 个结点。\n\n![Untitled](Assets/alg_66.png)\n\n所以在跳表中查询任意数据的时间复杂度就是 O(logn)。这个查找的时间复杂度跟二分查找是一样的。换句话说，我们其实是基于单链表实现了二分查找，是不是很神奇？不过，天下没有免费的午餐，这种查询效率的提升，前提是建立了很多级索引，也就是之前讲过的空间换时间的设计思路。\n\n### **跳表是不是很浪费内存？**\n\n假设原始链表大小为 n，那第一级索引大约有 n/2 个结点，第二级索引大约有 n/4 个结点，以此类推，每上升一级就减少一半，直到剩下 2 个结点。如果我们把每层索引的结点数写出来，就是一个等比数列。\n\n![Untitled](Assets/alg_67.png)\n\n这几级索引的结点总和就是 n/2+n/4+n/8…+8+4+2=n-2。所以，跳表的空间复杂度是 O(n)。\n\n实际上，在软件开发中，我们不必太在意索引占用的额外空间。在讲数据结构和算法时，我们习惯性地把要处理的数据看成整数，但是在实际的软件开发中，原始链表中存储的有可能是很大的对象，而索引结点只需要存储关键值和几个指针，并不需要存储对象，所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略了。\n\n### **高效的动态插入和删除**\n\n跳表这个动态数据结构，不仅支持查找操作，还支持动态的插入、删除操作，而且插入、删除操作的时间复杂度也是 O(logn)。\n\n我们知道，在单链表中，一旦定位好要插入的位置，插入结点的时间复杂度是很低的，就是 O(1)。但是，这里为了保证原始链表中数据的有序性，我们需要先找到要插入的位置，这个查找操作就会比较耗时。\n\n对于纯粹的单链表，需要遍历每个结点，来找到插入的位置。但是，对于跳表来说，我们讲过查找某个结点的时间复杂度是 O(logn)，所以这里查找某个数据应该插入的位置，方法也是类似的，时间复杂度也是 O(logn)。\n\n![Untitled](Assets/alg_68.png)\n\n在删除操作中，如果这个结点在索引中也有出现，我们除了要删除原始链表中的结点，还要删除索引中的。\n\n### **跳表索引动态更新**\n\n当我们不停地往跳表中插入数据时，如果我们不更新索引，就有可能出现某 2 个索引结点之间数据非常多的情况。极端情况下，跳表还会退化成单链表。\n\n作为一种动态数据结构，我们需要某种手段来维护索引与原始链表大小之间的平衡，也就是说，如果链表中结点多了，索引结点就相应地增加一些，避免复杂度退化，以及查找、插入、删除操作性能下降。\n\n跳表是通过随机函数来维护前面提到的“平衡性”。当我们往跳表中插入数据的时候，我们可以选择同时将这个数据插入到部分索引层中。如何选择加入哪些索引层呢？\n\n我们通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值 K，那我们就将这个结点添加到第一级到第 K 级这 K 级索引中。\n\n![Untitled](Assets/alg_69.png)\n\n## 散列表\n\n**散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。**\n\n![Untitled](Assets/alg_70.png)\n\n散列表用的就是数组支持按照下标随机访问的时候，时间复杂度是 O(1) 的特性。我们通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。\n\n### **散列冲突**\n\n- **开放寻址法**\n\n    开放寻址法的核心思想是，如果出现了散列冲突，我们就重新探测一个空闲位置，将其插入。那如何重新探测新的位置呢？我先讲一个比较简单的探测方法，**线性探测**（Linear Probing）。\n\n    当我们往散列表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，我们就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。\n\n    ![Untitled](Assets/alg_71.png)\n\n    在散列表中查找元素的过程有点儿类似插入过程。我们通过散列函数求出要查找元素的键值对应的散列值，然后比较数组中下标为散列值的元素和要查找的元素。如果相等，则说明就是我们要找的元素；否则就顺序往后依次查找。如果遍历到数组中的空闲位置，还没有找到，就说明要查找的元素并没有在散列表中。\n\n    ![Untitled](Assets/alg_72.png)\n\n    散列表跟数组一样，不仅支持插入、查找操作，还支持删除操作。对于使用线性探测法解决冲突的散列表，删除操作稍微有些特别。我们不能单纯地把要删除的元素设置为空。\n\n    在查找的时候，一旦我们通过线性探测方法，找到一个空闲位置，我们就可以认定散列表中不存在这个数据。但是，如果这个空闲位置是我们后来删除的，就会导致原来的查找算法失效。\n\n    所以，我们可以将删除的元素，特殊标记为 deleted。当线性探测查找的时候，遇到标记为 deleted 的空间，并不是停下来，而是继续往下探测。\n\n    ![Untitled](Assets/alg_73.png)\n\n    线性探测法其实存在很大问题。当散列表中插入的数据越来越多时，散列冲突发生的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。极端情况下，我们可能需要探测整个散列表，所以最坏情况下的时间复杂度为 O(n)。同理，在删除和查找时，也有可能会线性探测整张散列表，才能找到要查找或者删除的数据。\n\n    ---\n\n    对于开放寻址冲突解决方法，除了线性探测方法之外，还有另外两种比较经典的探测方法，**二次探测**（Quadratic probing）和**双重散列**（Double hashing）。\n\n    所谓二次探测，跟线性探测很像，线性探测每次探测的步长是 1，那它探测的下标序列就是 hash(key)+0，hash(key)+1，hash(key)+2……而二次探测探测的步长就变成了原来的“二次方”，也就是说，它探测的下标序列就是 hash(key)+0，hash(key)+12，hash(key)+22……\n\n    所谓双重散列，意思就是不仅要使用一个散列函数。我们使用一组散列函数 hash1(key)，hash2(key)，hash3(key)……我们先用第一个散列函数，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推，直到找到空闲的存储位置。\n\n    不管采用哪种探测方法，当散列表中空闲位置不多的时候，散列冲突的概率就会大大提高。为了尽可能保证散列表的操作效率，一般情况下，我们会尽可能保证散列表中有一定比例的空闲槽位。我们用**装载因子**（load factor）来表示空位的多少。\n\n    装载因子的计算公式是：\n\n    ```java\n    散列表的装载因子=填入表中的元素个数/散列表的长度\n    ```\n\n    装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。\n\n- **链表法**\n\n    链表法是一种更加常用的散列冲突解决办法，在散列表中，每个“桶（bucket）”或者“槽（slot）”会对应一条链表，所有散列值相同的元素我们都放到相同槽位对应的链表中。\n\n    ![Untitled](Assets/alg_74.png)\n\n    当插入的时候，我们只需要通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，所以插入的时间复杂度是 O(1)。当查找、删除一个元素时，我们同样通过散列函数计算出对应的槽，然后遍历链表查找或者删除。那查找或删除操作的时间复杂度是多少呢？\n\n    实际上，这两个操作的时间复杂度跟链表的长度 k 成正比，也就是 O(k)。对于散列比较均匀的散列函数来说，理论上讲，k=n/m，其中 n 表示散列中数据的个数，m 表示散列表中“槽”的个数。\n\n\n### **如何设计散列函数？**\n\n首先，**散列函数的设计不能太复杂**。过于复杂的散列函数，势必会消耗很多计算时间，也就间接地影响到散列表的性能。其次，**散列函数生成的值要尽可能随机并且均匀分布**，这样才能避免或者最小化散列冲突，而且即便出现冲突，散列到每个槽里的数据也会比较平均，不会出现某个槽内数据特别多的情况。\n\n实际工作中，我们还需要综合考虑各种因素。这些因素有关键字的长度、特点、分布、还有散列表的大小等。\n\n第一个例子是学生运动会的例子，我们通过分析参赛编号的特征，把编号中的后两位作为散列值。我们还可以用类似的散列函数处理手机号码，因为手机号码前几位重复的可能性很大，但是后面几位就比较随机，我们可以取手机号的后四位作为散列值。这种散列函数的设计方法，我们一般叫做“数据分析法”。\n\n第二个例子是如何实现 Word 拼写检查功能。这里面的散列函数，我们就可以这样设计：将单词中每个字母的ASCll 码值“进位”相加，然后再跟散列表的大小求余、取模，作为散列值。比如，英文单词 nice，我们转化出来的散列值就是下面这样：\n\n```java\nhash(\"nice\")=((\"n\" - \"a\") * 26*26*26 + (\"i\" - \"a\")*26*26 + (\"c\" - \"a\")*26+ (\"e\"-\"a\")) / 78978\n```\n\n实际上，散列函数的设计方法还有很多，比如直接寻址法、平方取中法、折叠法、随机数法等，这些只要了解就行了，不需要全都掌握。\n\n### **装载因子过大了怎么办？**\n\n针对散列表，当装载因子过大时，我们也可以进行动态扩容，重新申请一个更大的散列表，将数据搬移到这个新散列表中。\n\n针对散列表的扩容，数据搬移操作要复杂很多。因为散列表的大小变了，数据的存储位置也变了，所以我们需要通过散列函数重新计算每个数据的存储位置。\n\n比如下图中，在原来的散列表中，21 这个元素原来存储在下标为 0 的位置，搬移到新的散列表中，存储在下标为 7 的位置。\n\n![Untitled](Assets/alg_75.png)\n\n插入一个数据，最好情况下，不需要扩容，最好时间复杂度是 O(1)。最坏情况下，散列表装载因子过高，启动扩容，我们需要重新申请内存空间，重新计算哈希位置，并且搬移数据，所以时间复杂度是 O(n)。用摊还分析法，均摊情况下，时间复杂度接近最好情况，就是 O(1)。\n\n实际上，对于动态散列表，随着数据的删除，散列表中的数据会越来越少，空闲空间会越来越多。如果我们对空间消耗非常敏感，我们可以在装载因子小于某个值之后，启动动态缩容。当然，如果我们更加在意执行效率，能够容忍多消耗一点内存空间，那就可以不用费劲来缩容了。\n\n### **如何避免低效的扩容？**\n\n当装载因子已经到达阈值，需要先进行扩容，再插入数据。这个时候，插入数据就会变得很慢，甚至会无法接受。\n\n如果我们的业务代码直接服务于用户，尽管大部分情况下，插入一个数据的操作都很快，但是，极个别非常慢的插入操作，也会让用户崩溃。这个时候，“一次性”扩容的机制就不合适了。\n\n为了解决一次性扩容耗时过多的情况，我们可以将扩容操作穿插在插入操作的过程中，分批完成。当装载因子触达阈值之后，我们只申请新空间，但并不将老的数据搬移到新散列表中。\n\n当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。每次插入一个数据到散列表，我们都重复上面的过程。经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。这样没有了集中的一次性数据搬移，插入操作就都变得很快了。\n\n![Untitled](Assets/alg_76.png)\n\n对于查询操作，为了兼容了新、老散列表中的数据，我们先从新散列表中查找，如果没有找到，再去老的散列表中查找。\n\n通过这样均摊的方法，将一次性扩容的代价，均摊到多次插入操作中，就避免了一次性扩容耗时过多的情况。这种实现方式，任何情况下，插入一个数据的时间复杂度都是 O(1)。\n\n### **如何选择冲突解决方法？**\n\n- **开放寻址法**\n    - 优点\n\n        开放寻址法不像链表法，需要拉很多链表。散列表中的数据都存储在数组中，可以有效地利用 CPU 缓存加快查询速度。而且，这种方法实现的散列表，序列化起来比较简单。链表法包含指针，序列化起来就没那么容易。\n\n    - 缺点\n\n        用开放寻址法解决冲突的散列表，删除数据的时候比较麻烦，需要特殊标记已经删除掉的数据。而且，在开放寻址法中，所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。所以，使用开放寻址法解决冲突的散列表，装载因子的上限不能太大。这也导致这种方法比链表法更浪费内存空间。\n\n\n    **总结一下，当数据量比较小、装载因子小的时候，适合采用开放寻址法。**\n\n- **链表法**\n\n    链表法对内存的利用率比开放寻址法要高。因为链表结点可以在需要的时候再创建，并不需要像开放寻址法那样事先申请好。\n\n    链表法比起开放寻址法，对大装载因子的容忍度更高。开放寻址法只能适用装载因子小于 1 的情况。接近 1 时，就可能会有大量的散列冲突，导致大量的探测、再散列等，性能会下降很多。但是对于链表法来说，只要散列函数的值随机均匀，即便装载因子变成 10，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多。\n\n    链表因为要存储指针，所以对于比较小的对象的存储，是比较消耗内存的，还有可能会让内存的消耗翻倍。而且，因为链表中的结点是零散分布在内存中的，不是连续的，所以对 CPU 缓存是不友好的，这方面对于执行效率也有一定的影响。\n\n    当然，如果我们存储的是大对象，也就是说要存储的对象的大小远远大于一个指针的大小（4 个字节或者 8 个字节），那链表中指针的内存消耗在大对象面前就可以忽略了。\n\n    实际上，我们对链表法稍加改造，可以实现一个更加高效的散列表。那就是，我们将链表法中的链表改造为其他高效的动态数据结构，比如跳表、红黑树。这样，即便出现散列冲突，极端情况下，所有的数据都散列到同一个桶内，那最终退化成的散列表的查找时间也只不过是 O(logn)。这样也就有效避免了前面讲到的散列碰撞攻击。\n\n    ![Untitled](Assets/alg_77.png)\n\n    **总结一下，基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。**\n\n\n### **为什么散列表和链表经常会一起使用？**\n\n- **LRU 缓存淘汰算法**\n\n    ![Untitled](Assets/alg_78.png)\n\n    使用双向链表存储数据，链表中的每个结点处理存储数据（data）、前驱指针（prev）、后继指针（next）之外，还新增了一个特殊的字段 hnext。\n\n    因为我们的散列表是通过链表法解决散列冲突的，所以每个结点会在两条链中。一个链是刚刚我们提到的双向链表，另一个链是散列表中的拉链。前驱和后继指针是为了将结点串在双向链表中，hnext 指针是为了将结点串在散列表的拉链中。\n\n    首先，我们来看**如何查找一个数据**。我们前面讲过，散列表中查找数据的时间复杂度接近 O(1)，所以通过散列表，我们可以很快地在缓存中找到一个数据。当找到数据之后，我们还需要将它移动到双向链表的尾部。\n\n    其次，我们来看**如何删除一个数据**。我们需要找到数据所在的结点，然后将结点删除。借助散列表，我们可以在 O(1) 时间复杂度里找到要删除的结点。因为我们的链表是双向链表，双向链表可以通过前驱指针 O(1) 时间复杂度获取前驱结点，所以在双向链表中，删除结点只需要 O(1) 的时间复杂度。\n\n    最后，我们来看**如何添加一个数据**。添加数据到缓存稍微有点麻烦，我们需要先看这个数据是否已经在缓存中。如果已经在其中，需要将其移动到双向链表的尾部；如果不在其中，还要看缓存有没有满。如果满了，则将双向链表头部的结点删除，然后再将数据放到链表的尾部；如果没有满，就直接将数据放到链表的尾部。\n\n    这整个过程涉及的查找操作都可以通过散列表来完成。其他的操作，比如删除头结点、链表尾部插入数据等，都可以在 O(1) 的时间复杂度内完成。所以，这三个操作的时间复杂度都是 O(1)。至此，我们就通过散列表和双向链表的组合使用，实现了一个高效的、支持 LRU 缓存淘汰算法的缓存系统原型。\n\n- **Redis 有序集合**\n\n    在有序集合中，每个成员对象有两个重要的属性，key（键值）和 score（分值）。我们不仅会通过 score 来查找数据，还会通过 key 来查找数据。\n\n    举个例子，比如用户积分排行榜有这样一个功能：我们可以通过用户的 ID 来查找积分信息，也可以通过积分区间来查找用户 ID 或者姓名信息。这里包含 ID、姓名和积分的用户信息，就是成员对象，用户 ID 就是 key，积分就是 score。\n\n    所以，如果我们细化一下 Redis 有序集合的操作，那就是下面这样：\n\n    - 添加一个成员对象；\n    - 按照键值来删除一个成员对象；\n    - 按照键值来查找一个成员对象；\n    - 按照分值区间查找数据，比如查找积分在[100, 356]之间的成员对象；\n    - 按照分值从小到大排序成员变量；\n\n    如果我们仅仅按照分值将成员对象组织成跳表的结构，那按照键值来删除、查询成员对象就会很慢，解决方法与 LRU 缓存淘汰算法的解决方法类似。我们可以再按照键值构建一个散列表，这样按照 key 来删除、查找一个成员对象的时间复杂度就变成了 O(1)。同时，借助跳表结构，其他操作也非常高效。\n\n- **Java LinkedHashMap**\n\n    LinkedHashMap 是通过双向链表和散列表这两种数据结构组合实现的。LinkedHashMap 中的“Linked”实际上是指的是双向链表，并非指用链表法解决散列冲突。\n\n\n## **哈希算法**\n\n### 什么是哈希算法\n\n将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是**哈希算法**，而通过原始数据映射之后得到的二进制值串就是**哈希值**。但是，要想设计一个优秀的哈希算法并不容易，根据我的经验，我总结了需要满足的几点要求：\n\n- 从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法）；\n- 对输入数据非常敏感，哪怕原始数据只修改了一个 Bit，最后得到的哈希值也大不相同；\n- 散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小；\n- 哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值。\n\n哈希算法的应用非常非常多，最常见的有七个，分别是安全加密、唯一标识、数据校验、散列函数、负载均衡、数据分片、分布式存储。\n\n### 应用一：安全加密\n\n最常用于加密的哈希算法是 MD5（MD5 Message-Digest Algorithm，MD5 消息摘要算法）和 SHA（Secure Hash Algorithm，安全散列算法）。除了这两个之外，当然还有很多其他加密算法，比如 DES（Data Encryption Standard，数据加密标准）、AES（Advanced Encryption Standard，高级加密标准）。\n\n前面我讲到的哈希算法四点要求，对用于加密的哈希算法来说，有两点格外重要。第一点是很难根据哈希值反向推导出原始数据，第二点是散列冲突的概率要很小。\n\n有一点需要注意，哈希算法无法做到零冲突。哈希算法产生的哈希值的长度是固定且有限的。比如前面举的 MD5 的例子，哈希值是固定的 128 位二进制串，能表示的数据是有限的，最多能表示 2^128 个数据，而我们要哈希的数据是无穷的。基于鸽巢原理，如果我们对 2^128+1 个数据求哈希值，就必然会存在哈希值相同的情况。这里你应该能想到，一般情况下，哈希值越长的哈希算法，散列冲突的概率越低。\n\n不过，即便哈希算法存在散列冲突的情况，但是因为哈希值的范围很大，冲突的概率极低，所以相对来说还是很难破解的。像 MD5，有 2^128 个不同的哈希值，这个数据已经是一个天文数字了，所以散列冲突的概率要小于 1/2^128。\n\n### 应用二：唯一标识\n\n如果要在海量的图库中，搜索一张图是否存在，我们不能单纯地用图片的元信息（比如图片名称）来比对，因为有可能存在名称相同但图片内容不同，或者名称不同图片内容相同的情况。那我们该如何搜索呢？\n\n我们可以给每一个图片取一个唯一标识，或者说信息摘要。比如，我们可以从图片的二进制码串开头取 100 个字节，从中间取 100 个字节，从最后再取 100 个字节，然后将这 300 个字节放到一块，通过哈希算法（比如 MD5），得到一个哈希字符串，用它作为图片的唯一标识。通过这个唯一标识来判定图片是否在图库中，这样就可以减少很多工作量。\n\n### 应用三：数据校验\n\n网络传输是不安全的，下载的文件块有可能是被宿主机器恶意修改过的，又或者下载过程中出现了错误，所以下载的文件块可能不是完整的。如果我们没有能力检测这种恶意修改或者文件下载出错，就会导致最终合并后的电影无法观看，甚至导致电脑中毒。现在的问题是，如何来校验文件块的安全、正确、完整呢？\n\n我们通过哈希算法，对 100 个文件块分别取哈希值，并且保存在种子文件中。我们在前面讲过，哈希算法有一个特点，对数据很敏感。只要文件块的内容有一丁点儿的改变，最后计算出的哈希值就会完全不同。所以，当文件块下载完成之后，我们可以通过相同的哈希算法，对下载好的文件块逐一求哈希值，然后跟种子文件中保存的哈希值比对。如果不同，说明这个文件块不完整或者被篡改了，需要再重新从其他宿主机器上下载这个文件块。\n\n### 应用四：散列函数\n\n散列函数是设计一个散列表的关键。它直接决定了散列冲突的概率和散列表的性能。不过，相对哈希算法的其他应用，散列函数对于散列算法冲突的要求要低很多。即便出现个别散列冲突，只要不是过于严重，我们都可以通过开放寻址法或者链表法解决。\n\n不仅如此，散列函数对于散列算法计算得到的值，是否能反向解密也并不关心。散列函数中用到的散列算法，更加关注散列后的值是否能平均分布，也就是，一组数据是否能均匀地散列在各个槽中。除此之外，散列函数执行的快慢，也会影响散列表的性能，所以，散列函数用的散列算法一般都比较简单，比较追求效率。\n\n### 应用五：负载均衡\n\n我们可以通过哈希算法，对客户端 IP 地址或者会话 ID 计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号。 这样，我们就可以把同一个 IP 过来的所有请求，都路由到同一个后端服务器上。\n\n### 应用六：数据分片\n\n1. **如何统计“搜索关键词”出现的次数？**\n\n    假如我们有 1T 的日志文件，这里面记录了用户的搜索关键词，我们想要快速统计出每个关键词被搜索的次数，该怎么做呢？这个问题有两个难点，第一个是搜索日志很大，没办法放到一台机器的内存中。第二个难点是，如果只用一台机器来处理这么巨大的数据，处理时间会很长。\n\n    针对这两个难点，**我们可以先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度。**具体的思路是这样的：为了提高处理的速度，我们用 n 台机器并行处理。我们从搜索记录的日志文件中，依次读出每个搜索关键词，并且通过哈希函数计算哈希值，然后再跟 n 取模，最终得到的值，就是应该被分配到的机器编号。\n\n    这样，哈希值相同的搜索关键词就被分配到了同一个机器上。也就是说，同一个搜索关键词会被分配到同一个机器上。每个机器会分别计算关键词出现的次数，最后合并起来就是最终的结果。\n\n2. **如何快速判断图片是否在图库中？**\n\n    假设现在我们的图库中有 1 亿张图片，很显然，在单台机器上构建散列表是行不通的。因为单台机器的内存有限，而 1 亿张图片构建散列表显然远远超过了单台机器的内存上限。\n\n    我们同样可以对数据进行分片，然后采用多机处理。我们准备 n 台机器，让每台机器只维护某一部分图片对应的散列表。我们每次从图库中读取一个图片，计算唯一标识，然后与机器个数 n 求余取模，得到的值就对应要分配的机器编号，然后将这个图片的唯一标识和图片路径发往对应的机器构建散列表。\n\n    当我们要判断一个图片是否在图库中的时候，我们通过同样的哈希算法，计算这个图片的唯一标识，然后与机器个数 n 求余取模。假设得到的值是 k，那就去编号 k 的机器构建的散列表中查找。\n\n\n### 应用七：分布式存储\n\n现在互联网面对的都是海量的数据、海量的用户。我们为了提高数据的读取、写入能力，一般都采用分布式的方式来存储数据，比如分布式缓存。我们有海量的数据需要缓存，所以一个缓存机器肯定是不够的。于是，我们就需要将数据分布在多台机器上。\n\n该如何决定将哪个数据放到哪个机器上呢？我们可以借用前面数据分片的思想，即通过哈希算法对数据取哈希值，然后对机器个数取模，这个最终值就是应该存储的缓存机器编号。\n\n但添加新的机器会导致所有的数据都要重新计算哈希值，然后重新搬移到正确的机器上。这样就相当于，缓存中的数据一下子就都失效了。所有的数据请求都会穿透缓存，直接去请求数据库。这样就可能发生雪崩效应，压垮数据库。\n\n所以，我们需要一种方法，使得在新加入一个机器后，并不需要做大量的数据搬移。这时候，一致性哈希算法就要登场了。\n\n## 深度和广度优先搜索\n\n### 什么是“搜索”算法？\n\n算法是作用于具体数据结构之上的，深度优先搜索算法和广度优先搜索算法都是基于“图”这种数据结构的。这是因为，图这种数据结构的表达能力很强，大部分涉及搜索的场景都可以抽象成“图”。\n\n### 广度优先搜索（BFS）\n\n广度优先搜索（Breadth-First-Search），我们平常都简称 BFS。直观地讲，它其实就是一种“地毯式”层层推进的搜索策略，即先查找离起始顶点最近的，然后是次近的，依次往外搜索。\n\n![Untitled](Assets/alg_79.png)\n\n### 深度优先搜索（DFS）\n\n深度优先搜索（Depth-First-Search），简称 DFS。最直观的例子就是“走迷宫”。\n\n假设你站在迷宫的某个岔路口，然后想找到出口。你随意选择一个岔路口来走，走着走着发现走不通的时候，你就回退到上一个岔路口，重新选择一条路继续走，直到最终找到出口。这种走法就是一种深度优先搜索策略。\n\n搜索的起始顶点是 s，终止顶点是 t，我们希望在图中寻找一条从顶点 s 到顶点 t 的路径。这里面实线箭头表示遍历，虚线箭头表示回退。从图中我们可以看出，深度优先搜索找出来的路径，并不是顶点 s 到顶点 t 的最短路径。\n\n![Untitled](Assets/alg_80.png)\n\n## **贪心算法**\n\n### 如何理解“贪心算法”？\n\n假设我们有一个可以容纳 100kg 物品的背包，可以装各种物品。我们有以下 5 种豆子，每种豆子的总量和总价值都各不相同。为了让背包中所装物品的总价值最大，我们如何选择在背包中装哪些豆子？每种豆子又该装多少呢？\n\n![Untitled](Assets/alg_81.png)\n\n实际上，这个问题很简单，我估计你一下子就能想出来，没错，我们只要先算一算每个物品的单价，按照单价由高到低依次来装就好了。单价从高到低排列，依次是：黑豆、绿豆、红豆、青豆、黄豆，所以，我们可以往背包里装 20kg 黑豆、30kg 绿豆、50kg 红豆。\n\n这个问题的解决思路显而易见，它本质上借助的就是贪心算法。然而，用贪心算法解决问题的思路，并不总能给出最优解。\n\n我来举一个例子。在一个有权图中，我们从顶点 S 开始，找一条到顶点 T 的最短路径（路径中边的权值和最小）。贪心算法的解决思路是，每次都选择一条跟当前顶点相连的权最小的边，直到找到顶点 T。按照这种思路，我们求出的最短路径是 S-\u003eA-\u003eE-\u003eT，路径长度是 1+4+4=9。\n\n![Untitled](Assets/alg_82.png)\n\n但是，这种贪心的选择方式，最终求的路径并不是最短路径，因为路径 S-\u003eB-\u003eD-\u003eT 才是最短路径，因为这条路径的长度是 2+2+2=6。为什么贪心算法在这个问题上不工作了呢？\n\n在这个问题上，贪心算法不工作的主要原因是，前面的选择，会影响后面的选择。如果我们第一步从顶点 S 走到顶点 A，那接下来面对的顶点和边，跟第一步从顶点 S 走到顶点 B，是完全不同的。所以，即便我们第一步选择最优的走法（边最短），但有可能因为这一步选择，导致后面每一步的选择都很糟糕，最终也就无缘全局最优解了。\n\n### **分治算法**\n\n### 如何理解分治算法？\n\nMapRedue 的本质就是分治算法。\n\n分治算法（divide and conquer）的核心思想其实就是四个字，分而治之 ，也就是将原问题划分成 n 个规模较小，并且结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。\n\n这个定义看起来有点类似递归的定义。关于分治和递归的区别，我们在排序的时候讲过，**分治算法是一种处理问题的思想，递归是一种编程技巧**。实际上，分治算法一般都比较适合用递归来实现。分治算法的递归实现中，每一层递归都会涉及这样三个操作：\n\n- 分解：将原问题分解成一系列子问题；\n- 解决：递归地求解各个子问题，若子问题足够小，则直接求解；\n- 合并：将子问题的结果合并成原问题。\n\n分治算法能解决的问题，一般需要满足下面这几个条件：\n\n- 原问题与分解成的小问题具有相同的模式；\n- 原问题分解成的子问题可以独立求解，子问题之间没有相关性，这一点是分治算法跟动态规划的明显区别，等我们讲到动态规划的时候，会详细对比这两种算法；\n- 具有分解终止条件，也就是说，当问题足够小时，可以直接求解；\n- 可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了。\n\n### 分治思想在海量数据处理中的应用\n\n比如，给 10GB 的订单文件按照金额排序这样一个需求，看似是一个简单的排序问题，但是因为数据量大，有 10GB，而我们的机器的内存可能只有 2、3GB 这样子，无法一次性加载到内存，也就无法通过单纯地使用快排、归并等基础算法来解决了。\n\n要解决这种数据量大到内存装不下的问题，我们就可以利用分治的思想。我们可以将海量的数据集合根据某种方法，划分为几个小的数据集合，每个小的数据集合单独加载到内存来解决，然后再将小数据集合合并成大数据集合。实际上，利用这种分治的处理思路，不仅仅能克服内存的限制，还能利用多线程或者多机处理，加快处理的速度。\n\n## **回溯算法**\n\n### 如何理解“回溯算法”？\n\n笼统地讲，回溯算法很多时候都应用在“搜索”这类问题上。不过这里说的搜索，并不是狭义的指我们前面讲过的图的搜索算法，而是在一组可能的解中，搜索满足期望的解。\n\n回溯的处理思想，有点类似枚举搜索。我们枚举所有的解，找到满足期望的解。为了有规律地枚举所有可能的解，避免遗漏和重复，我们把问题求解的过程分为多个阶段。每个阶段，我们都会面对一个岔路口，我们先随意选一条路走，当发现这条路走不通的时候（不符合期望的解），就回退到上一个岔路口，另选一种走法继续走。\n\n回溯算法的思想非常简单，大部分情况下，都是用来解决广义的搜索问题，也就是，从一组可能的解中，选择出一个满足要求的解。回溯算法非常适合用递归来实现，在实现的过程中，剪枝操作是提高回溯效率的一种技巧。利用剪枝，我们并不需要穷举搜索所有的情况，从而提高搜索效率。\n\n## **动态规划**\n\n### 初识动态规划\n\n动态规划比较适合用来求解最优问题，比如求最大值、最小值等等。它可以非常显著地降低时间复杂度，提高代码的执行效率。不过，它也是出了名的难学。它的主要学习难点跟递归类似，那就是，求解问题的过程不太符合人类常规的思维方式。\n\n我们把问题分解为多个阶段，每个阶段对应一个决策。我们记录每一个阶段可达的状态集合（去掉重复的），然后通过当前阶段的状态集合，来推导下一个阶段的状态集合，动态地往前推进。这也是动态规划这个名字的由来。\n\n尽管动态规划的执行效率比较高，但对空间的消耗比较多。所以，有时候，我们会说，动态规划是一种空间换时间的解决思路。\n\n### “一个模型三个特征”理论讲解\n\n什么样的问题适合用动态规划来解决呢？换句话说，动态规划能解决的问题有什么规律可循呢？实际上，动态规划作为一个非常成熟的算法思想，很多人对此已经做了非常全面的总结。我把这部分理论总结为“一个模型三个特征”。\n\n首先，我们来看，什么是“**一个模型**”？它指的是动态规划适合解决的问题的模型。我把这个模型定义为“**多阶段决策最优解模型**”。下面我具体来给你讲讲。\n\n我们一般是用动态规划来解决最优问题。而解决问题的过程，需要经历多个决策阶段。每个决策阶段都对应着一组状态。然后我们寻找一组决策序列，经过这组决策序列，能够产生最终期望求解的最优值。\n\n我们再来看，什么是“**三个特征**”？它们分别是**最优子结构**、**无后效性**和**重复子问题**。\n\n1. **最优子结构**\n\n    最优子结构指的是，问题的最优解包含子问题的最优解。反过来说就是，我们可以通过子问题的最优解，推导出问题的最优解。如果我们把最优子结构，对应到我们前面定义的动态规划问题模型上，那我们也可以理解为，后面阶段的状态可以通过前面阶段的状态推导出来。\n\n2. **无后效性**\n\n    无后效性有两层含义，第一层含义是，在推导后面阶段的状态的时候，我们只关心前面阶段的状态值，不关心这个状态是怎么一步一步推导出来的。第二层含义是，某阶段状态一旦确定，就不受之后阶段的决策影响。无后效性是一个非常“宽松”的要求。只要满足前面提到的动态规划问题模型，其实基本上都会满足无后效性。\n\n3. **重复子问题**\n\n    这个概念比较好理解。前面一节，我已经多次提过。如果用一句话概括一下，那就是，不同的决策序列，到达某个相同的阶段时，可能会产生重复的状态。\n\n\n### 两种动态规划解题思路总结\n\n1. **状态转移表法**\n\n    一般能用动态规划解决的问题，都可以使用回溯算法的暴力搜索解决。所以，当我们拿到问题的时候，我们可以先用简单的回溯算法解决，然后定义状态，每个状态表示一个节点，然后对应画出递归树。从递归树中，我们很容易可以看出来，是否存在重复子问题，以及重复子问题是如何产生的。以此来寻找规律，看是否能用动态规划解决。\n\n    找到重复子问题之后，接下来，我们有两种处理思路，第一种是直接用**回溯加“备忘录”**的方法，来避免重复子问题。从执行效率上来讲，这跟动态规划的解决思路没有差别。第二种是使用动态规划的解决方法，**状态转移表法**。\n\n    我们先画出一个状态表。状态表一般都是二维的，所以你可以把它想象成二维数组。其中，每个状态包含三个变量，行、列、数组值。我们根据决策的先后过程，从前往后，根据递推关系，分阶段填充状态表中的每个状态。最后，我们将这个递推填表的过程，翻译成代码，就是动态规划代码了。\n\n    尽管大部分状态表都是二维的，但是如果问题的状态比较复杂，需要很多变量来表示，那对应的状态表可能就是高维的，比如三维、四维。那这个时候，我们就不适合用状态转移表法来解决了。一方面是因为高维状态转移表不好画图表示，另一方面是因为人脑确实很不擅长思考高维的东西。\n\n2. **状态转移方程法**\n\n    状态转移方程法有点类似递归的解题思路。我们需要分析，某个问题如何通过子问题来递归求解，也就是所谓的最优子结构。根据最优子结构，写出递归公式，也就是所谓的状态转移方程。有了状态转移方程，代码实现就非常简单了。一般情况下，我们有两种代码实现方法，一种是**递归加“备忘录”**，另一种是**迭代递推**。\n\n    强调一下，**状态转移方程是解决动态规划的关键**。如果我们能写出状态转移方程，那动态规划问题基本上就解决一大半了，而翻译成代码非常简单。但是很多动态规划问题的状态本身就不好定义，状态转移方程也就更不好想到。\n\n\n不是每个问题都同时适合这两种解题思路。有的问题可能用第一种思路更清晰，而有的问题可能用第二种思路更清晰，所以，你要结合具体的题目来看，到底选择用哪种解题思路。\n\n状态转移表法解题思路大致可以概括为，**回溯算法实现 - 定义状态 - 画递归树 - 找重复子问题 - 画状态转移表 - 根据递推关系填表 - 将填表过程翻译成代码**。状态转移方程法的大致思路可以概括为，**找最优子结构 - 写状态转移方程 - 将状态转移方程翻译成代码**\n\n### 四种算法思想比较分析\n\n贪心、分治、回溯和动态规划分析一下这四种算法，看看它们之间有什么区别和联系。\n\n如果我们将这四种算法思想分一下类，那贪心、回溯、动态规划可以归为一类，而分治单独可以作为一类，因为它跟其他三个都不大一样。前三个算法解决问题的模型，都可以抽象成我们今天讲的那个多阶段决策最优解模型，而分治算法解决的问题尽管大部分也是最优解问题，但是，大部分都不能抽象成多阶段决策模型。\n\n回溯算法是个“万金油”。基本上能用的动态规划、贪心解决的问题，我们都可以用回溯算法解决。回溯算法相当于穷举搜索。穷举所有的情况，然后对比得到最优解。不过，回溯算法的时间复杂度非常高，是指数级别的，只能用来解决小规模数据的问题。对于大规模数据的问题，用回溯算法解决的执行效率就很低了。\n\n尽管动态规划比回溯算法高效，但是，并不是所有问题，都可以用动态规划来解决。能用动态规划解决的问题，需要满足三个特征，最优子结构、无后效性和重复子问题。在重复子问题这一点上，动态规划和分治算法的区分非常明显。分治算法要求分割成的子问题，不能有重复子问题，而动态规划正好相反，动态规划之所以高效，就是因为回溯算法实现中存在大量的重复子问题。\n\n贪心算法实际上是动态规划算法的一种特殊情况。它解决问题起来更加高效，代码实现也更加简洁。不过，它可以解决的问题也更加有限。它能解决的问题需要满足三个条件，最优子结构、无后效性和贪心选择性（这里我们不怎么强调重复子问题）。\n\n其中，最优子结构、无后效性跟动态规划中的无异。“贪心选择性”的意思是，通过局部最优的选择，能产生全局的最优选择。每一个阶段，我们都选择当前看起来最优的决策，所有阶段的决策完成之后，最终由这些局部最优解构成全局最优解。\n\n# Note\n\n数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie 树\n\n递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法。\n\n递归（回溯）+备忘录 VS 动态规划","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["CS"]},"/CS/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84":{"title":"系统架构","content":"\n# 传统分层架构 vs 微服务\n\n![image.png](Assets/image_1665669921442_0.png)\n\n![image.png](Assets/image_1665669944183_0.png)\n\n# 微服务改造\n\n* 分离微服务的方法建议：\n\t* 审视并发现可以分离的业务逻辑业务逻辑\n\t* 寻找天生隔离的代码模块，可以借助于静态代码分析工具\n\t* 不同并发规模，不同内存需求的模块都可以分离出不同的微服务，此方法可提高资源利用率，节省成本\n* 一些常用的可微服务化的组件：\n\t* 用户和账户管理\n\t* 授权和会话管理\n\t* 系统配置\n\t* 通知和通讯服务\n\t* 照片，多媒体，元数据等\n* **分解原则：基于 size, scope and capabilities**\n\n# 微服务通信\n\n## 点对点\n\n多用于系统内部多组件之间通讯；\n\n有大量的重复模块如认证授权；\n\n缺少统一规范，如监控，审计等功能；\n\n后期维护成本高，服务和服务的依赖关系错综复杂难以管理。\n\n![image.png](Assets/image_1665670881178_0.png)\n\n## API网关\n\n基于一个轻量级的 message gateway\n\n新 API 通过注册至 Gateway 实现\n\n整合实现 Common function\n\n![image.png](Assets/image_1665670938591_0.png)\n\n# 微服务拆分\n\n微服务拆分并没有统一的标准，相同的业务在不同的公司很可能拆分方式会有所区别，用户规模、团队大小、组员能力等都会是考虑因素。但我们还是有一些基本原则可以遵循：\n\n由粗到细，避免过度拆分，遵循渐进式演进的原则\n\n不同服务之间应该是正交的，不要你中有我我中有你\n\n避免环形依赖，服务依赖关系应该是有向无环图\n\n避免不同服务之间共享同一个数据库\n\n# 12 factors\n\n* I. 基准代码\n\t* 一份基准代码，多份部署\n* II. 依赖\n\t* 显式声明依赖关系\n* III. 配置\n\t* 在环境中存储配置\n* IV. 后端服务\n\t* 把后端服务当作附加资源\n* V. 构建，发布，运行\n\t* 严格分离构建和运行\n* VI. 进程\n\t* 以一个或多个无状态进程运行应用\n* VII. 端口绑定\n\t* 通过端口绑定提供服务\n* VIII. 并发\n\t* 通过进程模型进行扩展\n* IX. 易处理\n\t* 快速启动和优雅终止可最大化健壮性\n* X. 开发环境与线上环境等价\n\t* 尽可能的保持开发，预发布，线上环境相同\n* XI. 日志\n\t* 把日志当作事件流\n* XII. 管理进程\n\t* 后台管理任务当作一次性进程运行","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["CS"]},"/CS/%E7%BC%96%E8%BE%91%E5%99%A8/VSCode":{"title":"VSCode","content":"\n# 多光标\n\n1. 按住Alt，用鼠标左键点击，可以出现多个光标，输入的代码可以在光标处同时增加。\n2. 按住Ctrl+Alt，再按键盘上向上或者向下的键，可以使一列上出现多个光标。\n3. 选中一段文字，按Shift+Alt+i，可以在每行末尾出现光标\n4. 光标放在一个地方，按Ctrl+Shift+l或者ctrl+F2，可以在页面中出现这个词的不同地方都出现光标（不区分大小写）。有时候这个快捷键的作用和F2重命名变量类似，但是它更加广泛，因为还可以对比如字符串相同的非同一变量或函数类的东西修改。\n5. Ctrl+d，第一次按下时，它会选中光标附近的单词；第二次按下时，它会找到这个单词第二次出现的位置，创建一个新的光标，并且选中它\n6. 按Shift+Alt，再使用鼠标拖动，也可以出现竖直的列光标，同时可以选中多列。\n7. 任何光标操作，可以按Ctrl + U取消\n\n## 参考资料\n\n1.  [VSCode 又酷又实用的多光标编辑](https://juejin.cn/post/7079693787328921637)\n3.  [VS Code 多光标模式](https://wangxiz.github.io/blog/posts/vscode-multi-cursor/)\n4.  [拒绝重复，你一定要学会的多光标特性](https://ppambler.github.io/time-geekbang/01-VS-Code/06-%E6%8B%92%E7%BB%9D%E9%87%8D%E5%A4%8D-%E4%BD%A0%E4%B8%80%E5%AE%9A%E8%A6%81%E5%AD%A6%E4%BC%9A%E7%9A%84%E5%A4%9A%E5%85%89%E6%A0%87%E7%89%B9%E6%80%A7.html)\n\n# Tips\n1.  触发参数提示：Shift+Cmd+Space\n\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["CS/编辑器"]},"/CS/%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1/DDD%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1":{"title":"DDD领域驱动设计","content":"\n# 基础\n\n## Domain-Driven Design\n\nDomain-Driven Design：基于 **领域** 的工程设计。\n\n领域暂时可以将其理解为“业务问题的范畴“。\n\n领域可大可小，对应着大小业务问题的边界。简单来说，领域驱动设计就是将业务上要做的一件大事，通过推演和抽象，拆分成多个内聚的领域。\n\n面向接口编程，高内聚、低耦合。\n\n## DP(Domain Primitive)\n\n构建领域的基础类型。作用：明确接口语义、内聚隐性逻辑、简化单元测试。\n\n**定义**：在 DDD里，DP 可以说是一切模型、方法、架构的基础。它是在特定领域、拥有精准定义、可以自我验证、拥有行为的对象。可以认为是领域的最小组成部分。\n\n**DP三条原则：**\n\n让隐性的概念显性化\n\n让隐性的上下文显性化\n\n封装多对象行为\n\n## Entity\n\n与DP的在本质上的差异在于：在语义上能否拥有数据状态\n\nDP：无状态，组成实体的基础类型\n\nEntity：有状态，领域实体\n\n## Domain Service\n\n涉及到了多个Entity状态改变的服务，被称为Domain Service。\n\nDomain Service主要用于封装多Entity或跨业务域的逻辑。\n\n## Repository\n\n# 概念归纳\n\nDP：抽象并封装自检和一些隐形属性的计算逻辑，且这些属性是无状态的。\n\nEntity：抽象并封装单对象有状态的逻辑\n\nDomain Service：抽象并封装多对象的有状态逻辑。\n\nRepository：抽象并封装外部数据访问逻辑。\n\n---\n\n1. 首先对需要处理的业务问题进行总览。\n2. 然后领域对象（Entity）进行划分，明确每个领域对象的包含的信息和职责边界。并进行跨对象，多对象的逻辑组织（Domain Service）。\n3. 接着在上层应用中根据业务描述去编排Entity和Domain Service。\n4. 最后再做一些下水道工作，去对下层的数据访问，RPC调用去做一些具体实现。\n\n# Aggregate\n\n存在引用关系的对象集合。聚合是对存在引用关系的一组对象的封装，它的目的就是屏蔽掉内部对象之间复杂的关联关系，只对外暴露统一接口。\n\n关于聚合，我们需要关注它的两个属性，根对象（root）和边界（Boundary）。\n\n**根对象**是整个聚合中唯一能够被外部引用的对象，也就是说聚合所暴露的接口只允许操作根对象。根对象是一个Entity，因为每个聚合需要ID和状态来区分其他聚合，所以这里也就是通过根对象的ID来作为整个聚合的ID。\n\n**边界**，简单来说，聚合的边界就是你判断哪些对象可以被放入当前聚合的条件。\n\n![CleanShot 2022-11-22 at 21.34.16-2x.png](Assets/CleanShot_2022-11-22_at_21.34.16-2x_1669124064817_0.png)\n\n---\n\n![CleanShot 2022-11-22 at 21.38.19-2x.png](Assets/CleanShot_2022-11-22_at_21.38.19-2x_1669124432651_0.png)\n\n![CleanShot 2022-11-22 at 21.40.42-2x.png](Assets/CleanShot_2022-11-22_at_21.40.42-2x_1669124449935_0.png)\n\n每个Entity都是存在标识的，标识一般是不可变类型，这里定义了一个Identifier接口，用于所有作为标识的DP的公共父接口。这里不推荐直接使用string作为Entity的标识，建议使用DP来作为Entity的标识。\n\n接下来定义了Entity接口，它的范型是IDentitifer的子类，该范型指定了Entity的标识类型，也就是刚才说实现了Identifier接口的DP。举个例子，PhoneNumber类，手机号可以作为一些账号的标识，所以这里PhoneNumber实现了Identifier接口。需要说明的是，并不是所有的DP都是用来作为标识的，比如这里的DP Red就不适合作为Entity的标识。\n\n接下来就是主角聚合根，聚合根是Entity，所以AggrateRoot继承了Entity。和Entity一样，聚合根也需要一个标识，所以这里指定泛型为Identiifer的子类。\n\n结合上文的例子，这里先定义账号类WechatAccount，它是一个Entity。WechatAccount的泛型为PhoneNumber，意味着手机号是每个微信号的全局标识，在账号操作这个业务聚合中它能够充当被外部引用的根。\n\n---\n\n**总结：**\n\n在复杂的软件中，一个业务动作会涉及大量存在关联的对象。聚合的价值就是通过封装，保持所有关联对象的状态一致。此外聚合存在两个必须关注的属性，根对象和边界，明确了这两个属性，整个聚合的面貌就清晰了。\n\n在运行时，领域方法通过持有聚合根，维护了对象的状态一致。在持久化时，repository通过持有聚合根，来处理数据一致。最后，因为聚合中存在大量对象持久化时，我们希望每次数据更新保持最小化原则（change tracking）\n\n# 参考资料\n\n1.  [【【领域驱动设计】DDD入门五板斧之一：Domain Primitive】](https://www.bilibili.com/video/BV11q4y1q74f?vd_source=95893bf3c956973d41f5d92572ef8a93)\n\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["CS/软件设计"]},"/CS/Docker/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4":{"title":"常用命令","content":"\n# 远程构建image\n\n## 使用build构建\n\n```bash\nDOCKER_HOST=\"ssh://user@docker-build.dev\" docker build -t {tag} .\ndocker use context remote-build-host \u0026\u0026 docker build -t {tag} .\ndocker -H ssh://user@docker-build.dev:22 build -t {tag} .\n```\n\n## 使用buildx构建\n\n### 使用方法\n\n\u003e https://dustinrue.com/2021/12/using-a-remote-docker-engine-with-buildx/\n\n\u003e https://docs.docker.com/engine/context/working-with-contexts/\n\n\u003e https://docs.docker.com/engine/reference/commandline/buildx_create/\n\n### 首先创建context（目前指定的kubeconfig没有生效）\n\n```bash\ndocker context create deskmini --default-stack-orchestrator=kubernetes --docker host=ssh://192.168.3.10 --kubernetes config-file=/Users/petrus/.kube/k0s.config\ndocker context use deskmini\n```\n\n### 再创建buildx\n\n```bash\ndocker buildx create --bootstrap --name=kube --driver=kubernetes --driver-opt=namespace=applications,qemu.install=true,rootless=true\ndocker buildx use kube\n```\n\n### 之后就可以正常使用docker buildx进行远程构建了\n\n```bash\nKUBECONFIG=~/.kube/k0s.config docker buildx build --load -f build/package/Dockerfile --platform linux/amd64 -t remote-multi-arch-test .\n```\n\n### 问题\n\n\u003e https://suyuying.github.io/docs/Docker/2023-03-10-docker-muiltiplatform\n\n\u003e [https://github.com/docker/buildx/issues/59](https://github.com/docker/buildx/issues/59)\n\n使用--platform 配合--load，不能支持多平台，一次一个平台，不则会报错。--push 支持一次多平台 image\n\n# 检查docker镜像内容\n\n```bash\ndocker run -it --rm --name \u003cdocker name\u003e  --entrypoint=/bin/sh \u003cdocker image\u003e:\u003ctag\u003e\n```\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Docker"]},"/CS/Docker/%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF":{"title":"核心技术","content":"\n# Docker\n\n基于 Linux 内核的 Cgroup，Namespace，以及 Union FS 等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术，由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。\n\n最初实现是基于 LXC，从 0.7 以后开始去除 LXC，转而使用自行开发的 Libcontainer，从 1.11 开始，则进一步演进为使用 runC 和 Containerd。\n\nDocker 在容器的基础上，进行了进一步的封装，从文件系统、网络互联到进程隔离等等，极大的简化了容器的创建和维护，使得 Docker 技术比虚拟机技术更为轻便、快捷。\n\n# OCI 容器标准\n\n* Open Container Initiative\n\t* OCI 组织于 2015 年创建，是一个致力于定义容器镜像标准和运行时标准的开放式组织。\n\t* OCI 定义了镜像标准（Runtime Specification）、运行时标准（Image Specification）和分发标准（Distribution Specification）\n\t\t* 镜像标准定义应用如何打包\n\t\t* 运行时标准定义如何解压应用包并运行\n\t\t* 分发标准定义如何分发容器镜像\n\n# Docker 引擎架构\n\n![image.png](Assets/image_1665734926295_0.png)\n\n# Namespace\n\n* Linux Namespace 是一种 Linux Kernel 提供的资源隔离方案：\n\t* 系统可以为进程分配不同的 Namespace；\n\t* 并保证不同的 Namespace 资源独立分配、进程彼此隔离，即不同的 Namespace 下的进程互不干扰。\n\n## pid namespace\n\n不同用户的进程就是通过 Pid namespace 隔离开的，且不同 namespace 中可以有相同 Pid。\n\n有了 Pid namespace, 每个 namespace 中的 Pid 能够相互隔离。\n\n## net namespace\n\n网络隔离是通过 net namespace 实现的， 每个 net namespace 有独立的 network devices, IP addresses, IP routing tables, /proc/net 目录。\n\nDocker 默认采用 veth 的方式将 container 中的虚拟网卡同 host 上的一个 docker bridge: docker0 连接在一起。\n\n## ipc namespace\n\nContainer 中进程交互还是采用 linux 常见的进程间交互方法 （interprocess communication – IPC）, 包括常见的信号量、消息队列和共享内存。\n\nContainer 的进程间交互实际上还是 host上 具有相同 Pid namespace 中的进程间交互，因此需要在 IPC资源申请时加入 namespace 信息 - 每个 IPC 资源有一个唯一的 32 位 ID。\n\n## mnt namespace\n\nmnt namespace 允许不同 namespace 的进程看到的文件结构不同，这样每个 namespace 中的进程所看到的文件目录就被隔离开了。\n\n## uts namespace\n\nUTS(“UNIX Time-sharing System”) namespace允许每个 container 拥有独立的 hostname 和domain name, 使其在网络上可以被视作一个独立的节点而非 Host 上的一个进程。\n\n## user namespace\n\n每个 container 可以有不同的 user 和 group id, 也就是说可以在 container 内部用 container 内部的用户执行程序而非 Host 上的用户。\n\n## 关于 namespace 的常用操作\n\n* 查看当前系统的 namespace：\n\t* `lsns –t \u003ctype\u003e`\n* 查看某进程的 namespace：\n\t* `ls -la /proc/\u003cpid\u003e/ns/`\n* 进入某 namespace 运行命令：\n\t* `nsenter -t \u003cpid\u003e -n ip addr`\n\n# Cgroups\n\nCgroups （Control Groups）是 Linux 下用于对一个或一组进程进行资源控制和监控的机制；\n\n可以对诸如 CPU 使用时间、内存、磁盘 I/O 等进程所需的资源进行限制；\n\n不同资源的具体管理工作由相应的 Cgroup 子系统（Subsystem）来实现 ；\n\n针对不同类型的资源限制，只要将限制策略在不同的的子系统上进行关联即可 ；\n\nCgroups 在不同的系统资源管理子系统中以层级树（Hierarchy）的方式来组织管理：每个 Cgroup 都可以包含其他的子 Cgroup，因此子 Cgroup 能使用的资源除了受本 Cgroup 配置的资源参数限制，还受到父Cgroup 设置的资源限制。\n\n## 可配额/可度量\n\ncgroups 实现了对资源的配额和度量\n\n**blkio**：这个子系统设置限制每个块设备的输入输出控制。例如:磁盘，光盘以及 USB 等等。\n\n**CPU**：这个子系统使用调度程序为 cgroup 任务提供 CPU 的访问。\n\n**cpuacct**： 产生 cgroup 任务的 CPU 资源报告。\n\n**cpuset**： 如果是多核心的 CPU，这个子系统会为 cgroup 任务分配单独的 CPU 和内存。\n\n**devices**： 允许或拒绝 cgroup\n\n**freezer**： 暂停和恢复 cgroup 任务。\n\n**memory**： 设置每个 cgroup 的内存限制以及产生内存资源报告。\n\n**net_cls**： 标记每个网络包以供 cgroup 方便使用。\n\n**ns**：名称空间子系统。\n\n**pid**：进程标识子系统。\n\n## CPU子系统\n\ncpu.shares：可出让的能获得 CPU 使用时间的相对值。\n\n![image.png](Assets/image_1665679422886_0.png){:height 118, :width 245}\n\ncpu.cfs_period_us：cfs_period_us 用来配置时间周期长度，单位为 us（微秒）。\n\ncpu.cfs_quota_us：cfs_quota_us 用来配置当前 Cgroup 在 cfs_period_us 时间内最多能使用的 CPU 时间数，单位为 us（微秒）。\n\n![image.png](Assets/image_1665679444552_0.png){:height 153, :width 251}\n\ncpu.stat ：Cgroup 内的进程使用的 CPU 时间统计。\n\nnr_periods ：经过 cpu.cfs_period_us 的时间周期数量。\n\nnr_throttled ：在经过的周期内，有多少次因为进程在指定的时间周期内用光了配额时间而受到限制。\n\nthrottled_time ：Cgroup 中的进程被限制使用 CPU 的总用时，单位是 ns（纳秒）。\n\n### Linux 调度器\n\n内核默认提供了5个调度器，Linux 内核使用 struct sched_class 来对调度器进行抽象：\n\nStop 调度器，stop_sched_class：优先级最高的调度类，可以抢占其他所有进程，不能被其他进程抢占；\n\nDeadline 调度器，dl_sched_class：使用红黑树，把进程按照绝对截止期限进行排序，选择最小进程进行调度运行；\n\nRT 调度器， rt_sched_class：实时调度器，为每个优先级维护一个队列；\n\n**CFS 调度器， cfs_sched_class：完全公平调度器，采用完全公平调度算法，引入虚拟运行时间概念；**\n\nIDLE-Task 调度器， idle_sched_class：空闲调度器，每个 CPU 都会有一个 idle 线程，当没有其他进程可以调度时，调度运行 idle 线程。\n\n### CFS 调度器\n\nCFS 是 Completely Fair Scheduler 简称，即完全公平调度器。\n\nCFS 实现的主要思想是维护为任务提供处理器时间方面的平衡，这意味着应给进程分配相当数量的处理器。\n\n分给某个任务的时间失去平衡时，应给失去平衡的任务分配时间，让其执行。\n\nCFS 通过虚拟运行时间（vruntime）来实现平衡，维护提供给某个任务的时间量。\n\n`vruntime = 实际运行时间*1024 / 进程权重`\n\n进程按照各自不同的速率在物理时钟节拍内前进，优先级高则权重大，其虚拟时钟比真实时钟跑得慢，但获得比较多的运行时间。\n\n### vruntime 红黑树\n\nCFS 调度器没有将进程维护在运行队列中，而是维护了一个以虚拟运行时间为顺序的红黑树。 红黑树的主要特点有：\n\n1. 自平衡，树上没有一条路径会比其他路径长出俩倍。\n\n2. O(log n) 时间复杂度，能够在树上进行快速高效地插入或删除进程。\n\n![image.png](Assets/image_1665679904583_0.png)\n\n## cpuacct 子系统\n\n* 用于统计 Cgroup 及其子 Cgroup 下进程的 CPU 的使用情况。\n\t* cpuacct.usage\n\t\t* 包含该 Cgroup 及其子 Cgroup 下进程使用 CPU 的时间，单位是 ns（纳秒）。\n\t* cpuacct.stat\n\t\t* 包含该 Cgroup 及其子 Cgroup 下进程使用的 CPU 时间，以及用户态和内核态的时间。\n\n## Memory 子系统\n\n* memory.usage_in_bytes\n\t* cgroup 下进程使用的内存，包含 cgroup 及其子 cgroup 下的进程使用的内存\n* memory.max_usage_in_bytes\n\t* cgroup 下进程使用内存的最大值，包含子 cgroup 的内存使用量。\n* memory.limit_in_bytes\n\t* 设置 cgroup 下进程最多能使用的内存。如果设置为 -1，表示对该 cgroup 的内存使用不做限制。\n* memory.soft_limit_in_bytes\n\t* 这个限制并不会阻止进程使用超过限额的内存，只是在系统内存足够时，会优先回收超过限额的内存，使之向限定值靠拢。\n* memory.oom_control\n\t* **设置是否在 Cgroup 中使用 OOM（Out of Memory）Killer，默认为使用。当属于该 cgroup 的进程使用的内存超过最大的限定值时，会立刻被 OOM Killer 处理。**\n\n## Cgroup driver\n\n* systemd:\n\t* 当操作系统使用 systemd 作为 init system 时，初始化进程生成一个根 cgroup 目录结构并作为 cgroup管理器。\n\t* systemd 与 cgroup 紧密结合，并且为每个 systemd unit 分配 cgroup。\n* cgroupfs:\n\t* docker 默认用 cgroupfs 作为 cgroup 驱动。\n* 存在问题：\n\t* 在 systemd 作为 init system 的系统中，默认并存着两套 groupdriver。\n\t* 这会使得系统中 Docker 和 kubelet 管理的进程被 cgroupfs 驱动管，而 systemd 拉起的服务由systemd 驱动管，让 cgroup 管理混乱且容易在资源紧张时引发问题。\n\t* **因此 kubelet 会默认--cgroup-driver=systemd，若运行时 cgroup 不一致时，kubelet 会报错**\n\n# 文件系统\n\n## Union FS\n\n将不同目录挂载到同一个虚拟文件系统下 （unite several directories into a single virtual filesystem）的文件系统\n\n支持为每一个成员目录（类似Git Branch）设定 readonly、readwrite 和 whiteout-able 权限\n\n文件系统分层, 对 readonly 权限的 branch 可以逻辑上进行修改(增量地, 不影响 readonly 部分的)。\n\n通常 Union FS 有两个用途, 一方面可以将多个 disk 挂到同一个目录下, 另一个更常用的就是将一个readonly 的 branch 和一个 writeable 的 branch 联合在一起。\n\n## 容器镜像\n\n![image.png](Assets/image_1665732577611_0.png)\n\n## Overlay FS\n\nOverlayFS 也是一种与 AUFS 类似的联合文件系统，同样属于文件级的存储驱动，包含了最初的 Overlay 和更新更稳定的 overlay2。\n\n**Overlay 只有两层：upper 层和 lower 层，Lower 层代表镜像层，upper 层代表容器可写层。**\n\n![image.png](Assets/image_1665733112134_0.png)\n\n# 网络\n\n## Docker的网络模式\n\n* Null(--net=None)\n\t* 把容器放入独立的网络空间但不做任何网络配置；\n\t* 用户需要通过运行 docker network 命令来完成网络配置。\n* Host\n\t* 使用主机网络名空间，复用主机网络。\n* Container\n\t* 重用其他容器的网络。\n* Bridge(--net=bridge)\n\t* 使用 Linux 网桥和 iptables 提供容器互联，Docker 在每台主机上创建一个名叫 docker0的网桥，通过 veth p\n* Overlay(libnetwork, libkv)\n\t* 通过网络封包实现。\n* Remote(work with remote drivers)\n\t* Underlay：\n\t\t* 使用现有底层网络，为每一个容器配置可路由的网络 IP。\n\t* Overlay：\n\t\t* 通过网络封包实现\n\n## 默认模式– 网桥和 NAT\n\n* 为主机 eth0 分配 IP 192.168.0.101；\n* 启动 docker daemon，查看主机 iptables；\n\t* POSTROUTING -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE\n* 在主机启动容器：\n\t* docker run -d --name ssh -p 2333:22 centos-ssh\n\t* Docker 会以标准模式配置网络：\n\t\t* 创建 veth pair；\n\t\t* 将 veth pair的一端连接到 docker0 网桥；\n\t\t* veth pair 的另外一端设置为容器名空间的 eth0；\n\t\t* 为容器名空间的 eth0 分配 ip；\n\t\t* 主机上的 Iptables 规则：PREROUTING -A DOCKER ! -i docker0 -p tcp -m tcp --dport 2333 -j DNAT --to\u0002destination 172.17.0.2:22。\n\n![image.png](Assets/image_1665738983058_0.png)\n\n## Underlay\n\n采用 Linux 网桥设备（sbrctl），通过物理网络连通容器；\n\n创建新的网桥设备 mydr0；\n\n将主机网卡加入网桥；\n\n把主机网卡的地址配置到网桥，并把默认路由规则转移到网桥 mydr0；\n\n启动容器；\n\n创建 veth 对，并且把一个 peer 添加到网桥 mydr0；\n\n配置容器把 veth 的另一个 peer 分配给容器网卡；\n\n![image.png](Assets/image_1665739288260_0.png)\n\n## Docker Libnetwork Overlay\n\nDocker overlay 网络驱动原生支持多主机网络；\n\nLibnetwork 是一个内置的基于 VXLAN 的网络驱动。\n\n## VXLAN\n\n![image.png](Assets/image_1665739423844_0.png){:height 580, :width 714}\n\n## Overlay network sample – Flannel\n\n同一主机内的 Pod 可以使用网桥进行通信。\n\n不同主机上的 Pod 将通过flanneld 将其流量封装在 UDP数据包中 。\n\n![image.png](Assets/image_1665739505611_0.png)","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Docker"]},"/CS/Docker/Dockerfile":{"title":"Dockerfile","content":"\n# 回顾 12 Factor 之进程\n\n[[系统架构#12 factors|12 factors]]\n\n* 运行环境中，应用程序通常是以一个和多个进程运行的。\n\t* 12-Factor 应用的进程必须无状态（Stateless）且无共享（Share nothing）。\n* 任何需要持久化的数据都要存储在后端服务内，比如数据库。\n\t* 应在构建阶段将源代码编译成待执行应用。\n* Session Sticky 是 12-Factor 极力反对的。\n\t* Session 中的数据应该保存在诸如 Memcached 或 Redis 这样的带有过期时间的缓存中\n* **Docker 遵循以上原则管理和构建应用。**\n\n# 理解构建上下文（Build Context）\n\n* 当运行 docker build 命令时，当前工作目录被称为构建上下文。\n* docker build 默认查找当前目录的 Dockerfile 作为构建输入，也可以通过 –f 指定 Dockerfile。\n\t* `docker build –f ./Dockerfile`\n* 当 docker build 运行时，首先会把构建上下文传输给 docker daemon，把没用的文件包含在构建上下文时，会导致传输时间长，构建需要的资源多，构建出的镜像大等问题。\n\t* 试着到一个包含文件很多的目录运行下面的命令，会感受到差异；\n\t* `docker build -f $GOPATH/src/github.com/cncamp/golang/httpserver/Dockerfile`\n\t* `docker build $GOPATH/src/github.com/cncamp/golang/httpserver/`\n\t* 可以通过.dockerignore文件从编译上下文排除某些文件。\n* 因此需要确保构建上下文清晰，比如创建一个专门的目录放置 Dockerfile，并在目录中运行 docker build。\n\n# Build Cache\n\n* 构建容器镜像时，Docker 依次读取 Dockerfile 中的指令，并按顺序依次执行构建指令。\n* Docker 读取指令后，会先判断缓存中是否有可用的已存镜像，只有已存镜像不存在时才会重新构建。\n\t* 通常 Docker 简单判断 Dockerfile 中的指令与镜像。\n\t* 针对 ADD 和 COPY 指令，Docker 判断该镜像层每一个文件的内容并生成一个 checksum，与现存镜像比较时，Docker 比较的是二者的 checksum。\n\t* 其他指令，比如 RUN apt-get -y update，Docker 简单比较与现存镜像中的指令字串是否一致。\n\t* 当某一层 cache 失效以后，剩下所有层级的 cache 均一并失效，后续指令都重新构建镜像。\n\n# 多段构建（Multi-stage build）\n\n有效减少镜像层级的方式\n\n``` Dockerfile\nFROM golang:1.16-alpine AS build\nRUN apk add --no-cache git\nRUN go get github.com/golang/dep/cmd/dep\nCOPY Gopkg.lock Gopkg.toml /go/src/project/\nWORKDIR /go/src/project/\nRUN dep ensure -vendor-only\nCOPY . /go/src/project/\nRUN go build -o /bin/project（只有这个二进制文件是产线需要的，其他都是waste）\nFROM scratch\nCOPY --from=build /bin/project /bin/project\nENTRYPOINT [\"/bin/project\"]\nCMD [\"--help\"]\n```\n\n# Dockerfile 最佳实践\n\n* 不要安装无效软件包。\n* 应简化镜像中同时运行的进程数，理想状况下，每个镜像应该只有一个进程。\n* 当无法避免同一镜像运行多进程时，应选择合理的初始化进程（init process）。\n* 最小化层级数\n\t* 最新的 docker 只有 RUN， COPY，ADD 创建新层，其他指令创建临时层，不会增加镜像大小。\n\t\t* 比如 EXPOSE 指令就不会生成新层。\n\t* 多条 RUN 命令可通过连接符连接成一条指令集以减少层数。\n\t* 通过多段构建减少镜像层数。\n* 把多行参数按字母排序，可以减少可能出现的重复参数，并且提高可读性。\n* 编写 dockerfile 的时候，应该把变更频率低的编译指令优先构建以便放在镜像底层以有效利用 build cache。\n* 复制文件时，每个文件应独立复制，这确保某个文件变更时，只影响改文件对应的缓存。\n\n**目标：易管理、少漏洞、镜像小、层级少、利用缓存。** ^bb5784\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Docker"]},"/CS/ElasticSearch/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98":{"title":"常见问题","content":"\n## Search加.keyword与不加.keyword的区别，为什么没有结果\n\n\u003e [https://segmentfault.com/q/1010000017312707](https://segmentfault.com/q/1010000017312707)\n\n```bash\nGET production-index-info/index_info/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"minimum_should_match\": 0,\n      \"must\": [\n        {\n          \"term\": {\n            \"is_resolved.keyword\": \"解决\"\n          }\n        }\n      ],\n      \"should\": []\n    }\n  }\n}\n```\n\n1. ES5.0及以后的版本取消了string类型，将原先的string类型拆分为text和keyword两种类型。它们的区别在于text会对字段进行分词处理而keyword则不会。\n\n2. 当你没有以IndexTemplate等形式为你的索引字段预先指定mapping的话，ES就会使用Dynamic Mapping，通过推断你传入的文档中字段的值对字段进行动态映射。例如传入的文档中字段price的值为12，那么price将被映射为long类型；字段addr的值为\"192.168.0.1\"，那么addr将被映射为ip类型。然而对于不满足ip和date格式的普通字符串来说，情况有些不同：ES会将它们映射为text类型，但为了保留对这些字段做精确查询以及聚合的能力，又同时对它们做了keyword类型的映射，作为该字段的fields属性写到_mapping中。例如，当ES遇到一个新的字段\"foobar\": \"some string\"时，会对它做如下的Dynamic Mapping：\n\n```bash\n{\n    \"foobar\": {\n        \"type\" \"text\",\n        \"fields\": {\n            \"keyword\": {\n                \"type\": \"keyword\",\n                \"ignore_above\": 256\n            }\n        }\n    }\n}\n```\n\n在之后的查询中使用foobar是将foobar作为text类型查询，而使用foobar.keyword则是将foobar作为keyword类型查询。前者会对查询内容做分词处理之后再匹配，而后者则是直接对查询结果做精确匹配。\n\n3. ES的term query做的是精确匹配而不是分词查询，因此对text类型的字段做term查询将是查不到结果的（除非字段本身经过分词器处理后不变，未被转换或分词）。此时，必须使用foobar.keyword来对foobar字段以keyword类型进行精确匹配。\n\n**参考资料**\n\n1. [Elasticsearch中Text和Keyword类型的区别](https://www.jianshu.com/p/f5d548559791)\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/ElasticSearch"]},"/CS/ElasticSearch/%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB":{"title":"数据迁移","content":"\n# Elasticdump\n\n适合数据量较小​​的场景\n\n`elasticdump --input=${SOURCE_ES}/$index --output=${DESTINATION_ES}/$index --type data --limit 1000 --overwrite true`\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/ElasticSearch"]},"/CS/ElasticSearch/%E6%9F%A5%E8%AF%A2%E4%B8%8E%E8%81%9A%E5%90%88":{"title":"查询与聚合","content":"\n# 查询和聚合的基础使用\n\n## 查询数据\n\n### 查询所有\n\n`match_all`表示查询所有的数据，`sort`即按照什么字段排序\n\n```bash\nGET /bank/_search\n{\n\"query\": { \"match_all\": {} },\n\"sort\": [\n  { \"account_number\": \"asc\" }\n]\n}\n```\n\n结果\n\n![Untitled.png](Assets/Untitled_1669983783135_0.png)\n\n相关字段解释\n\n`took` – Elasticsearch运行查询所花费的时间（以毫秒为单位）\n\n`timed_out` –搜索请求是否超时\n\n`_shards` - 搜索了多少个碎片，以及成功，失败或跳过了多少个碎片的细目分类。\n\n`max_score` – 找到的最相关文档的分数\n\n`hits.total.value` - 找到了多少个匹配的文档\n\n`hits.sort` - 文档的排序位置（不按相关性得分排序时）\n\n`hits._score` - 文档的相关性得分（使用match_all时不适用）\n\n### 分页查询(from+size)\n\n本质上就是from和size两个字段\n\n```bash\nGET /bank/_search\n{\n\"query\": { \"match_all\": {} },\n\"sort\": [\n  { \"account_number\": \"asc\" }\n],\n\"from\": 10,\n\"size\": 10\n}\n```\n\n### 指定字段查询：match\n\n如果要在字段中搜索特定字词，可以使用`match`; 如下语句将查询address 字段中包含 mill 或者 lane的数据\n\n```bash\nGET /bank/_search\n{\n\"query\": { \"match\": { \"address\": \"mill lane\" } }\n}\n```\n\n### 查询段落匹配：match_phrase\n\n如果我们希望查询的条件是 address字段中包含 \"mill lane\"，则可以使用`match_phrase`\n\n```bash\nGET /bank/_search\n{\n\"query\": { \"match_phrase\": { \"address\": \"mill lane\" } }\n}\n```\n\n### 多条件查询: bool\n\n如果要构造更复杂的查询，可以使用`bool`查询来组合多个查询条件。\n\n例如，以下请求在bank索引中搜索40岁客户的帐户，但不包括居住在爱达荷州（ID）的任何人\n\n```bash\nGET /bank/_search\n{\n\"query\": {\n  \"bool\": {\n    \"must\": [\n      { \"match\": { \"age\": \"40\" } }\n    ],\n    \"must_not\": [\n      { \"match\": { \"state\": \"ID\" } }\n    ]\n  }\n}\n}\n```\n\n### 查询条件：query or filter\n\n先看下如下查询, 在`bool`查询的子句中同时具备query/must 和 filter\n\n```bash\nGET /bank/_search\n{\n\"query\": {\n  \"bool\": {\n    \"must\": [\n      {\n        \"match\": {\n          \"state\": \"ND\"\n        }\n      }\n    ],\n    \"filter\": [\n      {\n        \"term\": {\n          \"age\": \"40\"\n        }\n      },\n      {\n        \"range\": {\n          \"balance\": {\n            \"gte\": 20000,\n            \"lte\": 30000\n          }\n        }\n      }\n    ]\n  }\n}\n}\n```\n\n两者都可以写查询条件，而且语法也类似。区别在于，**query 上下文的条件是用来给文档打分的，匹配越好 \\_score 越高；filter 的条件只产生两种结果：符合与不符合，后者被过滤掉**。\n\n## 聚合查询：Aggregation\n\n\u003e 我们知道SQL中有group by，在ES中它叫Aggregation，即聚合运算。\n\n### 简单聚合\n\n比如我们希望计算出account每个州的统计数量， 使用`aggs`关键字对`state`字段聚合，被聚合的字段无需对分词统计，所以使用`state.keyword`对整个字段统计\n\n```bash\nGET /bank/_search\n{\n\"size\": 0,\n\"aggs\": {\n  \"group_by_state\": {\n    \"terms\": {\n      \"field\": \"state.keyword\"\n    }\n  }\n}\n}\n```\n\n![Untitled 1.png](Assets/Untitled_1_1669984087231_0.png)\n\n因为无需返回条件的具体数据, 所以设置size=0，返回hits为空。\n\n`doc_count`表示bucket中每个州的数据条数。\n\n### 嵌套聚合\n\nES还可以处理个聚合条件的嵌套。\n\n比如承接上个例子， 计算每个州的平均结余。涉及到的就是在对state分组的基础上，嵌套计算avg(balance):\n\n```bash\nGET /bank/_search\n{\n\"size\": 0,\n\"aggs\": {\n  \"group_by_state\": {\n    \"terms\": {\n      \"field\": \"state.keyword\"\n    },\n    \"aggs\": {\n      \"average_balance\": {\n        \"avg\": {\n          \"field\": \"balance\"\n        }\n      }\n    }\n  }\n}\n}\n```\n\n### 对聚合结果排序\n\n可以通过在aggs中对嵌套聚合的结果进行排序\n\n比如承接上个例子， 对嵌套计算出的avg(balance)，这里是average_balance，进行排序\n\n```bash\nGET /bank/_search\n{\n\"size\": 0,\n\"aggs\": {\n  \"group_by_state\": {\n    \"terms\": {\n      \"field\": \"state.keyword\",\n      \"order\": {\n        \"average_balance\": \"desc\"\n      }\n    },\n    \"aggs\": {\n      \"average_balance\": {\n        \"avg\": {\n          \"field\": \"balance\"\n        }\n      }\n    }\n  }\n}\n}\n```\n\n![Untitled 2.png](Assets/Untitled_2_1669984227451_0.png)\n\n# DSL查询\n\n## 复合查询详解\n\n### 复合查询引入\n\n使用`bool`查询来组合多个查询条件。\n\n比如之前介绍的语句\n\n```bash\nGET /bank/_search\n{\n\"query\": {\n  \"bool\": {\n    \"must\": [\n      { \"match\": { \"age\": \"40\" } }\n    ],\n    \"must_not\": [\n      { \"match\": { \"state\": \"ID\" } }\n    ]\n  }\n}\n}\n```\n这种查询就是本文要介绍的**复合查询**，并且bool查询只是复合查询一种。\n\n### bool query(布尔查询)\n\n\u003e 通过布尔逻辑将较小的查询组合成较大的查询。\n\n#### 概念\n\nBool查询语法有以下特点\n\n子查询可以任意顺序出现\n\n可以嵌套多个查询，包括bool查询\n\n如果bool查询中没有must条件，should中必须至少满足一条才会返回结果。\n\nbool查询包含四种操作符，分别是must,should,must_not,filter。他们均是一种数组，数组里面是对应的判断条件。\n\n`must`： 必须匹配。贡献算分\n\n`must_not`：过滤子句，必须不能匹配，但不贡献算分\n\n`should`： 选择性匹配，至少满足一条。贡献算分\n\n`filter`： 过滤子句，必须匹配，但不贡献算分\n\n#### 例子\n\n```bash\nPOST _search\n{\n\"query\": {\n  \"bool\" : {\n    \"must\" : {\n      \"term\" : { \"user.id\" : \"kimchy\" }\n    },\n    \"filter\": {\n      \"term\" : { \"tags\" : \"production\" }\n    },\n    \"must_not\" : {\n      \"range\" : {\n        \"age\" : { \"gte\" : 10, \"lte\" : 20 }\n      }\n    },\n    \"should\" : [\n      { \"term\" : { \"tags\" : \"env1\" } },\n      { \"term\" : { \"tags\" : \"deployed\" } }\n    ],\n    \"minimum_should_match\" : 1,\n    \"boost\" : 1.0\n  }\n}\n}\n```\n\n### boosting query(提高查询)\n\n\u003e 不同于bool查询，bool查询中只要一个子查询条件不匹配那么搜索的数据就不会出现。而boosting query则是降低显示的权重/优先级（即score)。\n\n#### 概念\n\n比如搜索逻辑是 name = 'apple' and type ='fruit'，对于只满足部分条件的数据，不是不显示，而是降低显示的优先级（即score)\n\n#### 例子\n\n首先创建数据\n\n```bash\nPOST /test-dsl-boosting/_bulk\n{ \"index\": { \"_id\": 1 }}\n{ \"content\":\"Apple Mac\" }\n{ \"index\": { \"_id\": 2 }}\n{ \"content\":\"Apple Fruit\" }\n{ \"index\": { \"_id\": 3 }}\n{ \"content\":\"Apple employee like Apple Pie and Apple Juice\" }\n```\n\n对匹配`pie`的做降级显示处理\n\n```bash\nGET /test-dsl-boosting/_search\n{\n\"query\": {\n  \"boosting\": {\n    \"positive\": {\n      \"term\": {\n        \"content\": \"apple\"\n      }\n    },\n    \"negative\": {\n      \"term\": {\n        \"content\": \"pie\"\n      }\n    },\n    \"negative_boost\": 0.5\n  }\n}\n}\n```\n\n执行结果如下\n\n![Untitled 3.png](Assets/Untitled_3_1669984452609_0.png)\n\n### constant_score（固定分数查询）\n\n\u003e 查询某个条件时，固定的返回指定的score；显然当不需要计算score时，只需要filter条件即可，因为filter context忽略score。\n\n#### 例子\n\n首先创建数据\n\n```bash\nPOST /test-dsl-constant/_bulk\n{ \"index\": { \"_id\": 1 }}\n{ \"content\":\"Apple Mac\" }\n{ \"index\": { \"_id\": 2 }}\n{ \"content\":\"Apple Fruit\" }\n```\n\n查询apple\n\n```bash\nGET /test-dsl-constant/_search\n{\n\"query\": {\n  \"constant_score\": {\n    \"filter\": {\n      \"term\": { \"content\": \"apple\" }\n    },\n    \"boost\": 1.2\n  }\n}\n}\n```\n\n![Untitled 4.png](Assets/Untitled_4_1669984505827_0.png)\n\n### dis_max(最佳匹配查询）\n\n\u003e 分离最大化查询（Disjunction Max Query）指的是： 将任何与任一查询匹配的文档作为结果返回，但只将最佳匹配的评分作为查询的评分结果返回 。\n\n#### 例子\n\n假设有个网站允许用户搜索博客的内容，以下面两篇博客内容文档为例：\n\n```bash\nPOST /test-dsl-dis-max/_bulk\n{ \"index\": { \"_id\": 1 }}\n{\"title\": \"Quick brown rabbits\",\"body\":  \"Brown rabbits are commonly seen.\"}\n{ \"index\": { \"_id\": 2 }}\n{\"title\": \"Keeping pets healthy\",\"body\":  \"My quick brown fox eats rabbits on a regular basis.\"}\n```\n\n用户输入词组 “Brown fox” 然后点击搜索按钮。事先，我们并不知道用户的搜索项是会在 title 还是在 body 字段中被找到，但是，用户很有可能是想搜索相关的词组。用肉眼判断，文档 2 的匹配度更高，因为它同时包括要查找的两个词：\n\n现在运行以下 bool 查询：\n\n```bash\nGET /test-dsl-dis-max/_search\n{\n  \"query\": {\n      \"bool\": {\n          \"should\": [\n              { \"match\": { \"title\": \"Brown fox\" }},\n              { \"match\": { \"body\":  \"Brown fox\" }}\n          ]\n      }\n  }\n}\n```\n\n![Untitled 5.png](Assets/Untitled_5_1669984671810_0.png)\n\n**引入了dis_max**\n\n不使用 bool 查询，可以使用 dis_max 即分离 最大化查询（Disjunction Max Query） 。分离（Disjunction）的意思是 或（or） ，这与可以把结合（conjunction）理解成 与（and） 相对应。分离最大化查询（Disjunction Max Query）指的是： 将任何与任一查询匹配的文档作为结果返回，但只将最佳匹配的评分作为查询的评分结果返回 ：\n\n```bash\nGET /test-dsl-dis-max/_search\n{\n  \"query\": {\n      \"dis_max\": {\n          \"queries\": [\n              { \"match\": { \"title\": \"Brown fox\" }},\n              { \"match\": { \"body\":  \"Brown fox\" }}\n          ],\n          \"tie_breaker\": 0\n      }\n  }\n}\n```\n\n![Untitled 6.png](Assets/Untitled_6_1669984695930_0.png)\n\n### function_score(函数查询）\n\n\u003e 简而言之就是用自定义function的方式来计算_score。\n\n可以ES有哪些自定义function呢？\n\n`script_score` 使用自定义的脚本来完全控制分值计算逻辑。如果你需要以上预定义函数之外的功能，可以根据需要通过脚本进行实现。\n\n`weight` 对每份文档适用一个简单的提升，且该提升不会被归约：当weight为2时，结果为2 * _score。\n\n`random_score` 使用一致性随机分值计算来对每个用户采用不同的结果排序方式，对相同用户仍然使用相同的排序方式。\n\n`field_value_factor` 使用文档中某个字段的值来改变_score，比如将受欢迎程度或者投票数量考虑在内。\n\n`衰减函数(Decay Function)` - `linear`，`exp`，`gauss`\n\n## 全文搜索详解\n\n\u003e DSL查询极为常用的是对文本进行搜索，我们叫全文搜索，本文主要对全文搜索进行详解。\n\n### Match类型\n\n#### match 查询的步骤\n\n**准备一些数据**\n\n这里我们准备一些数据，通过实例看match 查询的步骤\n\n```bash\nPUT /test-dsl-match\n{ \"settings\": { \"number_of_shards\": 1 }}\n\nPOST /test-dsl-match/_bulk\n{ \"index\": { \"_id\": 1 }}\n{ \"title\": \"The quick brown fox\" }\n{ \"index\": { \"_id\": 2 }}\n{ \"title\": \"The quick brown fox jumps over the lazy dog\" }\n{ \"index\": { \"_id\": 3 }}\n{ \"title\": \"The quick brown fox jumps over the quick dog\" }\n{ \"index\": { \"_id\": 4 }}\n{ \"title\": \"Brown fox brown dog\" }\n\n```\n\n**查询数据**\n\n```bash\nGET /test-dsl-match/_search\n{\n  \"query\": {\n      \"match\": {\n          \"title\": \"QUICK!\"\n      }\n  }\n}\n\n```\n\nElasticsearch 执行上面这个 match 查询的步骤是：\n\n1.  **检查字段类型** 。\n标题 title 字段是一个 string 类型（ analyzed ）已分析的全文字段，这意味着查询字符串本身也应该被分析。\n\n2. **分析查询字符串** 。\n将查询的字符串 QUICK! 传入标准分析器中，输出的结果是单个项 quick 。因为只有一个单词项，所以 match 查询执行的是单个底层 term 查询。\n\n3. **查找匹配文档** 。\n用 term 查询在倒排索引中查找 quick 然后获取一组包含该项的文档，本例的结果是文档：1、2 和 3 。\n\n4. **为每个文档评分** 。\n用 term 查询计算每个文档相关度评分 \\_score ，这是种将词频（term frequency，即词 quick 在相关文档的 title 字段中出现的频率）和反向文档频率（inverse document frequency，即词 quick 在所有文档的 title 字段中出现的频率），以及字段的长度（即字段越短相关度越高）相结合的计算方式。\n\n**验证结果**\n\n![Untitled 7.png](Assets/Untitled_7_1669985231196_0.png)\n\n#### match多个词深入\n\n我们在上文中复合查询中已经使用了match多个词，比如“Quick pets”； 这里我们通过例子带你更深入理解match多个词\n\n**match多个词的本质**\n\n查询多个词\"BROWN DOG!\"\n\n```bash\nGET /test-dsl-match/_search\n{\n    \"query\": {\n        \"match\": {\n            \"title\": \"BROWN DOG\"\n        }\n    }\n}\n```\n\n![](https://pdai.tech/images/db/es/es-dsl-full-text-5.png)\n\n因为 match 查询必须查找两个词（ [\"brown\",\"dog\"] ），它在内部实际上先执行两次 term 查询，然后将两次查询的结果合并作为最终结果输出。为了做到这点，它将两个 term 查询包入一个 bool 查询中，\n\n所以上述查询的结果，和如下语句查询结果是等同的\n\n```bash\nGET /test-dsl-match/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        {\n          \"term\": {\n            \"title\": \"brown\"\n          }\n        },\n        {\n          \"term\": {\n            \"title\": \"dog\"\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\n**match多个词的逻辑**\n\n上面等同于should（任意一个满足），是因为 match还有一个operator参数，默认是or, 所以对应的是should。\n\n所以上述查询也等同于\n\n```bash\nGET /test-dsl-match/_search\n{\n  \"query\": {\n    \"match\": {\n      \"title\": {\n        \"query\": \"BROWN DOG\",\n        \"operator\": \"or\"\n      }\n    }\n  }\n}\n```\n\n那么我们如果是需要and操作呢，即同时满足呢？\n\n```bash\nGET /test-dsl-match/_search\n{\n  \"query\": {\n    \"match\": {\n      \"title\": {\n        \"query\": \"BROWN DOG\",\n        \"operator\": \"and\"\n      }\n    }\n  }\n}\n```\n\n等同于\n\n```bash\nGET /test-dsl-match/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"term\": {\n            \"title\": \"brown\"\n          }\n        },\n        {\n          \"term\": {\n            \"title\": \"dog\"\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\n![](https://pdai.tech/images/db/es/es-dsl-full-text-7.png)\n\n#### 控制match的匹配精度\n\n如果用户给定 3 个查询词，想查找至少包含其中 2 个的文档，该如何处理？将 operator 操作符参数设置成 and 或者 or 都是不合适的。\n\nmatch 查询支持 minimum_should_match 最小匹配参数，这让我们可以指定必须匹配的词项数用来表示一个文档是否相关。我们可以将其设置为某个具体数字，更常用的做法是将其设置为一个百分数，因为我们无法控制用户搜索时输入的单词数量：\n\n```bash\nGET /test-dsl-match/_search\n{\n  \"query\": {\n    \"match\": {\n      \"title\": {\n        \"query\":                \"quick brown dog\",\n        \"minimum_should_match\": \"75%\"\n      }\n    }\n  }\n}\n```\n\n当给定百分比的时候， minimum_should_match 会做合适的事情：在之前三词项的示例中， 75% 会自动被截断成 66.6% ，即三个里面两个词。无论这个值设置成什么，至少包含两个词项的文档才会被认为是匹配的。\n\n当然也等同于\n\n```bash\nGET /test-dsl-match/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        { \"match\": { \"title\": \"quick\" }},\n        { \"match\": { \"title\": \"brown\"   }},\n        { \"match\": { \"title\": \"dog\"   }}\n      ],\n      \"minimum_should_match\": 2\n    }\n  }\n}\n```\n\n#### 其它match类型\n\n##### match_pharse\n\nmatch_phrase在前文中我们已经有了解，我们再看下另外一个例子。\n\n```bash\nGET /test-dsl-match/_search\n{\n  \"query\": {\n    \"match_phrase\": {\n      \"title\": {\n        \"query\": \"quick brown\"\n      }\n    }\n  }\n}\n```\n\n![](https://pdai.tech/images/db/es/es-dsl-full-text-11.png)\n\n很多人对它仍然有误解的，比如如下例子：\n\n```bash\nGET /test-dsl-match/_search\n{\n  \"query\": {\n    \"match_phrase\": {\n      \"title\": {\n        \"query\": \"quick brown f\"\n      }\n    }\n  }\n}\n```\n\n这样的查询是查不出任何数据的，因为前文中我们知道了match本质上是对term组合，match_phrase本质是连续的term的查询，所以f并不是一个分词，不满足term查询，所以最终查不出任何内容了。\n\n![](https://pdai.tech/images/db/es/es-dsl-full-text-12.png)\n\n##### match_pharse_prefix\n\n那有没有可以查询出`quick brown f`的方式呢？ELasticSearch在match_phrase基础上提供了一种可以查最后一个词项是前缀的方法，这样就可以查询`quick brown f`了\n\n```bash\nGET /test-dsl-match/_search\n{\n  \"query\": {\n    \"match_phrase_prefix\": {\n      \"title\": {\n        \"query\": \"quick brown f\"\n      }\n    }\n  }\n}\n```\n\n![](https://pdai.tech/images/db/es/es-dsl-full-text-13.png)\n\n(ps: prefix的意思不是整个text的开始匹配，而是最后一个词项满足term的prefix查询而已)\n\n##### match_bool_prefix\n\n除了match_phrase_prefix，ElasticSearch还提供了match_bool_prefix查询\n\n```bash\nGET /test-dsl-match/_search\n{\n  \"query\": {\n    \"match_bool_prefix\": {\n      \"title\": {\n        \"query\": \"quick brown f\"\n      }\n    }\n  }\n}\n```\n\n![](https://pdai.tech/images/db/es/es-dsl-full-text-14.png)\n\n它们两种方式有啥区别呢？match_bool_prefix本质上可以转换为：\n\n```bash\nGET /test-dsl-match/_search\n{\n  \"query\": {\n    \"bool\" : {\n      \"should\": [\n        { \"term\": { \"title\": \"quick\" }},\n        { \"term\": { \"title\": \"brown\" }},\n        { \"prefix\": { \"title\": \"f\"}}\n      ]\n    }\n  }\n}\n```\n\n所以这样你就能理解，match_bool_prefix查询中的quick,brown,f是无序的。\n\n##### multi_match\n\n如果我们期望一次对多个字段查询，怎么办呢？ElasticSearch提供了multi_match查询的方式\n\n```bash\n{\n  \"query\": {\n    \"multi_match\" : {\n      \"query\":    \"Will Smith\",\n      \"fields\": [ \"title\", \"*_name\" ]\n    }\n  }\n}\n```\n\n`*`表示前缀匹配字段。\n\n### query string类型\n\n#### query_string\n\n此查询使用语法根据运算符（例如AND或）来解析和拆分提供的查询字符串NOT。然后查询在返回匹配的文档之前独立分析每个拆分的文本。\n\n可以使用该query_string查询创建一个复杂的搜索，其中包括通配符，跨多个字段的搜索等等。尽管用途广泛，但查询是严格的，如果查询字符串包含任何无效语法，则返回错误。\n\n例如：\n\n```bash\nGET /test-dsl-match/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"(lazy dog) OR (brown dog)\",\n      \"default_field\": \"title\"\n    }\n  }\n}\n```\n\n这里查询结果，你需要理解本质上查询这四个分词（term）or的结果而已，所以doc 3和4也在其中\n\n![](https://pdai.tech/images/db/es/es-dsl-full-text-15.png)\n\n#### query_string_simple\n\n该查询使用一种简单的语法来解析提供的查询字符串并将其拆分为基于特殊运算符的术语。然后查询在返回匹配的文档之前独立分析每个术语。\n\n尽管其语法比query_string查询更受限制 ，但**simple_query_string 查询不会针对无效语法返回错误。而是，它将忽略查询字符串的任何无效部分**。\n\n举例：\n\n```bash\nGET /test-dsl-match/_search\n{\n  \"query\": {\n    \"simple_query_string\" : {\n        \"query\": \"\\\"over the\\\" + (lazy | quick) + dog\",\n        \"fields\": [\"title\"],\n        \"default_operator\": \"and\"\n    }\n  }\n}\n```\n\n![](https://pdai.tech/images/db/es/es-dsl-full-text-16.png)\n\n### Interval类型\n\nIntervals是时间间隔的意思，本质上将多个规则按照顺序匹配。\n\n```bash\nGET /test-dsl-match/_search\n{\n  \"query\": {\n    \"intervals\" : {\n      \"title\" : {\n        \"all_of\" : {\n          \"ordered\" : true,\n          \"intervals\" : [\n            {\n              \"match\" : {\n                \"query\" : \"quick\",\n                \"max_gaps\" : 0,\n                \"ordered\" : true\n              }\n            },\n            {\n              \"any_of\" : {\n                \"intervals\" : [\n                  { \"match\" : { \"query\" : \"jump over\" } },\n                  { \"match\" : { \"query\" : \"quick dog\" } }\n                ]\n              }\n            }\n          ]\n        }\n      }\n    }\n  }\n}\n```\n\n![](https://pdai.tech/images/db/es/es-dsl-full-text-17.png)\n\n因为interval之间是可以组合的，所以它可以表现的很复杂。\n\n## Term详解\n\n### 准备数据\n\n```bash\nPUT /test-dsl-term-level\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"keyword\"\n      },\n      \"programming_languages\": {\n        \"type\": \"keyword\"\n      },\n      \"required_matches\": {\n        \"type\": \"long\"\n      }\n    }\n  }\n}\n\nPOST /test-dsl-term-level/_bulk\n{ \"index\": { \"_id\": 1 }}\n{\"name\": \"Jane Smith\", \"programming_languages\": [ \"c++\", \"java\" ], \"required_matches\": 2}\n{ \"index\": { \"_id\": 2 }}\n{\"name\": \"Jason Response\", \"programming_languages\": [ \"java\", \"php\" ], \"required_matches\": 2}\n{ \"index\": { \"_id\": 3 }}\n{\"name\": \"Dave Pdai\", \"programming_languages\": [ \"java\", \"c++\", \"php\" ], \"required_matches\": 3, \"remarks\": \"hello world\"}\n```\n\n### 字段是否存在:exist\n\n由于多种原因，文档字段的索引值可能不存在：\n\n源JSON中的字段是null或[]\n\n该字段已\"index\" : false在映射中设置\n\n字段值的长度超出ignore_above了映射中的设置\n\n字段值格式错误，并且ignore_malformed已在映射中定义\n\n所以exist表示查找是否存在字段。\n\n![](https://pdai.tech/images/db/es/es-dsl-term-2.png)\n\n### id查询:ids\n\nids 即对id查找\n\n```bash\nGET /test-dsl-term-level/_search\n{\n  \"query\": {\n    \"ids\": {\n      \"values\": [3, 1]\n    }\n  }\n}\n```\n\n![](https://pdai.tech/images/db/es/es-dsl-term-3.png)\n\n### 前缀:prefix\n\n通过前缀查找某个字段\n\n```bash\nGET /test-dsl-term-level/_search\n{\n  \"query\": {\n    \"prefix\": {\n      \"name\": {\n        \"value\": \"Jan\"\n      }\n    }\n  }\n}\n```\n\n![](https://pdai.tech/images/db/es/es-dsl-term-4.png)\n\n### 分词匹配:term\n\n前文最常见的根据分词查询\n\n```bash\nGET /test-dsl-term-level/_search\n{\n\"query\": {\n  \"term\": {\n    \"programming_languages\": \"php\"\n  }\n}\n}\n```\n\n![](https://pdai.tech/images/db/es/es-dsl-term-5.png)\n\n### 多个分词匹配:terms\n\n按照多个分词term匹配，它们是or的关系\n\n```bash\nGET /test-dsl-term-level/_search\n{\n  \"query\": {\n    \"terms\": {\n      \"programming_languages\": [\"php\",\"c++\"]\n    }\n  }\n}\n```\n\n![](https://pdai.tech/images/db/es/es-dsl-term-6.png)\n\n### 按某个数字字段分词匹配:term set\n\n设计这种方式查询的初衷是用文档中的数字字段动态匹配查询满足term的个数\n\n```bash\nGET /test-dsl-term-level/_search\n{\n  \"query\": {\n    \"terms_set\": {\n      \"programming_languages\": {\n        \"terms\": [ \"java\", \"php\" ],\n        \"minimum_should_match_field\": \"required_matches\"\n      }\n    }\n  }\n}\n```\n\n![](https://pdai.tech/images/db/es/es-dsl-term-7.png)\n\n### 通配符:wildcard\n\n通配符匹配，比如`*`\n\n```bash\nGET /test-dsl-term-level/_search\n{\n  \"query\": {\n    \"wildcard\": {\n      \"name\": {\n        \"value\": \"D*ai\",\n        \"boost\": 1.0,\n        \"rewrite\": \"constant_score\"\n      }\n    }\n  }\n}\n```\n\n![](https://pdai.tech/images/db/es/es-dsl-term-8.png)\n\n### 范围:range\n\n常常被用在数字或者日期范围的查询\n\n```bash\nGET /test-dsl-term-level/_search\n{\n  \"query\": {\n    \"range\": {\n      \"required_matches\": {\n        \"gte\": 3,\n        \"lte\": 4\n      }\n    }\n  }\n}\n```\n\n![](https://pdai.tech/images/db/es/es-dsl-term-9.png)\n\n### 正则:regexp\n\n通过[正则表达式](https://pdai.tech/md/develop/regex/dev-regex-all.html)查询\n\n以\"Jan\"开头的name字段\n\n```bash\nGET /test-dsl-term-level/_search\n{\n  \"query\": {\n    \"regexp\": {\n      \"name\": {\n        \"value\": \"Ja.*\",\n        \"case_insensitive\": true\n      }\n    }\n  }\n}\n```\n\n![](https://pdai.tech/images/db/es/es-dsl-term-10.png)\n\n### 模糊匹配:fuzzy\n\n官方文档对模糊匹配：编辑距离是将一个术语转换为另一个术语所需的一个字符更改的次数。这些更改可以包括：\n\n更改字符（box→ fox）\n\n删除字符（black→ lack）\n\n插入字符（sic→ sick）\n\n转置两个相邻字符（act→ cat）\n\n```bash\nGET /test-dsl-term-level/_search\n{\n  \"query\": {\n    \"fuzzy\": {\n      \"remarks\": {\n        \"value\": \"hell\"\n      }\n    }\n  }\n}\n```\n\n![](https://pdai.tech/images/db/es/es-dsl-term-11.png)\n聚合查询\n\n## Bucket聚合详解\n\n### 聚合的引入\n\n我们在SQL结果中常有：\n\n```\nSELECT COUNT(color)\nFROM table\nGROUP BY color\n```\n\nElasticSearch中**桶**在概念上类似于 SQL 的分组（`GROUP BY`），而**指标**则类似于 `COUNT()` 、 `SUM()` 、 `MAX()` 等统计方法。\n\n进而引入了两个概念：\n\n**桶（Buckets）** 满足特定条件的文档的集合\n\n**指标（Metrics）** 对桶内的文档进行统计计算\n\n所以ElasticSearch包含3种聚合（Aggregation)方式\n\n**桶聚合（Bucket Aggregation)** - 本文中详解\n\n**指标聚合（Metric Aggregation)** - 下文中讲解\n\n**管道聚合（Pipline Aggregation)** - 再下一篇讲解\n\n聚合管道化，简单而言就是上一个聚合的结果成为下个聚合的输入；\n\n（PS:指标聚合和桶聚合很多情况下是组合在一起使用的，其实你也可以看到，桶聚合本质上是一种特殊的指标聚合，它的聚合指标就是数据的条数count)\n\n### 按知识点学习聚合\n\n#### 准备数据\n\n让我们先看一个例子。我们将会创建一些对汽车经销商有用的聚合，数据是关于汽车交易的信息：车型、制造商、售价、何时被出售等。\n\n首先我们批量索引一些数据：\n\n```bash\nPOST /test-agg-cars/_bulk\n{ \"index\": {}}\n{ \"price\" : 10000, \"color\" : \"red\", \"make\" : \"honda\", \"sold\" : \"2014-10-28\" }\n{ \"index\": {}}\n{ \"price\" : 20000, \"color\" : \"red\", \"make\" : \"honda\", \"sold\" : \"2014-11-05\" }\n{ \"index\": {}}\n{ \"price\" : 30000, \"color\" : \"green\", \"make\" : \"ford\", \"sold\" : \"2014-05-18\" }\n{ \"index\": {}}\n{ \"price\" : 15000, \"color\" : \"blue\", \"make\" : \"toyota\", \"sold\" : \"2014-07-02\" }\n{ \"index\": {}}\n{ \"price\" : 12000, \"color\" : \"green\", \"make\" : \"toyota\", \"sold\" : \"2014-08-19\" }\n{ \"index\": {}}\n{ \"price\" : 20000, \"color\" : \"red\", \"make\" : \"honda\", \"sold\" : \"2014-11-05\" }\n{ \"index\": {}}\n{ \"price\" : 80000, \"color\" : \"red\", \"make\" : \"bmw\", \"sold\" : \"2014-01-01\" }\n{ \"index\": {}}\n{ \"price\" : 25000, \"color\" : \"blue\", \"make\" : \"ford\", \"sold\" : \"2014-02-12\" }\n```\n\n#### 标准的聚合\n\n有了数据，开始构建我们的第一个聚合。汽车经销商可能会想知道哪个颜色的汽车销量最好，用聚合可以轻易得到结果，用 terms 桶操作：\n\n```bash\nGET /test-agg-cars/_search\n{\n    \"size\" : 0,\n    \"aggs\" : {\n        \"popular_colors\" : {\n            \"terms\" : {\n              \"field\" : \"color.keyword\"\n            }\n        }\n    }\n}\n```\n\n1. 聚合操作被置于顶层参数 aggs 之下（如果你愿意，完整形式 aggregations 同样有效）。\n2. 然后，可以为聚合指定一个我们想要名称，本例中是： popular_colors 。\n3. 最后，定义单个桶的类型 terms 。\n\n结果如下：\n\n![](https://pdai.tech/images/db/es/es-agg-bucket-3.png)\n\n因为我们设置了 size 参数，所以不会有 hits 搜索结果返回。\n\npopular_colors 聚合是作为 aggregations 字段的一部分被返回的。\n\n每个桶的 key 都与 color 字段里找到的唯一词对应。它总会包含 doc_count 字段，告诉我们包含该词项的文档数量。\n\n每个桶的数量代表该颜色的文档数量。\n\n#### 多个聚合\n\n同时计算两种桶的结果：对color和对make。\n\n```bash\nGET /test-agg-cars/_search\n{\n    \"size\" : 0,\n    \"aggs\" : {\n        \"popular_colors\" : {\n            \"terms\" : {\n              \"field\" : \"color.keyword\"\n            }\n        },\n        \"make_by\" : {\n            \"terms\" : {\n              \"field\" : \"make.keyword\"\n            }\n        }\n    }\n}\n```\n\n结果如下：\n\n![](https://pdai.tech/images/db/es/es-agg-bucket-4.png)\n\n#### 聚合的嵌套\n\n这个新的聚合层让我们可以将 avg 度量嵌套置于 terms 桶内。实际上，这就为每个颜色生成了平均价格。\n\n```bash\nGET /test-agg-cars/_search\n{\n   \"size\" : 0,\n   \"aggs\": {\n      \"colors\": {\n         \"terms\": {\n            \"field\": \"color.keyword\"\n         },\n         \"aggs\": {\n            \"avg_price\": {\n               \"avg\": {\n                  \"field\": \"price\"\n               }\n            }\n         }\n      }\n   }\n}\n```\n\n结果如下：\n\n![](https://pdai.tech/images/db/es/es-agg-bucket-5.png)\n\n正如 颜色 的例子，我们需要给度量起一个名字（ avg_price ）这样可以稍后根据名字获取它的值。最后，我们指定度量本身（ avg ）以及我们想要计算平均值的字段（ price ）\n\n#### 动态脚本的聚合\n\n这个例子告诉你，ElasticSearch还支持一些基于脚本（生成运行时的字段）的复杂的动态聚合。\n\n```bash\nGET /test-agg-cars/_search\n{\n  \"runtime_mappings\": {\n    \"make.length\": {\n      \"type\": \"long\",\n      \"script\": \"emit(doc['make.keyword'].value.length())\"\n    }\n  },\n  \"size\" : 0,\n  \"aggs\": {\n    \"make_length\": {\n      \"histogram\": {\n        \"interval\": 1,\n        \"field\": \"make.length\"\n      }\n    }\n  }\n}\n```\n\n结果如下：\n\n![](https://pdai.tech/images/db/es/es-agg-bucket-6.png)\n\n### 按分类学习Bucket聚合\n\n\u003e 我们在具体学习时，也无需学习每一个点，基于上面图的认知，我们只需用20%的时间学习最为常用的80%功能即可，其它查查文档而已。@pdai\n\n#### 前置条件的过滤：filter\n\n在当前文档集上下文中定义与指定过滤器(Filter)匹配的所有文档的单个存储桶。通常，这将用于将当前聚合上下文缩小到一组特定的文档。\n\n```bash\nGET /test-agg-cars/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"make_by\": {\n      \"filter\": { \"term\": { \"type\": \"honda\" } },\n      \"aggs\": {\n        \"avg_price\": { \"avg\": { \"field\": \"price\" } }\n      }\n    }\n  }\n}\n```\n\n结果如下：\n\n![](https://pdai.tech/images/db/es/es-agg-bucket-7.png)\n\n#### 对filter进行分组聚合：filters\n\n设计一个新的例子, 日志系统中，每条日志都是在文本中，包含warning/info等信息。\n\n```bash\nPUT /test-agg-logs/_bulk?refresh\n{ \"index\" : { \"_id\" : 1 } }\n{ \"body\" : \"warning: page could not be rendered\" }\n{ \"index\" : { \"_id\" : 2 } }\n{ \"body\" : \"authentication error\" }\n{ \"index\" : { \"_id\" : 3 } }\n{ \"body\" : \"warning: connection timed out\" }\n{ \"index\" : { \"_id\" : 4 } }\n{ \"body\" : \"info: hello pdai\" }\n```\n\n我们需要对包含不同日志类型的日志进行分组，这就需要filters:\n\n```bash\nGET /test-agg-logs/_search\n{\n  \"size\": 0,\n  \"aggs\" : {\n    \"messages\" : {\n      \"filters\" : {\n        \"other_bucket_key\": \"other_messages\",\n        \"filters\" : {\n          \"infos\" :   { \"match\" : { \"body\" : \"info\"   }},\n          \"warnings\" : { \"match\" : { \"body\" : \"warning\" }}\n        }\n      }\n    }\n  }\n}\n```\n\n结果如下：\n\n![](https://pdai.tech/images/db/es/es-agg-bucket-8.png)\n\n#### 对number类型聚合：Range\n\n基于多桶值源的聚合，使用户能够定义一组范围-每个范围代表一个桶。在聚合过程中，将从每个存储区范围中检查从每个文档中提取的值，并“存储”相关/匹配的文档。请注意，此聚合包括from值，但不包括to每个范围的值。\n\n```bash\nGET /test-agg-cars/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"price_ranges\": {\n      \"range\": {\n        \"field\": \"price\",\n        \"ranges\": [\n          { \"to\": 20000 },\n          { \"from\": 20000, \"to\": 40000 },\n          { \"from\": 40000 }\n        ]\n      }\n    }\n  }\n}\n```\n\n结果如下：\n\n![](https://pdai.tech/images/db/es/es-agg-bucket-9.png){:height 452, :width 654}\n\n#### 对IP类型聚合：IP Range\n\n专用于IP值的范围聚合。\n\n```bash\nGET /ip_addresses/_search\n{\n  \"size\": 10,\n  \"aggs\": {\n    \"ip_ranges\": {\n      \"ip_range\": {\n        \"field\": \"ip\",\n        \"ranges\": [\n          { \"to\": \"10.0.0.5\" },\n          { \"from\": \"10.0.0.5\" }\n        ]\n      }\n    }\n  }\n}\n```\n\n返回\n\n```bash\n{\n  ...\n\n  \"aggregations\": {\n    \"ip_ranges\": {\n      \"buckets\": [\n        {\n          \"key\": \"*-10.0.0.5\",\n          \"to\": \"10.0.0.5\",\n          \"doc_count\": 10\n        },\n        {\n          \"key\": \"10.0.0.5-*\",\n          \"from\": \"10.0.0.5\",\n          \"doc_count\": 260\n        }\n      ]\n    }\n  }\n}\n```\n\n**CIDR Mask分组**\n\n此外还可以用CIDR Mask分组\n\n```bash\nGET /ip_addresses/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"ip_ranges\": {\n      \"ip_range\": {\n        \"field\": \"ip\",\n        \"ranges\": [\n          { \"mask\": \"10.0.0.0/25\" },\n          { \"mask\": \"10.0.0.127/25\" }\n        ]\n      }\n    }\n  }\n}\n```\n\n返回\n\n```bash\n{\n  ...\n\n  \"aggregations\": {\n    \"ip_ranges\": {\n      \"buckets\": [\n        {\n          \"key\": \"10.0.0.0/25\",\n          \"from\": \"10.0.0.0\",\n          \"to\": \"10.0.0.128\",\n          \"doc_count\": 128\n        },\n        {\n          \"key\": \"10.0.0.127/25\",\n          \"from\": \"10.0.0.0\",\n          \"to\": \"10.0.0.128\",\n          \"doc_count\": 128\n        }\n      ]\n    }\n  }\n}\n```\n\n**增加key显示**\n\n```bash\nGET /ip_addresses/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"ip_ranges\": {\n      \"ip_range\": {\n        \"field\": \"ip\",\n        \"ranges\": [\n          { \"to\": \"10.0.0.5\" },\n          { \"from\": \"10.0.0.5\" }\n        ],\n        \"keyed\": true // here\n      }\n    }\n  }\n}\n```\n\n返回\n\n```bash\n{\n  ...\n\n  \"aggregations\": {\n    \"ip_ranges\": {\n      \"buckets\": {\n        \"*-10.0.0.5\": {\n          \"to\": \"10.0.0.5\",\n          \"doc_count\": 10\n        },\n        \"10.0.0.5-*\": {\n          \"from\": \"10.0.0.5\",\n          \"doc_count\": 260\n        }\n      }\n    }\n  }\n}\n```\n\n**自定义key显示**\n\n```bash\nGET /ip_addresses/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"ip_ranges\": {\n      \"ip_range\": {\n        \"field\": \"ip\",\n        \"ranges\": [\n          { \"key\": \"infinity\", \"to\": \"10.0.0.5\" },\n          { \"key\": \"and-beyond\", \"from\": \"10.0.0.5\" }\n        ],\n        \"keyed\": true\n      }\n    }\n  }\n}\n```\n\n返回\n\n```bash\n{\n  ...\n\n  \"aggregations\": {\n    \"ip_ranges\": {\n      \"buckets\": {\n        \"infinity\": {\n          \"to\": \"10.0.0.5\",\n          \"doc_count\": 10\n        },\n        \"and-beyond\": {\n          \"from\": \"10.0.0.5\",\n          \"doc_count\": 260\n        }\n      }\n    }\n  }\n}\n```\n\n#### 对日期类型聚合：Date Range\n\n专用于日期值的范围聚合。\n\n```bash\nGET /test-agg-cars/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"range\": {\n      \"date_range\": {\n        \"field\": \"sold\",\n        \"format\": \"yyyy-MM\",\n        \"ranges\": [\n          { \"from\": \"2014-01-01\" },\n          { \"to\": \"2014-12-31\" }\n        ]\n      }\n    }\n  }\n}\n```\n\n结果如下：\n\n![](https://pdai.tech/images/db/es/es-agg-bucket-10.png)\n\n此聚合与Range聚合之间的主要区别在于 from和to值可以在[Date Math表达式](https://www.elastic.co/guide/en/elasticsearch/reference/7.12/search-aggregations-bucket-daterange-aggregation.html#date-format-pattern)中表示，并且还可以指定日期格式，通过该日期格式将返回from and to响应字段。请注意，此聚合包括from值，但**不包括to每个范围的值**。\n\n#### 对柱状图功能：Histrogram\n\n直方图 histogram 本质上是就是为柱状图功能设计的。\n\n创建直方图需要指定一个区间，如果我们要为售价创建一个直方图，可以将间隔设为 20,000。这样做将会在每个 $20,000 档创建一个新桶，然后文档会被分到对应的桶中。\n\n对于仪表盘来说，我们希望知道每个售价区间内汽车的销量。我们还会想知道每个售价区间内汽车所带来的收入，可以通过对每个区间内已售汽车的售价求和得到。\n\n可以用 histogram 和一个嵌套的 sum 度量得到我们想要的答案：\n\n```bash\nGET /test-agg-cars/_search\n{\n   \"size\" : 0,\n   \"aggs\":{\n      \"price\":{\n         \"histogram\":{\n            \"field\": \"price.keyword\",\n            \"interval\": 20000\n         },\n         \"aggs\":{\n            \"revenue\": {\n               \"sum\": {\n                 \"field\" : \"price\"\n               }\n             }\n         }\n      }\n   }\n}\n```\n\n1. histogram 桶要求两个参数：一个数值字段以及一个定义桶大小间隔。\n2. sum 度量嵌套在每个售价区间内，用来显示每个区间内的总收入。\n\n如我们所见，查询是围绕 price 聚合构建的，它包含一个 histogram 桶。它要求字段的类型必须是数值型的同时需要设定分组的间隔范围。 间隔设置为 20,000 意味着我们将会得到如 [0-19999, 20000-39999, ...] 这样的区间。\n\n接着，我们在直方图内定义嵌套的度量，这个 sum 度量，它会对落入某一具体售价区间的文档中 price 字段的值进行求和。 这可以为我们提供每个售价区间的收入，从而可以发现到底是普通家用车赚钱还是奢侈车赚钱。\n\n响应结果如下：\n\n![](https://pdai.tech/images/db/es/es-agg-bucket-11.png)\n\n结果很容易理解，不过应该注意到直方图的键值是区间的下限。键 0 代表区间 0-19，999 ，键 20000 代表区间 20，000-39，999 ，等等。\n\n![](https://pdai.tech/images/db/es/es-agg-bucket-33.png)\n\n当然，我们可以为任何聚合输出的分类和统计结果创建条形图，而不只是 直方图 桶。让我们以最受欢迎 10 种汽车以及它们的平均售价、标准差这些信息创建一个条形图。 我们会用到 terms 桶和 extended_stats 度量：\n\n```bash\nGET /test-agg-cars/_search\n{\n  \"size\" : 0,\n  \"aggs\": {\n    \"makes\": {\n      \"terms\": {\n        \"field\": \"make.keyword\",\n        \"size\": 10\n      },\n      \"aggs\": {\n        \"stats\": {\n          \"extended_stats\": {\n            \"field\": \"price\"\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n上述代码会按受欢迎度返回制造商列表以及它们各自的统计信息。我们对其中的 stats.avg 、 stats.count 和 stats.std_deviation 信息特别感兴趣，并用 它们计算出标准差：\n\n```bash\nstd_err = std_deviation / count\n```\n\n![](https://pdai.tech/images/db/es/es-agg-bucket-12.png)\n\n![](https://pdai.tech/images/db/es/es-agg-bucket-34.png)\n\n## Metric聚合详解\n\n### 如何理解metric聚合\n\n\u003e  metric聚合又如何理解呢？我认为从两个角度：\n\n**从分类看**：Metric聚合分析分为**单值分析**和**多值分析**两类\n\n**从功能看**：根据具体的应用场景设计了一些分析api, 比如地理位置，百分数等等\n\n\u003e  融合上述两个方面，我们可以梳理出大致的一个mind图：\n\n* **单值分析**: 只输出一个分析结果\n\t* 标准stat型\n\t\t* `avg` 平均值\n\t\t* `max` 最大值\n\t\t* `min` 最小值\n\t\t* `sum` 和\n\t\t* `value_count` 数量\n\t* 其它类型\n\t\t* `cardinality` 基数（distinct去重）\n\t\t* `weighted_avg` 带权重的avg\n\t\t* `median_absolute_deviation` 中位值\n* **多值分析**: 单值之外的\n\t* stats型\n\t\t* `stats` 包含avg,max,min,sum和count\n\t\t* `matrix_stats` 针对矩阵模型\n\t\t* `extended_stats`\n\t\t* `string_stats` 针对字符串\n\t* 百分数型\n\t\t* `percentiles` 百分数范围\n\t\t* `percentile_ranks` 百分数排行\n\t* 地理位置型\n\t\t* `geo_bounds` Geo bounds\n\t\t* `geo_centroid` Geo-centroid\n\t\t* `geo_line` Geo-Line\n\t* Top型\n\t\t* `top_hits` 分桶后的top hits\n\t\t* `top_metrics`\n\n\u003e **通过上述列表（我就不画图了），我们构筑的体系是基于`分类`和`功能`，而不是具体的项（比如avg,percentiles...)；这是不同的认知维度: 具体的项是碎片化，分类和功能这种是你需要构筑的体系**。@pdai\n\n### 单值分析: 标准stat类型\n\n#### `avg`   平均值\n\n计算班级的平均分\n\n```bash\nPOST /exams/_search?size=0\n{\n  \"aggs\": {\n    \"avg_grade\": { \"avg\": { \"field\": \"grade\" } }\n  }\n}\n```\n\n返回\n\n```bash\n{\n  ...\n  \"aggregations\": {\n    \"avg_grade\": {\n      \"value\": 75.0\n    }\n  }\n}\n```\n\n#### `max`   最大值\n\n计算销售最高价\n\n```bash\nPOST /sales/_search?size=0\n{\n  \"aggs\": {\n    \"max_price\": { \"max\": { \"field\": \"price\" } }\n  }\n}\n```\n\n返回\n\n```bash\n{\n  ...\n  \"aggregations\": {\n      \"max_price\": {\n          \"value\": 200.0\n      }\n  }\n}\n```\n\n### `min`   最小值\n\n计算销售最低价\n\n```bash\nPOST /sales/_search?size=0\n{\n  \"aggs\": {\n    \"min_price\": { \"min\": { \"field\": \"price\" } }\n  }\n}\n```\n\n返回\n\n```bash\n{\n  ...\n\n  \"aggregations\": {\n    \"min_price\": {\n      \"value\": 10.0\n    }\n  }\n}\n```\n\n#### `sum`   和\n\n计算销售总价\n\n```bash\nPOST /sales/_search?size=0\n{\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"match\": { \"type\": \"hat\" }\n      }\n    }\n  },\n  \"aggs\": {\n    \"hat_prices\": { \"sum\": { \"field\": \"price\" } }\n  }\n}\n```\n\n返回\n\n```bash\n{\n  ...\n  \"aggregations\": {\n    \"hat_prices\": {\n      \"value\": 450.0\n    }\n  }\n}\n```\n\n#### `value_count`   数量\n\n销售数量统计\n\n```bash\nPOST /sales/_search?size=0\n{\n  \"aggs\" : {\n    \"types_count\" : { \"value_count\" : { \"field\" : \"type\" } }\n  }\n}\n```\n\n返回\n\n```bash\n{\n  ...\n  \"aggregations\": {\n    \"types_count\": {\n      \"value\": 7\n    }\n  }\n}\n```\n\n### 单值分析: 其它类型\n\n#### `weighted_avg`   带权重的avg\n\n```bash\nPOST /exams/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"weighted_grade\": {\n      \"weighted_avg\": {\n        \"value\": {\n          \"field\": \"grade\"\n        },\n        \"weight\": {\n          \"field\": \"weight\"\n        }\n      }\n    }\n  }\n}\n```\n\n返回\n\n```bash\n{\n  ...\n  \"aggregations\": {\n    \"weighted_grade\": {\n      \"value\": 70.0\n    }\n  }\n}\n```\n\n#### `cardinality`   基数（distinct去重）\n\n```bash\nPOST /sales/_search?size=0\n{\n  \"aggs\": {\n    \"type_count\": {\n      \"cardinality\": {\n        \"field\": \"type\"\n      }\n    }\n  }\n}\n```\n\n返回\n\n```bash\n{\n  ...\n  \"aggregations\": {\n    \"type_count\": {\n      \"value\": 3\n    }\n  }\n}\n```\n\n#### `median_absolute_deviation`   中位值\n\n```bash\nGET reviews/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"review_average\": {\n      \"avg\": {\n        \"field\": \"rating\"\n      }\n    },\n    \"review_variability\": {\n      \"median_absolute_deviation\": {\n        \"field\": \"rating\"\n      }\n    }\n  }\n}\n```\n\n返回\n\n```bash\n{\n  ...\n  \"aggregations\": {\n    \"review_average\": {\n      \"value\": 3.0\n    },\n    \"review_variability\": {\n      \"value\": 2.0\n    }\n  }\n}\n```\n\n### 非单值分析：stats型\n\n#### `stats`   包含avg,max,min,sum和count\n\n```bash\nPOST /exams/_search?size=0\n{\n  \"aggs\": {\n    \"grades_stats\": { \"stats\": { \"field\": \"grade\" } }\n  }\n}\n```\n\n返回\n\n```bash\n{\n  ...\n\n  \"aggregations\": {\n    \"grades_stats\": {\n      \"count\": 2,\n      \"min\": 50.0,\n      \"max\": 100.0,\n      \"avg\": 75.0,\n      \"sum\": 150.0\n    }\n  }\n}\n```\n\n#### `matrix_stats`   针对矩阵模型\n\n以下示例说明了使用矩阵统计量来描述收入与贫困之间的关系。\n\n```bash\nGET /_search\n{\n  \"aggs\": {\n    \"statistics\": {\n      \"matrix_stats\": {\n        \"fields\": [ \"poverty\", \"income\" ]\n      }\n    }\n  }\n}\n```\n\n返回\n\n```bash\n{\n  ...\n  \"aggregations\": {\n    \"statistics\": {\n      \"doc_count\": 50,\n      \"fields\": [ {\n          \"name\": \"income\",\n          \"count\": 50,\n          \"mean\": 51985.1,\n          \"variance\": 7.383377037755103E7,\n          \"skewness\": 0.5595114003506483,\n          \"kurtosis\": 2.5692365287787124,\n          \"covariance\": {\n            \"income\": 7.383377037755103E7,\n            \"poverty\": -21093.65836734694\n          },\n          \"correlation\": {\n            \"income\": 1.0,\n            \"poverty\": -0.8352655256272504\n          }\n        }, {\n          \"name\": \"poverty\",\n          \"count\": 50,\n          \"mean\": 12.732000000000001,\n          \"variance\": 8.637730612244896,\n          \"skewness\": 0.4516049811903419,\n          \"kurtosis\": 2.8615929677997767,\n          \"covariance\": {\n            \"income\": -21093.65836734694,\n            \"poverty\": 8.637730612244896\n          },\n          \"correlation\": {\n            \"income\": -0.8352655256272504,\n            \"poverty\": 1.0\n          }\n        } ]\n    }\n  }\n}\n```\n\n#### `extended_stats`\n\n根据从汇总文档中提取的数值计算统计信息。\n\n```bash\nGET /exams/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"grades_stats\": { \"extended_stats\": { \"field\": \"grade\" } }\n  }\n}\n```\n\n上面的汇总计算了所有文档的成绩统计信息。聚合类型为extended_stats，并且字段设置定义将在其上计算统计信息的文档的数字字段。\n\n```bash\n{\n  ...\n\n  \"aggregations\": {\n    \"grades_stats\": {\n      \"count\": 2,\n      \"min\": 50.0,\n      \"max\": 100.0,\n      \"avg\": 75.0,\n      \"sum\": 150.0,\n      \"sum_of_squares\": 12500.0,\n      \"variance\": 625.0,\n      \"variance_population\": 625.0,\n      \"variance_sampling\": 1250.0,\n      \"std_deviation\": 25.0,\n      \"std_deviation_population\": 25.0,\n      \"std_deviation_sampling\": 35.35533905932738,\n      \"std_deviation_bounds\": {\n        \"upper\": 125.0,\n        \"lower\": 25.0,\n        \"upper_population\": 125.0,\n        \"lower_population\": 25.0,\n        \"upper_sampling\": 145.71067811865476,\n        \"lower_sampling\": 4.289321881345245\n      }\n    }\n  }\n}\n```\n\n#### `string_stats`   针对字符串\n\n用于计算从聚合文档中提取的字符串值的统计信息。这些值可以从特定的关键字字段中检索。\n\n```bash\nPOST /my-index-000001/_search?size=0\n{\n  \"aggs\": {\n    \"message_stats\": { \"string_stats\": { \"field\": \"message.keyword\" } }\n  }\n}\n```\n\n返回\n\n```bash\n{\n  ...\n\n  \"aggregations\": {\n    \"message_stats\": {\n      \"count\": 5,\n      \"min_length\": 24,\n      \"max_length\": 30,\n      \"avg_length\": 28.8,\n      \"entropy\": 3.94617750050791\n    }\n  }\n}\n```\n\n### 非单值分析：百分数型\n\n#### `percentiles`   百分数范围\n\n针对从聚合文档中提取的数值计算一个或多个百分位数。\n\n```bash\nGET latency/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"load_time_outlier\": {\n      \"percentiles\": {\n        \"field\": \"load_time\"\n      }\n    }\n  }\n}\n```\n\n默认情况下，百分位度量标准将生成一定范围的百分位：[1，5，25，50，75，95，99]。\n\n```bash\n\n  ...\n\n \"aggregations\": {\n    \"load_time_outlier\": {\n      \"values\": {\n        \"1.0\": 5.0,\n        \"5.0\": 25.0,\n        \"25.0\": 165.0,\n        \"50.0\": 445.0,\n        \"75.0\": 725.0,\n        \"95.0\": 945.0,\n        \"99.0\": 985.0\n      }\n    }\n  }\n}\n```\n\n#### `percentile_ranks`   百分数排行\n\n根据从汇总文档中提取的数值计算一个或多个百分位等级。\n\n```bash\nGET latency/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"load_time_ranks\": {\n      \"percentile_ranks\": {\n        \"field\": \"load_time\",\n        \"values\": [ 500, 600 ]\n      }\n    }\n  }\n}\n```\n\n返回\n\n```bash\n{\n  ...\n\n \"aggregations\": {\n    \"load_time_ranks\": {\n      \"values\": {\n        \"500.0\": 90.01,\n        \"600.0\": 100.0\n      }\n    }\n  }\n}\n```\n\n上述结果表示90.01％的页面加载在500ms内完成，而100％的页面加载在600ms内完成。\n\n### 非单值分析：地理位置型\n\n#### `geo_bounds`   Geo bounds\n\n```bash\nPUT /museums\n{\n  \"mappings\": {\n    \"properties\": {\n      \"location\": {\n        \"type\": \"geo_point\"\n      }\n    }\n  }\n}\n\nPOST /museums/_bulk?refresh\n{\"index\":{\"_id\":1}}\n{\"location\": \"52.374081,4.912350\", \"name\": \"NEMO Science Museum\"}\n{\"index\":{\"_id\":2}}\n{\"location\": \"52.369219,4.901618\", \"name\": \"Museum Het Rembrandthuis\"}\n{\"index\":{\"_id\":3}}\n{\"location\": \"52.371667,4.914722\", \"name\": \"Nederlands Scheepvaartmuseum\"}\n{\"index\":{\"_id\":4}}\n{\"location\": \"51.222900,4.405200\", \"name\": \"Letterenhuis\"}\n{\"index\":{\"_id\":5}}\n{\"location\": \"48.861111,2.336389\", \"name\": \"Musée du Louvre\"}\n{\"index\":{\"_id\":6}}\n{\"location\": \"48.860000,2.327000\", \"name\": \"Musée d'Orsay\"}\n\nPOST /museums/_search?size=0\n{\n  \"query\": {\n    \"match\": { \"name\": \"musée\" }\n  },\n  \"aggs\": {\n    \"viewport\": {\n      \"geo_bounds\": {\n        \"field\": \"location\",\n        \"wrap_longitude\": true\n      }\n    }\n  }\n}\n```\n\n上面的汇总展示了如何针对具有商店业务类型的所有文档计算位置字段的边界框\n\n```bash\n{\n  ...\n  \"aggregations\": {\n    \"viewport\": {\n      \"bounds\": {\n        \"top_left\": {\n          \"lat\": 48.86111099738628,\n          \"lon\": 2.3269999679178\n        },\n        \"bottom_right\": {\n          \"lat\": 48.85999997612089,\n          \"lon\": 2.3363889567553997\n        }\n      }\n    }\n  }\n}\n```\n\n#### `geo_centroid`   Geo-centroid\n\n```bash\nPUT /museums\n{\n  \"mappings\": {\n    \"properties\": {\n      \"location\": {\n        \"type\": \"geo_point\"\n      }\n    }\n  }\n}\n\nPOST /museums/_bulk?refresh\n{\"index\":{\"_id\":1}}\n{\"location\": \"52.374081,4.912350\", \"city\": \"Amsterdam\", \"name\": \"NEMO Science Museum\"}\n{\"index\":{\"_id\":2}}\n{\"location\": \"52.369219,4.901618\", \"city\": \"Amsterdam\", \"name\": \"Museum Het Rembrandthuis\"}\n{\"index\":{\"_id\":3}}\n{\"location\": \"52.371667,4.914722\", \"city\": \"Amsterdam\", \"name\": \"Nederlands Scheepvaartmuseum\"}\n{\"index\":{\"_id\":4}}\n{\"location\": \"51.222900,4.405200\", \"city\": \"Antwerp\", \"name\": \"Letterenhuis\"}\n{\"index\":{\"_id\":5}}\n{\"location\": \"48.861111,2.336389\", \"city\": \"Paris\", \"name\": \"Musée du Louvre\"}\n{\"index\":{\"_id\":6}}\n{\"location\": \"48.860000,2.327000\", \"city\": \"Paris\", \"name\": \"Musée d'Orsay\"}\n\nPOST /museums/_search?size=0\n{\n  \"aggs\": {\n    \"centroid\": {\n      \"geo_centroid\": {\n        \"field\": \"location\"\n      }\n    }\n  }\n}\n```\n\n上面的汇总显示了如何针对所有具有犯罪类型的盗窃文件计算位置字段的质心。\n\n```bash\n{\n  ...\n  \"aggregations\": {\n    \"centroid\": {\n      \"location\": {\n        \"lat\": 51.00982965203002,\n        \"lon\": 3.9662131341174245\n      },\n      \"count\": 6\n    }\n  }\n}\n```\n\n#### `geo_line`   Geo-Line\n\n```bash\nPUT test\n{\n    \"mappings\": {\n        \"dynamic\": \"strict\",\n        \"_source\": {\n            \"enabled\": false\n        },\n        \"properties\": {\n            \"my_location\": {\n                \"type\": \"geo_point\"\n            },\n            \"group\": {\n                \"type\": \"keyword\"\n            },\n            \"@timestamp\": {\n                \"type\": \"date\"\n            }\n        }\n    }\n}\n\nPOST /test/_bulk?refresh\n{\"index\": {}}\n{\"my_location\": {\"lat\":37.3450570, \"lon\": -122.0499820}, \"@timestamp\": \"2013-09-06T16:00:36\"}\n{\"index\": {}}\n{\"my_location\": {\"lat\": 37.3451320, \"lon\": -122.0499820}, \"@timestamp\": \"2013-09-06T16:00:37Z\"}\n{\"index\": {}}\n{\"my_location\": {\"lat\": 37.349283, \"lon\": -122.0505010}, \"@timestamp\": \"2013-09-06T16:00:37Z\"}\n\nPOST /test/_search?filter_path=aggregations\n{\n  \"aggs\": {\n    \"line\": {\n      \"geo_line\": {\n        \"point\": {\"field\": \"my_location\"},\n        \"sort\": {\"field\": \"@timestamp\"}\n      }\n    }\n  }\n}\n```\n\n将存储桶中的所有geo_point值聚合到由所选排序字段排序的LineString中。\n\n```bash\n{\n  \"aggregations\": {\n    \"line\": {\n      \"type\" : \"Feature\",\n      \"geometry\" : {\n        \"type\" : \"LineString\",\n        \"coordinates\" : [\n          [\n            -122.049982,\n            37.345057\n          ],\n          [\n            -122.050501,\n            37.349283\n          ],\n          [\n            -122.049982,\n            37.345132\n          ]\n        ]\n      },\n      \"properties\" : {\n        \"complete\" : true\n      }\n    }\n  }\n}\n```\n\n### 非单值分析：Top型\n\n#### `top_hits`   分桶后的top hits\n\n```bash\nPOST /sales/_search?size=0\n{\n  \"aggs\": {\n    \"top_tags\": {\n      \"terms\": {\n        \"field\": \"type\",\n        \"size\": 3\n      },\n      \"aggs\": {\n        \"top_sales_hits\": {\n          \"top_hits\": {\n            \"sort\": [\n              {\n                \"date\": {\n                  \"order\": \"desc\"\n                }\n              }\n            ],\n            \"_source\": {\n              \"includes\": [ \"date\", \"price\" ]\n            },\n            \"size\": 1\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n返回\n\n```bash\n{\n  ...\n  \"aggregations\": {\n    \"top_tags\": {\n       \"doc_count_error_upper_bound\": 0,\n       \"sum_other_doc_count\": 0,\n       \"buckets\": [\n          {\n             \"key\": \"hat\",\n             \"doc_count\": 3,\n             \"top_sales_hits\": {\n                \"hits\": {\n                   \"total\" : {\n                       \"value\": 3,\n                       \"relation\": \"eq\"\n                   },\n                   \"max_score\": null,\n                   \"hits\": [\n                      {\n                         \"_index\": \"sales\",\n                         \"_type\": \"_doc\",\n                         \"_id\": \"AVnNBmauCQpcRyxw6ChK\",\n                         \"_source\": {\n                            \"date\": \"2015/03/01 00:00:00\",\n                            \"price\": 200\n                         },\n                         \"sort\": [\n                            1425168000000\n                         ],\n                         \"_score\": null\n                      }\n                   ]\n                }\n             }\n          },\n          {\n             \"key\": \"t-shirt\",\n             \"doc_count\": 3,\n             \"top_sales_hits\": {\n                \"hits\": {\n                   \"total\" : {\n                       \"value\": 3,\n                       \"relation\": \"eq\"\n                   },\n                   \"max_score\": null,\n                   \"hits\": [\n                      {\n                         \"_index\": \"sales\",\n                         \"_type\": \"_doc\",\n                         \"_id\": \"AVnNBmauCQpcRyxw6ChL\",\n                         \"_source\": {\n                            \"date\": \"2015/03/01 00:00:00\",\n                            \"price\": 175\n                         },\n                         \"sort\": [\n                            1425168000000\n                         ],\n                         \"_score\": null\n                      }\n                   ]\n                }\n             }\n          },\n          {\n             \"key\": \"bag\",\n             \"doc_count\": 1,\n             \"top_sales_hits\": {\n                \"hits\": {\n                   \"total\" : {\n                       \"value\": 1,\n                       \"relation\": \"eq\"\n                   },\n                   \"max_score\": null,\n                   \"hits\": [\n                      {\n                         \"_index\": \"sales\",\n                         \"_type\": \"_doc\",\n                         \"_id\": \"AVnNBmatCQpcRyxw6ChH\",\n                         \"_source\": {\n                            \"date\": \"2015/01/01 00:00:00\",\n                            \"price\": 150\n                         },\n                         \"sort\": [\n                            1420070400000\n                         ],\n                         \"_score\": null\n                      }\n                   ]\n                }\n             }\n          }\n       ]\n    }\n  }\n}\n```\n\n#### `top_metrics`\n\n```bash\nPOST /test/_bulk?refresh\n{\"index\": {}}\n{\"s\": 1, \"m\": 3.1415}\n{\"index\": {}}\n{\"s\": 2, \"m\": 1.0}\n{\"index\": {}}\n{\"s\": 3, \"m\": 2.71828}\nPOST /test/_search?filter_path=aggregations\n{\n  \"aggs\": {\n    \"tm\": {\n      \"top_metrics\": {\n        \"metrics\": {\"field\": \"m\"},\n        \"sort\": {\"s\": \"desc\"}\n      }\n    }\n  }\n}\n```\n\n返回\n\n```bash\n{\n  \"aggregations\": {\n    \"tm\": {\n      \"top\": [ {\"sort\": [3], \"metrics\": {\"m\": 2.718280076980591 } } ]\n    }\n  }\n}\n```\n\n## Pipline聚合详解\n\n### 如何理解pipeline聚合\n\n如何理解管道聚合呢？最重要的是要站在设计者角度看这个功能的要实现的目的：让上一步的聚合结果成为下一个聚合的输入，这就是管道。\n\n#### 管道机制的常见场景\n\n##### 责任链模式\n\n管道机制在设计模式上属于责任链模式，如果你不理解，请参看如下文章：\n\n[责任链模式(Chain of responsibility pattern)](/md/dev-spec/pattern/15_chain.html): 通过责任链模式, 你可以为某个请求创建一个对象链. 每个对象依序检查此请求并对其进行处理或者将它传给链中的下一个对象。\n\n##### FilterChain\n\n在软件开发的常接触的责任链模式是FilterChain，它体现在很多软件设计中：\n\n**比如Spring Security框架中**\n\n![](https://pdai.tech/images/tomcat/tomcat-x-pipline-6.jpg)\n\n**比如HttpServletRequest处理的过滤器中**\n\n当一个request过来的时候，需要对这个request做一系列的加工，使用责任链模式可以使每个加工组件化，减少耦合。也可以使用在当一个request过来的时候，需要找到合适的加工方式。当一个加工方式不适合这个request的时候，传递到下一个加工方法，该加工方式再尝试对request加工。\n\n![](https://pdai.tech/images/tomcat/tomcat-x-pipline-5.jpg)\n\n#### ElasticSearch设计管道机制\n\n简单而言：让上一步的聚合结果成为下一个聚合的输入，这就是管道。\n\n接下来，无非就是对不同类型的聚合有接口的支撑，比如：\n\n\u003e 第一个维度：管道聚合有很多不同**类型**，每种类型都与其他聚合计算不同的信息，但是可以将这些类型分为两类：\n\n**父级** 父级聚合的输出提供了一组管道聚合，它可以计算新的存储桶或新的聚合以添加到现有存储桶中。\n\n**兄弟** 同级聚合的输出提供的管道聚合，并且能够计算与该同级聚合处于同一级别的新聚合。\n\n\u003e 第二个维度：根据**功能设计**的意图\n\n比如前置聚合可能是Bucket聚合，后置的可能是基于Metric聚合，那么它就可以成为一类管道\n\n进而引出了：`xxx bucket`(是不是很容易理解了 @pdai)\n\n**Bucket聚合 -\u003e Metric聚合**： bucket聚合的结果，成为下一步metric聚合的输入\n\n* Average bucket\n* Min bucket\n* Max bucket\n* Sum bucket\n* Stats bucket\n* Extended stats bucket\n\n对构建体系而言，理解上面的已经够了，其它的类型不过是锦上添花而言。\n\n### 一些例子\n\n#### Average bucket 聚合\n\n```bash\nPOST _search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"sales_per_month\": {\n      \"date_histogram\": {\n        \"field\": \"date\",\n        \"calendar_interval\": \"month\"\n      },\n      \"aggs\": {\n        \"sales\": {\n          \"sum\": {\n            \"field\": \"price\"\n          }\n        }\n      }\n    },\n    \"avg_monthly_sales\": {\n// tag::avg-bucket-agg-syntax[]\n      \"avg_bucket\": {\n        \"buckets_path\": \"sales_per_month\u003esales\",\n        \"gap_policy\": \"skip\",\n        \"format\": \"#,##0.00;(#,##0.00)\"\n      }\n// end::avg-bucket-agg-syntax[]\n    }\n  }\n}\n```\n\n嵌套的bucket聚合：聚合出按月价格的直方图\n\nMetic聚合：对上面的聚合再求平均值。\n\n**字段类型**：\n\nbuckets_path：指定聚合的名称，支持多级嵌套聚合。\n\ngap_policy 当管道聚合遇到不存在的值，有点类似于term等聚合的(missing)时所采取的策略，可选择值为：skip、insert_zeros。\n\nskip：此选项将丢失的数据视为bucket不存在。它将跳过桶并使用下一个可用值继续计算。\n\nformat 用于格式化聚合桶的输出(key)。\n\n输出结果如下\n\n```bash\n{\n  \"took\": 11,\n  \"timed_out\": false,\n  \"_shards\": ...,\n  \"hits\": ...,\n  \"aggregations\": {\n    \"sales_per_month\": {\n      \"buckets\": [\n        {\n          \"key_as_string\": \"2015/01/01 00:00:00\",\n          \"key\": 1420070400000,\n          \"doc_count\": 3,\n          \"sales\": {\n            \"value\": 550.0\n          }\n        },\n        {\n          \"key_as_string\": \"2015/02/01 00:00:00\",\n          \"key\": 1422748800000,\n          \"doc_count\": 2,\n          \"sales\": {\n            \"value\": 60.0\n          }\n        },\n        {\n          \"key_as_string\": \"2015/03/01 00:00:00\",\n          \"key\": 1425168000000,\n          \"doc_count\": 2,\n          \"sales\": {\n            \"value\": 375.0\n          }\n        }\n      ]\n    },\n    \"avg_monthly_sales\": {\n      \"value\": 328.33333333333333,\n      \"value_as_string\": \"328.33\"\n    }\n  }\n}\n```\n\n#### Stats bucket 聚合\n\n进一步的stat bucket也很容易理解了\n\n```bash\nPOST /sales/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"sales_per_month\": {\n      \"date_histogram\": {\n        \"field\": \"date\",\n        \"calendar_interval\": \"month\"\n      },\n      \"aggs\": {\n        \"sales\": {\n          \"sum\": {\n            \"field\": \"price\"\n          }\n        }\n      }\n    },\n    \"stats_monthly_sales\": {\n      \"stats_bucket\": {\n        \"buckets_path\": \"sales_per_month\u003esales\"\n      }\n    }\n  }\n}\n```\n\n返回\n\n```bash\n{\n   \"took\": 11,\n   \"timed_out\": false,\n   \"_shards\": ...,\n   \"hits\": ...,\n   \"aggregations\": {\n      \"sales_per_month\": {\n         \"buckets\": [\n            {\n               \"key_as_string\": \"2015/01/01 00:00:00\",\n               \"key\": 1420070400000,\n               \"doc_count\": 3,\n               \"sales\": {\n                  \"value\": 550.0\n               }\n            },\n            {\n               \"key_as_string\": \"2015/02/01 00:00:00\",\n               \"key\": 1422748800000,\n               \"doc_count\": 2,\n               \"sales\": {\n                  \"value\": 60.0\n               }\n            },\n            {\n               \"key_as_string\": \"2015/03/01 00:00:00\",\n               \"key\": 1425168000000,\n               \"doc_count\": 2,\n               \"sales\": {\n                  \"value\": 375.0\n               }\n            }\n         ]\n      },\n      \"stats_monthly_sales\": {\n         \"count\": 3,\n         \"min\": 60.0,\n         \"max\": 550.0,\n         \"avg\": 328.3333333333333,\n         \"sum\": 985.0\n      }\n   }\n}\n```\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/ElasticSearch"]},"/CS/Golang/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86":{"title":"内存管理","content":"\n# 堆内存管理\n\n![image.png](Assets/image_1665644324748_0.png)\n\n# 堆内存管理\n\n初始化连续内存块作为堆\n\n有内存申请的时候，Allocator 从堆内存的未分配区域分割小内存块\n\n用链表将已分配内存连接起来\n\n需要信息描述每个内存块的元数据：大小，是否使用，下一个内存块的地址等\n\n![image.png](Assets/image_1665644795653_0.png)\n\n# TCMalloc 概览\n\n![image.png](Assets/image_1665645649424_0.png)\n\n* page:内存页，一块 8K 大小的内存空间。Go 与操作系统之间的内存申请和释放，都是以page 为单位\n* span: 内存块，一个或多个连续的 page 组成一个 span\n* sizeclass : 空间规格，每个 span 都带有一个 sizeclass ，标记着该 span 中的 page 应该如何\n* 使用\n* object : 对象，用来存储一个变量数据内存空间，一个 span 在初始化时，会被切割成一堆等大的 object ；假设 object 的大小是 16B ，span 大小是 8K ，那么就会把 span 中的 page 就会被初始化 8K / 16B = 512 个 object 。所谓内存分配，就是分配一个 object 出去\n\n* 对象大小定义\n\t* 小对象大小：0~256KB\n\t* 中对象大小：256KB~1MB\n\t* 大对象大小：\u003e1MB\n* 小对象的分配流程\n\t* ThreadCache -\u003e CentralCache -\u003e HeapPage，大部分时候，ThreadCache 缓存都是足够的，不需要去访问CentralCache 和 HeapPage，无系统调用配合无锁分配，分配效率是非常高的\n* 中对象分配流程\n\t* 直接在 PageHeap 中选择适当的大小即可，128 Page 的 Span 所保存的最大内存就是 1MB\n* 大对象分配流程\n\t* 从 large span set 选择合适数量的页面组成 span，用来存储数据\n\n# Go语言内存分配\n\n![image.png](Assets/image_1665646120105_0.png)\n\n两个大小一样的span class对应一个size class，一个存指针，一个存直接引用存直接引用的span无需内存回收\n\n* mcache：小对象的内存分配直接走\n\t* size class 从 1 到 66，每个 class 两个 span\n\t* Span 大小是 8KB，按 span class 大小切分\n* mcentral\n\t* Span 内的所有内存块都被占用时，没有剩余空间继续分配对象，mcache 会向 mcentral 申请1个span，mcache 拿到 span 后继续分配对象\n\t* 当 mcentral 向 mcache 提供 span 时，如果没有符合条件的 span，mcentral 会向 mheap 申请span\n* mheap\n\t* 当 mheap 没有足够的内存时，mheap 会向 OS 申请内存\n\t* Mheap 把 Span 组织成了树结构，而不是链表\n\t* 然后把 Span 分配到 heapArena 进行管理，它包含地址映射和 span 是否包含指针等位图\n\t\t* 为了更高效的分配、回收和再利用内存\n\n# 内存回收\n\n* 引用计数（Python，PHP，Swift）\n\t* 对每一个对象维护一个引用计数，当引用该对象的对象被销毁的候，引用计数减 1，当引用计数为 0 的时候，回收该对象\n\t* 优点：对象可以很快的被回收，不会出现内存耗尽或达到某个阀值时才回收\n\t* 缺点：不能很好的处理循环引用，而且实时维护引用计数，有也一定的代价\n* 标记-清除（Golang）\n\t* 从根变量开始遍历所有引用的对象，引用的对象标记为\"被引用\"，没有被标记进行回收\n\t* 优点：解决引用计数的缺点\n\t* 缺点：需要 STW（stop the word），即要暂停程序运行\n* 分代收集（Java）\n\t* 按照生命周期进行划分不同的代空间，生命周期长的放入老年代，短的放入新生代，新生代的回收频率高于老年代的频率\n\n# mspan\n\n* allocBits\n\t* 记录了每块内存分配的情况\n* gcmarkBits\n\t* 记录了每块内存的引用情况，标记阶段对每块内存进行标记，有对象引用的内存标记为1，没有的标记为 0\n\n![image.png](Assets/image_1665647360865_0.png)\n\n这两个位图的数据结构是完全一致的，标记结束则进行内存回收，回收的时候，将 allocBits 指向 gcmarkBits，标记过的则存在，未进行标记的则进行回收\n\n![image.png](Assets/image_1665647455220_0.png)\n\n# GC工作流程\n\n* Golang GC 的大部分处理是和用户代码并行的\n* Mark：\n\t* Mark Prepare: 初始化 GC 任务，包括开启写屏障 (write barrier) 和辅助 GC(mutator assist)，统计root对象的任务数量等。这个过程需要STW\n\t* GC Drains: 扫描所有 root 对象，包括全局指针和 goroutine(G) 栈上的指针（扫描对应 G 栈时需停止该G)，将其加入标记队列(灰色队列)，并循环处理灰色队列的对象，直到灰色队列为空。该过程后台并行执行\n* Mark Termination：完成标记工作，重新扫描(re-scan)全局指针和栈。因为 Mark 和用户程序是并行的，所以在 Mark 过程中可能会有新的对象分配和指针赋值，这个时候就需要通过写屏障（write barrier）记录下来，re-scan 再检查一下，这个过程也是会 STW 的\n* Sweep：按照标记结果回收所有的白色对象，该过程后台并行执行\n* Sweep Termination：对未清扫的 span 进行清扫, 只有上一轮的 GC 的清扫工作完成才可以开始新一轮的 GC\n\n![image.png](Assets/image_1665648006969_0.png)\n\n# 三色标记\n\nGC 开始时，认为所有 object 都是 白色，即垃圾。\n\n从 root 区开始遍历，被触达的 object 置成 灰色。\n\n遍历所有灰色 object，将他们内部的引用变量置成 灰色，自身置成 黑色\n\n循环第 3 步，直到没有灰色 object 了，只剩下了黑白两种，白色的都是垃圾。\n\n对于黑色 object，如果在标记期间发生了写操作，写屏障会在真正赋值前将新对象标记为 灰色。\n\n标记过程中，mallocgc 新分配的 object，会先被标记成 黑色 再返回。\n\n![image.png](Assets/image_1665648093332_0.png)\n\n# 垃圾回收触发机制\n\n* 内存分配量达到阀值触发 GC\n\t* 每次内存分配时都会检查当前内存分配量是否已达到阀值，如果达到阀值则立即启动 GC。\n\t\t* 阀值 = 上次GC 内存分配量 * 内存增长率\n\t\t* 内存增长率由环境变量 GOGC 控制，默认为 100，即每当内存扩大一倍时启动 GC。\n* 定期触发 GC\n\t* 默认情况下，最长 2 分钟触发一次 GC，这个间隔在 src/runtime/proc.go:forcegcperiod 变量中被声明\n* 手动触发\n\t* 程序代码中也可以使用 runtime.GC()来手动触发 GC。这主要用于 GC 性能测试和统计。\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Golang"]},"/CS/Golang/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1":{"title":"面向对象","content":"\n# 常用方法\n\n* 可见性控制\n\t* public - 常量、变量、类型、接口、结构、函数等的名称大写\n\t* private - 非大写就只能在包内使用\n* 继承\n\t* 通过组合实现，内嵌一个或多个 struct\n* 多态\n\t* 通过接口实现，通过接口定义方法集，编写多套实现","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Golang"]},"/CS/Golang/GMP":{"title":"GMP","content":"\n\u003e 有关系统的进程和进程的知识，可参考 [[Linux进程]]\n\n# 协程\n\n* 进程：\n\t* 分配系统资源（CPU 时间、内存等）基本单位\n\t* 有独立的内存空间，切换开销大\n* 线程：进程的一个执行流，是 CPU 调度并能独立运行的的基本单位\n\t* 同一进程中的多线程共享内存空间，线程切换代价小\n\t* 多线程通信方便\n\t* 从内核层面来看线程其实也是一种特殊的进程，它跟父进程共享了打开的文件和文件系统信息，共享了地址空间和信号处理函数\n* 协程\n\t* Go 语言中的轻量级线程实现\n\t* Golang 在 runtime、系统调用等多方面对 goroutine 调度进行了封装和处理，当遇到长时间执行或者进行系统调用时，会主动把当前 goroutine 的 CPU (P) 转让出去，让其他 goroutine 能被调度并执行，也就是 Golang 从语言层面支持了协程\n\n# 线程和协程的差异\n\n* 每个 goroutine (协程) 默认占用内存远比 Java 、C 的线程少\n\t* goroutine：2KB\n\t* 线程：8MB\n* 线程/goroutine 切换开销方面，goroutine 远比线程小\n\t* 线程：涉及模式切换(从用户态切换到内核态)、16个寄存器、PC、SP...等寄存器的刷新\n\t* goroutine：只有三个寄存器的值修改 - PC / SP / DX.\n* GOMAXPROCS\n\t* 控制并行线程数量\n\n# Goroutine\n\nGo 语言基于 GMP 模型实现用户态线程\n\n* G：表示 goroutine，每个 goroutine 都有自己的栈空间，定时器，初始化的栈空间在 2k 左右，空间会随着需求增长。\n* M：抽象化代表内核线程，记录内核线程栈信息，当 goroutine 调度到线程时，使用该 goroutine 自己的栈信息。\n* P：代表调度器，负责调度 goroutine，维护一个本地 goroutine 队列，M 从 P 上获得 goroutine 并执行，同时还负责部分内存的管理。P的数量一般与CPU数量一直。\n\n![image.png](Assets/image_1665580343096_0.png)\n\n# GMP的对应关系\n\n![image.png](Assets/image_1665581569027_0.png)\n\n# GMP模型细节\n\n![image.png](Assets/image_1665581701236_0.png)\n\n# G 所处的位置\n\n* 进程都有一个全局的 G 队列\n* 每个 P 拥有自己的本地执行队列\n* 有不在运行队列中的 G\n\t* 处于 channel 阻塞态的 G 被放在 sudog\n\t* 脱离 P 绑定在 M 上的 G，如系统调用\n\t* 为了复用，执行结束进入 P 的 gFree 列表中的 G\n\n# Goroutine 创建过程\n\n* 获取或者创建新的 Goroutine 结构体\n\t* 从处理器的 gFree 列表中查找空闲的 Goroutine\n\t* 如果不存在空闲的 Goroutine，会通过 runtime.malg 创建一个栈大小足够的新结构体\n* 将函数传入的参数移到 Goroutine 的栈上\n* 更新 Goroutine 调度相关的属性，更新状态为_Grunnable\n* 返回的 Goroutine 会存储到全局变量 allgs 中\n\n# 将 Goroutine 放到运行队列上\n\nGoroutine 设置到处理器的 runnext 作为下一个处理器执行的任务\n\n当处理器的**本地运行队列已经没有剩余空间时（256）**，就会把本地队列中的一部分 Goroutine 和待加入的 Goroutine **通过 runtime.runqputslow 添加到调度器持有的全局运行队列上**\n\n# 调度器行为\n\n* 为了保证公平，当全局运行队列中有待执行的 Goroutine 时，通过 schedtick 保证有一定几率（1/61）会从**全局的运行队列**中查找对应的 Goroutine\n* 从处理器**本地的运行队列**中查找待执行的 Goroutine\n* 如果前两种方法都没有找到 Goroutine，会通过 runtime.findrunnable **进行阻塞地查找**Goroutine\n\t* 从本地运行队列、全局运行队列中查找\n\t* 从网络轮询器中查找是否有 Goroutine 等待运行\n\t* 通过 runtime.runqsteal 尝试从其他随机的处理器中窃取待运行的 Goroutine\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Golang"]},"/CS/Golang/Modules":{"title":"Modules","content":"\n开启modules\n\n`$ go env -w GO111MODULE=on`\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Golang"]},"/CS/Golang/Proxy":{"title":"Proxy","content":"\n# GOPROXY 和 GOPRIVAT\n\n为拉取 Go 依赖设置代理\n\n`export GOPROXY=https://goproxy.cn`\n\n在设置 GOPROXY 以后，默认所有依赖拉取都需要经过 proxy 连接 git repo，拉取代码，并做checksum 校验\n\n某些私有代码仓库是 goproxy.cn 无法连接的，因此需要设置 GOPRIVATE 来声明私有代码仓库\n\n`GOPRIVATE=*.corp.example.com`\n`GOPROXY=proxy.example.com`\n`GONOPROXY=myrepo.corp.example.com`\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Golang"]},"/CS/Kubernetes/%E5%9F%BA%E4%BA%8EIstio%E7%9A%84%E9%AB%98%E7%BA%A7%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86":{"title":"基于Istio的高级流量管理","content":"\n# 微服务架构的演变\n\n## Evolution\n\n![image.png](Assets/image_1667232096191_0.png)\n\n![image.png](Assets/image_1667232110413_0.png)\n\n## 从单块系统到微服务系统的演进\n\n![image.png](Assets/image_1667232153471_0.png)\n\n## 微服务架构的演进\n\n![image.png](Assets/image_1667232200200_0.png)\n\n## 典型的微服务业务场景\n\n![image.png](Assets/image_1667232226792_0.png)\n\n## 更完整的微服务架构\n\n![image.png](Assets/image_1667232293928_0.png)\n\n## 系统边界\n\n![image.png](Assets/image_1667232353905_0.png)\n\n# 微服务到服务网格还缺什么?\n\n## Sidecar 的工作原理\n\n![image.png](Assets/image_1667232736020_0.png)\n\n## Service Mesh\n\n![image.png](Assets/image_1667232759244_0.png)\n\n**适应性**\n- 熔断\n- 重试\n- 超时处理\n- 失败处理\n- 负载均衡\n- Failover\n\n**服务发现**\n- 路由\n\n**安全和访问控制**\n- TLS和证书管理\n\n**可观察行**\n- Metrics\n- 监控\n- 分布式日志\n- 分布式tracing\n\n**部署**\n- 容器\n\n**通讯**\n- HTTP\n- WS\n- gRPC\n- TCP\n\n## 微服务的优劣\n\n**优势**\n- 将基础架构逻辑从业务代码中剥离出来\n\t- 分布式 tracing\n\t- 日志\n- 自由选择技术栈\n- 帮助业务开发部门只关注业务逻辑\n\n**劣势**\n- 复杂\n\t- 更多的运行实例\n- 可能带来额外的网络跳转\n\t- 每个服务调用都要经过 Sidecar\n- 解决了一部分问题，同时要付出代价\n\t- 依然要处理复杂路由，类型映射，与外部系统整合等方面问题\n- 不解决业务逻辑或服务整合，服务组合等问题\n\n## 服务网格可选方案\n\n![image.png](Assets/image_1667234149779_0.png)\n\n## 什么是服务网格\n\n**服务网格（Service Mesh）这个术语通常用于描述构成这些应用程序的微服务网络以及应用之间的交互。** 随着规模和复杂性的增长，服务网格越来越难以理解和管理。\n\n它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。\n\n![image.png](Assets/image_1667236455656_0.png)\n\n## 为什么要使用 Istio\n\n**HTTP、gRPC、WebSocket 和 TCP**流量的自动负载均衡。\n\n通过丰富的路由规则、重试、故障转移和故障注入，可以对流量行为进行细粒度控制。\n\n可插入的策略层和配置 API，支持访问控制、速率限制和配额。\n\n对出入集群入口和出口中所有流量的自动度量指标、日志记录和跟踪。\n\n通过强大的基于身份的验证和授权，在集群中实现安全的服务间通信。\n\n## Istio 功能概览\n\n![image.png](Assets/image_1667236582525_0.png)\n\n## 流量管理\n\n**连接**\n\n通过简单的规则配置和流量路由，可以控制服务之间的流量和 API调用。Istio 简化了断路器，超时和重试等服务级别属性的配置，并且可以轻松设置 A/B 测试、金丝雀部署和基于百分比的流量分割的分阶段部署等重要任务。\n\n**控制**\n\n通过更好地了解流量和开箱即用的故障恢复功能，可以在问题出现之前先发现问题，使调用更可靠，并且使您的网络更加强大——无论您面临什么条件。\n\n## 安全\n\n**开发人员可以专注于应用程序级别的安全性。** Istio 提供底层安全通信信道，并大规模管理服务通信的认证、授权和加密。使用Istio，服务通信在默认情况下是安全的，它允许跨多种协议和运行时一致地实施策略所有这些都很少或根本不需要应用程序更改。\n\n**虽然Istio 与平台无关，但将其与Kubernetes（或基础架构）网络策略结合使用，其优势会更大，包括在网络和应用层保护 Pod 间或服务间通信的能力。**\n\n## 可观察性\n\nIstio 生成以下类型的遥测数据，以提供对整个服务网格的可观察性∶\n\n指标：Istio 基于4个监控的黄金标识（延迟、流量、错误、饱和）生成了一系列服务指标。Istio 还为网格控制平面提供了更详细的指标。除此以外还提供了一组默认的基干这些指标的网格监控仪表板。\n\n分布式追踪：Istio 为每个服务生成分布式追踪 span，运维人员可以理解网格内服务的依赖和调用流程。\n\n访问日志：当流量流入网格中的服务时，Istio 可以生成每个请求的完整记录，包括源和目标的元数据。此信息使运维人员能够将服务行为的审查控制到单个工作负载实例的级别。\n\n**所有这些功能可以更有效地设置、监控和实施服务上的 SLO，快速有效地检测和修复问题。**\n\n## Istio 架构演进\n\n- 数据平面\n\t- 由一组以Sidecar方式部署的智能代理（Envoy）组成。这些代理可以调节和控制微服务及Mixer之所有的网络通信。\n- 控制平面\n\t- 负责管理和配置代理来路由流量。此外控制平面配置 Mixer 以实施策略和收集遥测数据。\n- 架构演进\n\t- 从微服务回归单体\n\n![image.png](Assets/image_1667237338403_0.png)\n\n## 设计目标\n\n**最大化透明度**\n- Istio 将自身自动注入到服务间所有的网络路径中，运维和开发人员只需付出很少的代价就可以从中受益。\n- Istio 使用 Sidecar 代理来捕获流量，并且在尽可能的地方自动编程网络层，以路由流量通过这些代理，而无需对已部署的应用程序代码进行任何改动。\n- 在Kubernetes中，代理被注入到 Pod中，通过编写iptables规则来捕获流量。注入 Sidecar 代理到Pod 中并且修改路由规则后，Istio 就能够调解所有流量。\n- 所有组件和 API在设计时都必须考虑性能和规模。\n\n**增量**\n- 预计最大的需求是扩展策略系统，集成其他策略和控制来源，并将网格行为信号传播到其他系统进行分析。策略运行时支持标准扩展机制以便插入到其他服务中。\n\n**可移植性**\n- 将基于 Istio 的服务移植到新环境应该是轻而易举的，而使用 Istio 将一个服务同时部署到多个环境中也是可行的（例如，在多个云上进行冗余部署）。\n\n**策略一致性**\n- 在服务间的 API调用中，策略的应用使得可以对网格间行为进行全面的控制，但对于无需在 API级别表达的资源来说，对资源应用策略也同样重要。\n- 因此，策略系统作为独特的服务来维护，具有自己的 API，而不是将其放到代理/sidecar 中，这容许服 务根据需要直接与其集成。\n\n# 深入理解数据平面Envoy\n\n## 主流七层代理的比较\n\n![image.png](Assets/image_1667237802067_0.png)\n\n## Envoy的优势\n\n**性能：**\n\n在具备大量特性的同时，Envoy 提供极高的吞吐量和低尾部延迟差异，而CPU 和 RAM消耗却相对较少。\n\n**可扩展性：**\n\nEnvoy 在 L4和L7都提供了丰富的可插拔过滤器能力，使用户可以轻松添加 开源版本中没有的功能。\n\n**API可配置性：**\n\nEnvoy提供了一组可以通过控制平面服务实现的管理 API。如果控制平面实现所有的 API，则可以使用通用引导配置在整个基础架构上运行Envoy。所有进一步的配置更改通过管理服务器以无缝方式动态传送，因此 EnvoV从不需要重新启动。这使得EnvoV成为通用数据平面当它与一个足够复杂的控制平面相结合时，会极大的降低整体运维的复杂性。\n\n## Envoy 线程模式\n\n- **Envoy 采用单进程多线程模式：**\n\t- 主线程负责协调；\n\t- 子线程负责监听过滤和转发。\n- 当某连接被监听器接受，那么该连接的全部生命周期会与某线程绑定。\n- Envoy基于非阻塞模式（Epoll）。\n- **建议 Envoy 配置的 worker数量与 Envoy所在的硬件线程数一致。**\n\n## Envoy 架构\n\n![image.png](Assets/image_1667239065136_0.png)\n\n## v1 API的缺点和v2的引入\n\n**v1 API仅使用 JSON/REST，本质上是轮询。** 这有几个缺点∶\n\n尽管Envoy在内部使用的是 JSON 模式，但 API本身并不是强类型，而且安全实现它们的通用服务器也很难。\n\n虽然轮询工作在实践中是很正常的用法，但更强大的控制平面更喜欢 streaming API，当其就绪后，可以将更新推送给每个EnvoV。这可以将更新传播时间从 30-60秒降低到250-500 毫秒，即使在极其庞大的部署中也是如此。\n\n**v2 API具有以下属性∶**\n\n新的 API模式使用 proto3指定，并同时以gRPC和 REST+JSON/YAML端点实现。\n\n它们被定义在一个名为 envoy-api的新的专用源代码仓库中。proto3的使用意味着这些 API是强类型的，同时仍然通过 proto3的JSON/YAML 表示来支持JSON/YAML变体。\n\n专用存储仓库的使用意味着项目可以更容易的使用 API并用 gRPC支持的所有语言生成存根（**实际上，对于希望使用它的用户，我们将继续支持基于REST 的JSON/YAML 变体）。**\n\n## xDS- Envoy 的发现机制\n\n**Endpoint Discovery Service (EDS):**\n\n这是 v1 SDS API的替代品。此外，gRPC的双向流性质将允许将负载/健康信息报告回管理服务器，为将来的全局负载均衡功能开启大门。\n\n**Cluster Discovery Service (CDS):**\n\n和V1 没有实质性变化。\n\n**Route Discovery Service (RDS):**\n\n和V1没有实质性变化。\n\n**Listener Discovery Service (LDS):**\n\n和v1 的唯一主要变化是∶现在允许监听器定义多个并发过滤栈，这些过滤栈可以基于一组监听器路由规则（例如，SNI，源/目的地IP 匹配等）来选择。这是处理\"原始目的地\"策略路由的更简洁的方式，这种路由是透明数据平面解决方案（如 Istio）所需要的。\n\n**Secret Discovery Service (SDS):**\n\n一个专用的 API来传递TLS密钥材料。这将解耦通过 LDS/CDS发送主要监听器、集群配置和通过专用密钥管理系统发送秘钥素材。\n\n**Health Discovery Service (HDS):**\n\n该 API将允许Envoy成为分布式健康检查网络的成员。中央健康检查服务可以使用一组Envoy作为健康检查终点并将状态报告回来，从而缓解 N健康检查问题，这个问题指的是其间的每个Envoy 都可能需要对每个其他 Envoy 进行健康检查。\n\n**Aggregated Discovery Service (ADS):**\n\n总的来说，Envoy的设计是最终一致的。这意味着默认情况下，每个管理API都并发运行，并且不会相互交互。在某些情况下，一次一个管理服务器处理单个Envoy的所有更新是有益的（例如，如果需要对更新进行排序以避免流量下降）。此 API允许通过单个管理服务器的单个gRPC 双向流对所有其他 API进行编组，从而实现确定性排序。\n\n## Envoy 的过滤器模式\n\n![image.png](Assets/image_1667239733328_0.png)\n\n## Isito流量管理\n\n## 流量管理\n\n- Gateway\n- VirtualService\n- DestinationRule\n- ServieEntry\n- WorkloadEntry\n- Sidecar\n\n## Istio 的流量劫持机制\n\n**为用户应用注入Sidecar**\n- 自动注入\n- 手动注入\n\t- istioctl kube-inject -f yaml/istio-bookinfo/bookinfo.yaml\n\n**注入后的结果**\n- 注入了 init-container istio-init\n\t- `istio-iptables -p 15001-z15006-u 1337-m REDIRECT -i * -x -b 9080-d 15090,15021,15020`\n- 注入了 sidecar container istio-proxy\n\n![image.png](Assets/image_1667293416806_0.png)\n\n## Init container\n\n**将应用容器的所有流量都转发到Envoy的15001端口。**\n\n使用 istio-proxy 用户身份运行，UID为1337，即Envoy所处的用户空间，这也是istio-proxy容器默认使用的用户，见YAML配置中的runAsUser字段。\n\n使用默认的REDIRECT模式来重定向流量。\n\n将所有出站流量都重定向到Envoy代理。\n\n将所有访问9080端口（即应用容器productpage的端口）的流量重定向到Envoy代理。\n\n```\n//所有入站TCP流量走ISTIO_INBOUND\n-A PREROUTING-P tcp-j ISTIO_INBOUND\n\n//所有出站TCP流量走ISTIO_OUTPUT\n-A OUTPUT-P tcp-j ISTIO  OUTPUT\n\n//忽略ssh，health check等端口\n-A ISTIO_INBOUND-p tcp-m tcp--dport 22-j RETURN\n-A ISTIO_INBOUND-p tcp-m tcp--dport 15090-j RETURN\n-A ISTIO_INBOUND-p tcp-m tcp--dport 15021-j RETURN\n-A ISTIO INBOUND-p tcp-m tcp--dport 15020-j RETURN\n//所有入站TCP流量走ISTIO_IN_REDIRECT\n-A ISTIO_INBOUND-P tcp-j ISTIO_IN_REDIRECT\n\n//TCP流量转发至15006端口\n-AISTIO_IN_REDIRECT-p tcp-j REDIRECT--to-ports 15006\n\n//loopback passthrough\n-A ISTIO OUTPUT-S 127.0.0.6/32-o lo-j RETURN\n//从loopback口出来，目标非本机地址，owner是envoy，交由ISTIO_IN_REDIRECT处理\n-A ISTIO OUTPUT!-d 127.0.0.1/32-o lo-m owner--uid-owner 1337-j ISTIO_IN_REDIRECT\n//从lo口出来，owner非envoy，return\n-A ISTIO_OUTPUT-o lo -m owner!--uid-owner 1337-j RETURN\n//owner是envoy，return\n-A ISTIO_OUTPUT-m owner--uid-owner 1337-j RETURN\n-A ISTIO OUTPUT!-d 127.0.0.1/32-o lo-m owner--gid-owner 1337-j ISTIO_IN_REDIRECT\n-A ISTIO_OUTPUT-o lo-m owner!--gid-owner 1337-j RETURN\n-A ISTIO OUTPUT-m owner--gid-owner 1337-j RETURN\n-A ISTIO OUTPUT-d 127.0.0.1/32-j RETURN\n//如以上规则都不匹配，则交给ISTIO REDIRECT处理\n-A ISTIO_OUTPUT-j ISTIO_REDIRECT\n\n-A ISTIO_REDIRECT-p tcp -j REDIRECT --to-ports 15001\n```\n\n## Sidecar containe\n\n**Istio会生成以下监听器：**\n- 0.0.0.0∶15001上的监听器接收进出Pod的所有流量，然后将请求移交给虚拟监听器。\n- 每个service IP一个虚拟监听器，每个出站TCP/HTTPS流量一个非HTTP监听器。\n- 每个Pod入站流量暴露的端口一个虚拟监听器。\n- 每个出站HTTP流量的HTTP 0.0.0.0端口一个虚拟监听器。\n\n`istioctl proxy-config listeners productpage-v1-8b96c8794-bttdd -n bookinfo--port 15001-ojson`\n\n```json\n{\n    \"name\": \"virtual\",\n    \"address\": {\n        \"socketAddress\": {\n            \"address\": \"0.0.0.0\",\n            \"portValue\": 15001\n        }\n    },\n    \"filterChains\": [\n        {\n            \"filters\": [\n                {\n                    \"name\": \"envoy.tcp_proxy\",\n                    \"config\": {\n                        \"cluster\": \"BlackHoleCluster\",\n                        \"stat_prefix\": \"BlackHoleCluster\"\n                    }\n                }\n            ]\n        }\n    ],\n    \"useOriginalDst\": true\n}\n```\n\n---\n\n**我们的请求是到9080 端口的 HTTP 出站请求，这意味着它被切换到 0.0.0.0∶9080虚拟监听器。**然后，此监听器在其配置的RDS中香找路由配置。在这种情 况下，它将查我由 Pilot 配置的 RDS中的路由9080（通过ADS）。\n\n```bash\nnetstat -nalgrep LISTEN\ntcp  0 0    127.0.0.1:15000 0.0.0.0:*   LISTEN\ntcp  0 0    0.0.0.0.15001   0.0.0.0:*   LISTEN\ntcp6 0 0    ::9080          :::*        LISTEN\n```\n\n```json\n\"rds\": {\n    \"config_source\": {\n        \"ads\": {}\n    },\n    \"route_config name\": \"9080\"\n}\n```\n\n---\n\n**9080路由配置仅为每个服务提供虚拟主机。** 我们的请求正在前往 reviews 服务，因此 Envoy 将选择我们的请求与域匹配的虚拟主机。一旦在域上匹配，Envoy会查找与请求匹配的第一条路径。在这种情况下，我们没有任何高级路由，因此只有一条路由匹配所有内容。\n\n这条路由告诉Envoy将请求发送到outbound\\[9080\\]lreviews.default.svc.cluster.local 集群。\n\n`istioctl proxy-config routes productpage-v1-8b96c8794-bttdd --name 9080 -o json -n bookinfo`\n\n```json\n{\n    \"name\": \"9080\",\n    \"virtualHosts\": [\n        {\n            \"name\": \"reviews.bookinfo.svc.cluster.local:9080\",\n            \"domains\": [\n                \"reviews.bookinfo.svc.cluster.local\",\n                \"reviews.bookinfo.svc.cluster.local:9080\",\n                \"reviews\",\n                \"reviews:9080\",\n                \"reviews.bookinfo.svc.cluster\",\n                \"reviews.bookinfo.svc.cluster:9080\",\n                \"reviews.bookinfo.svc\",\n                \"reviews.bookinfo.svc:9080\",\n                \"reviews.bookinfo\",\n                \"reviews.bookinfo:9080\",\n                \"192.168.143.232\",\n                \"192.168.143.232:9080\"\n            ],\n            \"routes\": [\n                {\n                    \"match\": {\n                        \"prefix\": \"/\"\n                    },\n                    \"route\": {\n                        \"cluster\": \"outbound|9080]lreviews.bookinfo.svc.cluster.local\",\n                        \"timeout\": \"0.000s\",\n                        \"maxGrpcTimeout\": \"0.000s\"\n                    }\n                }\n            ]\n        }\n    ]\n}\n```\n\n---\n\n**此集群配置为从 Pilot（通过 ADS）检索关联的端点。** 因此，Envoy 将使用 serviceName字段作\n\n为密钥来查找端点列表并将请求代理到其中一个端点。\n\n`istioctl proxy-config clusters productpage-v1-8b96c8794-bttdd --fqdn reviews.bookinfo.svc.cluster.local -n bookinfo-ojson`\n\n```json\n{\n    \"name\": \"outbound|9080l|reviews.bookinfo.svc.cluster.local\",\n    \"type\": \"EDS\",\n    \"edsclusterConfig\": {\n        \"edsConfig\": {\n            \"ads\": {}\n        },\n        \"serviceName\": \"outbound|9080|lreviews.bookinfo.svc.cluster.local\",\n        \"connectTimeout\": \"1.000s\",\n        \"circuitBreakers\": {\n            \"thresholds\": [\n                {}\n            ]\n        }\n    }\n}\n```\n\n## 流量管理\n\n![image.png](Assets/image_1667296531856_0.png)\n\n## 请求路由\n\n**特定网格中服务的规范表示由 Pilot 维护。** 服务的 Istio 模型和在底层平台（Kubernetes、Mesos以及 Cloud Foundrv 等）中的表达无关。特定平台的适配器负责从各自平台中获取元数据的各种字段，然后对服务模型进行填充。\n\n**Istio 引入了服务版本的概念，可以通过版本（v1、v2）或环境（staging、prod）对服务进行进一步的细分。** 这些版本不一定是不同的 API版本∶它们可能是部署在不同环境（prod、staging或者 dev 等）中的同一服务的不同迭代。使用这种方式的常见场景包括A/B测试或金丝雀部署。\n\n**Istio 的流量路由规则可以根据服务版本来对服务之间流量进行附加控制。**\n\n## 服务之间的通讯\n\n**服务的客户端不知道服务不同版本间的差异。** 它们可以使用服务的主机名或者 IP地址继续访问服务。Envoy sidecar/ 代理拦截并转发客户端和服务器之间的所有请求和响应。\n\nIstio 还为同一服务版本的多个实例提供流量负载均衡。可以在服务发现和负载均衡中找到更多信息。\n\nIstio 不提供 DNS。**应用程序可以尝试使用底层平台（kube-dns、mesos-dns 等）中存在的DNS服务来解析 FQDN。**\n\n## Ingress 和 Egress\n\n**lIstio 假定进入和离开服务网络的所有流量都会通过 Envoy 代理进行传输。**\n\n通过将 Envoy 代理部署在服务之前，运维人员可以针对面向用户的服务进行A/B 测试、部署金丝雀服务等。\n\n类似地，通过使用 Envoy 将流量路由到外部 Web 服务（例如，访问 Maps API或视频服务 API）的方式，运维人员可以为这些服务添加超时控制、重试、断路器等功能.同时还能从服务连接中获取各种细节指标。\n\n![image.png](Assets/image_1667296805340_0.png)\n\n## 服务发现和负载均衡\n\n**Istio 负载均衡服务网格中实例之间的通信。**\n\nIstio 假定存在服务注册表，以跟踪应用程序中服务的 **pod/VM**。它还假设服务的新实例自动注册到服务注册表，并且不健康的实例将被自动删除。诸如Kubernetes、Mesos 等平台已经为基于容器的应用程序提供了这样的功能。为基于虚拟机的应用程序提供的解决方案就更多了。\n\nPilot 使用来自服务注册的信息，并提供与平台无关的服务发现接口。网格中的Envoy 实例执行服务发现，并相应地动态更新其负载均衡池。\n\n网格中的服务使用其 DNS名称访问彼此。服务的所有 HTTP流量都会通过Envoy 自动重新路由。Envoy在负载均衡池中的实例之间分发流量。**虽然Envoy支持多种复杂的负载均衡算法，但Istio 目前仅允许三种负载均衡模式∶轮循、随机和带权重的最少请求。**\n\n---\n\n**除了负载均衡外，Envoy 还会定期检查池中每个实例的运行状况。** Envoy遵循熔断器风格模式.根据健康检查 API 调用的失败率将实例分类为不健康和健康两种。当给定实例的健康检香失败次数超过预定阈值时，将会被从负载均衡池中弹出。类似地，当通过的健康检查数超过预定阈值时，该实例将被添加回负载均衡池。您可以在处理故障中了解更多有关Envoy 的故障处理功能。\n\n**服务可以通过使用 HTTP 503响应健康检查来主动减轻负担。在这种情况下，服务实例将立即从调用者的负载均衡池中删除。**\n\n## 故障处理\n\n![image.png](Assets/image_1667297029724_0.png)\n\n## 微调\n\n**Istio 的流量管理规则允许运维人员为每个服务/版本设置故障恢复的全局默认值。** 然而，服务的消费者也可以通过特殊的HTTP头提供的请求级别值覆盖超时和重试的默认值。在 Envoy代理的实现中，对应的Header 分别是×-envoy-upstream-rq-timeout-ms 和x-envoy-max-retries。\n\n## 故障注入\n\n- **为什么需要错误注入：**\n\t- 微服务架构下，需要测试端到端的故障恢复能力。\n- Istio 允许在网络层面按协议注入错误来模拟错误，无需通过应用层面删除Pod，或者人为在 TCP层造成网络故障来模拟。\n- **注入的错误可以基于特定的条件，可以设置出现错误的比例∶**\n\t- Delay-提高网络延时；\n\t- Aborts-直接返回特定的错误码。\n\n## 规则配置\n\n- **VirtualService** 在 Istio 服务网格中定义路由规则，控制路由如何路由到服务上。\n- **DestinationRule** 是VirtualService 路由生效后，配置应用与请求的策略集。\n- **ServiceEntry** 是通常用于在 Istio 服务网格之外启用对服务的请求。\n- **Gateway**为HTTP/TCP流量配置负载均衡器，最常见的是在网格的边缘的操作，以启用应用程序的入口流量。\n\n## VirtualService\n\n**是在 Istio 服务网格内对服务的请求如何进行路由控制。**\n\n## 规则的目标描述\n\n**路由规则对应着一或多个用 VirtualService 配置指定的请求目的主机。** 这些主机可以是也可以不是实际的目标负载，甚至可以不是同一网格内可路由的服务。例如要给到reviews 服务的请求定义路由规则，可以使用内部的名称 reviews，也可以用域名 bookinfo.com，VirtualService 可以定义这样的host字段：\n\nhost 字段用显示或者隐式的方式定义了一或多个完全限定名（FQDN）。上面的 reviews，会隐式的扩展成为特定的 FQDN，例如在 Kubernetes 环境中，全名会从 VirtualService 所在的集群和命名空间中继承而来（比如说 reviews.default.svc.cluster.local）。\n\n```yaml\nhosts:\n- reviews\n- bookinfo.com\n```\n\n## 在服务之间分拆流量\n\n例如下面的规则会把25%的reviews 服务流量分配给v2标签；其余的75%流\u0000\u0000量分配给v1。\n\n```yaml\napiVersion: networking.istio.io/vlalpha3\nkind: VirtualService\nmetadata:\n  name: reviews\nspec:\n  hosts:\n  - reviews\n  http:\n  - route:\n    - destination:\n      host: reviews\n      subset: v1\n      weight: 75\n    - destination:\n      host: reviews\n      subset: v2\n      weight: 25\n```\n\n## 超时和重试\n\n```yaml\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: ratings\nspec:\n  hosts:\n  - ratings\n  http:\n  - route:\n    - destination:\n      host: ratings\n      subset: v1\n    timeout: 10s\n```\n\n```yaml\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: ratings\nspec:\n  hosts:\n  - ratings\n  http:\n  - route:\n    - destination:\n      host: ratings\n      subset: v1\n    retries:\n      attempts: 3\n      perTryTimeout: 2s\n```\n\n## 错误注入\n\n```yaml\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: ratings\nspec:\n  hosts:\n  - ratings\n  http:\n  - fault:\n      delay:\n        percent: 10\n        fixedDelay: 5s\n  route:\n    - destination:\n      host: ratings\n      subset: v1\n```\n\n```yaml\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: ratings\nspec:\n  hosts:\n  - ratings\n  http:\n  - fault:\n      abort:\n        percent: 10\n        httpStatus: 400\n  route:\n  - destination:\n    host: ratings\n    subset: v1\n```\n\n## 条件规则\n\n```yaml\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: productpage\nspec:\n  hosts:\n  - productpage\n  http:\n  - match:\n    - uri:\n      prefix: /api/v1\n  ...\n```\n\n```yaml\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name:reviews\nspec:\n  hosts:\n  - reviews\n  http:\n  - match:\n    - headers:\n      end-user:\n        exact: jason\n  ...\n```\n\n```yaml\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: ratings\nspec:\n  hosts:\n  - ratings\n  http:\n  - match:\n    sourceLabels:\n      app: reviews\n...\n```\n\n## 流量镜像\n\nmirror 规则可以使 Envoy 截取所有 request，并在转发请求的同时，将 request 转发至\n版本 ， 同时在Mirror Header的Host/Authority加上-shadow。\n\n**这些mirror请求会工作在fire and forget模式，所有的response都会被废弃。**\n\n```yaml\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: httpbin\nspec:\n  hosts:\n  - httpbin\n  http:\n  - route:\n    - destination:\n      host: httpbin\n      subset: v1\n    weight: 100\n  mirror:\n    host: httpbin\n    subset: v2\n```\n\n## 规则委托\n\n```yaml\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: bookinfo\nspec:\n  hosts:\n  - \"bookinfo.com\"\n  gateways:\n  - mygateway\n  http:\n    - match:\n      - uri:\n        prefix: \"/productpage\"\n      delegate:\n        name: productpage\n        namespace: nsA\n    - match:\n      - uri:\n        prefix: \"/reviews\"\n      delegate:\n        name: reviews\n        namespace: nsB\n```\n\n```yaml\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: productpage\n  namespace: nsA\nspec:\n  http:\n  - match:\n    - uri:\n      prefix: \"/productpage/v1/\"\n    route:\n    - destination:\n      host: productpage-v1.nsA.svc.cluster.local\n  - route:\n    - destination:\n      host: productpage.nsA.svc.cluster.local\n```\n\n## 优先级\n\n**当对同一目标有多个规则时，会按照在 VirtualService 中的顺序进行应用，换句话说，列表中的第一条规则具有最高优先级。**\n\n当对某个服务的路由是完全基于权重的时候，就可以在单一规则中完成。另一方面，如果有多重条件（例如来自特定用户的请求）用来进行路由，就会需要不止一条规则。这样就出现了优先级问题，需要通过优先级来保证根据正确的顺序来执行规则。\n\n常见的路由模式是提供一或多个高优先级规则，这些优先规则使用源服务以及 Header 来进行路由判断，然后才提供一条单独的基于权重的规则，这些低优先级规则不设置匹配规则，仅根据权重对所有剩余流量进行分流。\n\n## 目标规则\n\n**在请求被VirtualService路由之后，DestinationRule配置的一系列策略就生效了。**\n\n这些策略由服务属主编写，包含断路器、负载均衡以及TLS等的配置内容。\n\n```yaml\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: reviews\nspec:\n  host: reviews\n  trafficPolicy:\n    loadBalancer:\n      simple: RANDOM\n  subsets:\n  - name: v1\n    labels:\n      version: v1\n  - name: v2\n    labels:\n      version: v2\n    trafficPolicy:\n      loadBalancer:\n        simple: ROUND_ROBIN\n  - name: v3\n    labels:\n      version: v3\n```\n\n## 断路器\n\n**可以用一系列的标准，例如连接数和请求数限制来定义简单的断路器。**\n\n可以通过定义outlierDetection自定义健康检查模式。\n\n```yaml\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: httpbin\nspec:\n  host: httpbin\n  trafficPolicy:\n    connectionPool:\n      tcp:\n        maxConnections: 1\n      http:\n        http1MaxPendingRequests: 1\n        maxRequestsPerConnection: 1\n    outlierDetection:\n      consecutiveErrors: 1\n      interval: 1s\n      baseEjectionTime: 3m\n      maxEjectionPercent: 100\n```\n\n## ServiceEntry\n\n**Istio 内部会维护一个服务注册表，可以用ServiceEntry向其中加入额外的条目。** 通常这个对象用来启用对Istio服务网格之外的服务发出请求。\n\nServiceEntry中使用hosts字段来指定目标，字段值可以是一个完全限定名，也可以是个通配符域名。其中包含的白名单，包含一或多个允许网格中服务访问的服务。\n\n**只要ServiceEntry涉及到了匹配host的服务，就可以和VirtualService以及DestinationRule配合工作。**\n\n```yaml\napiVersion: networking.istio.io/v1alpha3\nkind: ServiceEntry\nmetadata:\n  name: foo-ext-svc\nspec:\n  hosts:\n  - *.foo.com\n  ports:\n  - number: 80\n    name: http\n    protocol: HTTP\n  - number: 443\n    name: https\n    protocol: HTTPS\n```\n\n## WorkloadEntry\n\n```yaml\napiVersion: networking.istio.io/v1 beta1\nkind: ServiceEntry\nmetadata:\n  name: details-svc\nspec:\n  hosts:\n  - details.bookinfo.com\n  location: MESH_INTERNAL\n  ports:\n  - number: 80\n    name: http\n    protocol: HTTP\n  resolution: STATIC\n  workloadSelector:\n    labels:\n     app: details-legacy\n```\n\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: WorkloadEntry\nmetadata:\n  name: details-svc\nspec:\n  serviceAccount: details-legacy\n  address: 2.2.2.2\n  labels:\n    app: details-legacy\n    instance-id: vm1\n```\n\n## Gateway\n\n**Gateway 为 HTTP/TCP流量配置了一个负载均衡，多数情况下在网格边缘进行操作，用于启用一个服务的入口（ingress）流量。**\n\n和Kubernetes Ingress不同，Istio Gateway只配置四层到六层的功能（例如开放端口或者TLS配置）。绑定一个VirtualService 到 Gateway上，用户就可以使用标准的 Istio 规则来控制进入的 HTTP和 TCP流量。\n\n```yaml\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: bookinfo-gateway\nspec:\n  servers:\n  - port:\n    number: 443\n    name: https\n    protocol: HTTPS\n  hosts:\n  - bookinfo.com\n  tls:\n    mode: SIMPLE\n    serverCertificate: /tmp/tls.crt\n    privateKey: /tmp/tls.key\n```\n\n```yaml\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: bookinfo\nspec:\n  hosts:\n  - bookinfo.com\n  gateways:\n  - bookinfo-gateway\n  http:\n  - match:\n    - uri:\n      prefix: /reviews\n    route:\n    ···\n```\n\n## 开启网关安全加固\n\n```yaml\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: bookinfo-gateway\nspec:\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    hosts:\n    - bookinfo.com\n    tls:\n      mode: SIMPLE\n      serverCertificate: /tmp/tls.crt\n      privateKey: /tmp/tls.key\n```\n\n```yaml\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: bookinfo-gateway\nspec:\n  servers:\n  - port:\n    number: 443\n    name: https\n    protocol: HTTPS\n  hosts:\n  - bookinfo.com\n  tls:\n    mode: SIMPLE\n    credentialName: foo\n```\n\n## 遥测（ Telemetry V2）\n\n- 基本Metrics\n- 针对HTTP，HTTP/2和GRPC协议，Istio收集以下指标：\n\t- Request Count(istio_requests_total)\n\t\t- 请总数\n\t- Request Duration (istio_request_duration_milliseconds)\n\t\t- 请求处理时长\n\t- Request Size (istio_request_bytes)\n\t\t- 请求包大小\n\t- Response Size(istio_response_bytes)\n\t\t- 响应包大小\n- 针对TCP协议，Istio收集以下指标：\n\t- Tcp Byte Sent(istio_tcp_sent_bytes_total)\n\t\t- 发送的数据响应包的总大小\n\t- Tcp Byte Received (istio_tcp_received_bytes_total)\n\t\t- 接收到的数据包大小\n\t- Tcp Connections Opened(istio_tcp_connections_opened_total)\n\t\t- TCP连接数\n\t- Tcp Connections Closed(istio_tcp_connections_closed_total)\n\t\t- 关闭的TCP连接数\n\t- 同时支持WASM（Web Assembly）plugin收集数据\n\t\t- 但启用WASM后资源开销显著上升\n\n## 多集群网格扩展\n\n基于 Gateway\n\n![image.png](Assets/image_1667311524904_0.png)\n\n基于VPN 或可互通的集群\n\n![image.png](Assets/image_1667311572370_0.png)\n\n# 跟踪采样\n\n## 跟踪采样配置\n\n**lstio 默认捕获所有请求的跟踪。**例如，何时每次访问时都使用上面的 Bookinfo 示例应用程序/productpage 你在Jaeger 看到了相应的痕迹仪表板。此采样率适用于测试或低流量目。\n\n在运行的网格中，编辑 istio-pilot 部署并使用以下步骤更改环境变量∶\n\n`istioctl upgrade --set values.global.tracer.zipkin.address=jaeger-collector:9417`\n\n## 应用程序埋点\n\n**虽然Istio代理能够自动发送 Span信息，但还是需要一些辅助手段来把整个跟踪过程统一起来。**应用程序应该自行传播跟踪相关的HTTP Header，这样在代理发送Span 信息的时候，才能正确的把同一个跟踪过程统一起来。\n\n为了完成跟踪的传播过程，应用应该从请求源头中收集下列的HTTP Header，并传播给外发请求∶\n- x-request-id\n- x-b3-traceid\n- x-b3-spanid\n- x-b3-parentspanid\n- x-b3-sampled\n- x-b3-flag5\n- x-ot-span-Context\n\n## Service Mesh 涉及的网络栈\n\n![image.png](Assets/image_1667312881173_0.png)\n\n## Cilium 数据平面加速\n\n![image.png](Assets/image_1667312900212_0.png)\n\n## 小结\n\n微服务架构是当前业界普遍认可的架构模式，容器的封装性，隔离性为微服务架构的兴盛提供了有力的保障。\n\nKubernetes作为声明式集群管理系统，代表了分布式系统架构的大方向∶\n- kube-proxy 本身提供了基于iptables/ipvs的四层 Service Mesh方案;\n- Istio/linkerd作为基于Kubernetes 的七层 Service Mesh方案，近期会有比较多的生产部署案例。\n\n生产系统需要考虑的，除了 Service Mesh 架构带来的便利性，还需要考虑：\n- 配置一致性检查；\n- endpoint 健康检查；\n- 海量转发规则情况下的scalability。\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/%E5%9F%BA%E4%BA%8EKubernetes%E5%92%8CIstio%E7%9A%84%E5%AE%89%E5%85%A8%E4%BF%9D%E8%AF%81":{"title":"基于Kubernetes和Istio的安全保证","content":"\n# 云原生语境下的安全保证\n\n## 云原生语境下的安全保证\n\n安全保证是贯穿软件整个生命周期的重要部分。\n\n安全与效率有时候是相违背的。\n\n如何将二者统一起来，提升整体效率是关键。\n\n这需要我们将安全思想贯穿到软件开发运维的所有环节。\n\n## 云原生层次模型\n\n软件的生命周期∶开发-\u003e分发→部署-\u003e运行\n\n![image.png](Assets/image_1667396770608_0.png)\n\n## 开发环节的安全保证\n\nSaaS 应用的 [[系统架构#12 factors|12-factor]] 设计原则的一些理念与云原生安全不谋而合。\n\n传统的**安全三元素 CIA（ConfidentialitVIntegrity和 AVailability）**，在云原生安全中被充分应用，如对工作负载的完整性保护，与I（Integrity）完整性保护相对应。\n\n- 机密性（Confidentiality）指只有授权用户可以获取信息。\n- 完整性（Integrity）指信息在输入和传输的过程中，不被非法授权修改和破坏，保证数据的一致性。\n- 可用性（Availability）指保证合法用户对信息和资源的使用不会被不正当地拒绝。\n\n**基础设施即代码（Infrastructure as Code，简称 laC）**也与云原生的实践紧密相关。\n\n这些方法和原则，都强调通过早期集成安全检测，以确保对过程的控制，使其按预期运行。\n\n通过早期检测的预防性成本，降低后续的修复成本，提升了安全的价值。\n\n## 开发\n\n![image.png](Assets/image_1667397465681_0.png)\n\n## 分发环节的安全保证\n\n云原生应用生命周期中的分发阶段不仅需要包括验证工作负载本身的完整性的方法，还需要包括创建工作负载的过程和操作手段。\n\n对于软件生产周期流水线中产生的工件（如容器镜像），，需要进行持续的自动扫描和更新来确保安全，防止漏洞、恶意软件、不安全的编码和其他不安全的行为。\n\n在完成这些检查后，更重要的是对产品进行加密签名，以确保产品的完整性及不可抵赖性。\n\n## 分发\n\n![image.png](Assets/image_1667397782368_0.png)\n\n## 部署环节的安全保证\n\n在整个开发和集成发布阶段，应对候选工作负载的安全性进行实时和持续的验证，如，对签名的工件进行校验，确保容器镜像安全和运行时安全，并可验证主机的适用性。\n\n安全工作负载的监控能力，应以可信的方式监控日志和可用指标，与工作负载一同部署来完善整体的安全性。\n\n# 部署\n\n![image.png](Assets/image_1667397846895_0.png)\n\n## 运行时环节的安全保\n\n应用程序通常由多个独立和单一职责的微服务组成，容器编排层使得这些微服务通过服务层抽象进行相互通信。\n\n确保这种相互关联的组件架构安全的最佳实践，包括以下几点∶\n\n1. 只有经过批准的进程能在容器命名空间内运行;\n\n2. 禁止并报告未经授权的资源访问;\n\n3. 监控网络流量以检测恶意的活动;\n\n4. 服务网格是另一种常见的服务层抽象，它为已经编排的服务提供了整合和补充功能，而不会改变工作负载软件本身（例如，API流量的日志记录、传输加密、可观察性标记、认证和授权）。\n\n## 运行环境\n\n![image.png](Assets/image_1667397948189_0.png)\n\n# 容器运行时的安全保证\n\n## 以 Non-root 身份运行容器\n\n**在 Dockerfile 中通过 USER命令切换成非 root用户。**\n\n原因分析：\n\n防止某些坏的镜像窃取主机的 root 权限并造成危害。\n\n有些运行时容器内部的root 用户与主机的 root 用户是同一个用户，如不进行用户切换很可能因为权限过大引发严重的问题，比如一个最简单的 case，主机上的重要文件夹被mount 到容器内部，并被容器修改配置。\n\n即使在容器内部也应该进行权限隔离，比如当我们希望构建不可变配置的容器镜像时，应该将运行容器的用户切换为非 root 用户，并且限制其读写权限和读写目录。\n\n```Dockerfile\nFROM ubuntu\nRUN user add patrick\nUSER patrick\n```\n\n## User Namespace 和 rootless container\n\n**User Namespace:**\n- 依赖于 User namespace，任何容器内部的用户都会映射成主机的非 root 用户。\n- 但该功能未被默认enable，因其引入配置复杂性，比如系统不知道主机用户和容器用户的映射关系， 在 mount 文件的时候无法设置适当的权限。\n\n**Rootless container:**\n- rootless Container 是指容器运行时以非 root 身份启动。\n- 在该配置下，即使容器被突破，在主机层面获得的用户权限也是非 root 身份的用户，这确保了安全。\n- Docker 和其他运行时本身的后台 Daemon（如Docker Daemon）需要 root 身份运行，然后其他用户的容器才能以rootless 身份运行。\n- 一些运行时，比如 Podman，无需 Daemon 进程，因为可以完全不需要 root 身份。\n\n## 集群的安全性保证\n\n保证容器与容器之间、容器与主机之间隔离，限制容器对其他容器和主机的消极影响。\n\n保证组件、用户及容器应用程序都是最小权限，限制它们的权限范围。\n\n保证集群的敏感数据的传输和存储安全。\n\n常用手段\n- Pod安全上下文（Pod Security Context）\n- API Server的认证、授权、审计和准入控制\n- 数据的加密机制等\n\n# Kubernetes 的安全保证\n\n## 集群的安全通信\n\nKubernetes 期望集群中所有的API通信在默认情况下都使用TLS加密，大多数安装方法也允许创建所需的证书并且分发到集群组件中。\n\n![image.png](Assets/image_1667398917035_0.png)\n\n## 控制面安全保证\n\n**认证**\n\n小型的单用户集群可能希望使用简单的证书或静态承载令牌方法。\n\n更大的集群则可能希望整合现有的、OIDC、LDAP等允许用户分组的服务器。\n\n所有 API客户端都必须经过身份验证，即使它是基础设施的一部分，比如节点、代理、调度程序和卷插件。这些客户端通常使用**服务帐户或 X509 客户端证书**，并在集群启动时自动创建或是作为集群安装的一部分进行设置。\n\n**授权**\n\n与身份验证一样，简单而广泛的角色可能适合于较小的集群，但是随着更多的用户与集群交互，可能需要将团队划分成有更多角色限制的单独的命名空间。\n\n**配额**\n\n资源配额限制了授予命名空间的资源的数量或容量。这通常用于限制命名空间可以分配的CPU、内存或持久磁盘的数量，但也可以控制每个命名空间中有多少个Pod、服务或卷的存在。\n\n## NodeRestriction\n\n准入控制器限制了kubelet 可以修改的 Node 和 Pod 对象，kubelet只可修改自己的 Node API对象，只能修改绑定到节点本身的 Pod 对象。\n\nNodeRestriction准入插件可防止kubelet 删除 Node API对象。\n\n防止 kubelet 添加/删除/更新带有 node-restriction.kubernetes.io/前缀的标签。\n\n将来的版本可能会增加其他限制，以确保 kubelet 具有正常运行所需的最小权限集。\n\n思考为什么?\n\n降低获得kubelet kubeconfig 的人能做成的破坏。\n\n## Security Context\n\nPod 定义包含了一个安全上下文，用于描述允许它请求访问某个节点上的特定 Linux用户（如root）、获得特权或访问主机网络，以及允许它在主机节点上不受约束地运行的其它控件。\n\n**Pod 安全策略**可以限制哪些用户或服务帐户可以提供危险的安全上下文设置。例如∶Pod 的安全策略可以限制卷挂载，尤其是 hostpath，这些都是 Pod 应该控制的一些方面。\n\n一般来说，大多数应用程序需要限制对主机资源的访问，他们可以在不能访问主机信息的情况下成功以根进程（UID 0）运行。但是，考虑到与root 用户相关的特权，在编写应用程序容器时，你应该使用非 root 用户运行。\n\n类似地，希望阻止客户端应用程序逃避其容器的管理员，应该使用限制性的Pod 安全策略。\n\n**Kubernetes提供了三种配置 Security Context 的方法∶**\n- Container-level Security Context∶仅应用到指定的容器。\n- Pod-level Security Context∶应用到 Pod 内所有容器以及Volume。\n- Pod Security Policies（PSP）∶应用到集群内部所有 Pod 以及Volume。\n\n## Container-level Security Context\n\n**Container-level Security Context**仅应用到指定的容器上，并且不会影响Volume。比如设置容器运行在特权模式：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: hello-world\nspec:\n  containers:\n  - name: hello-world-container # The container definition #…\n    securityContext:\n      privileged: true\n```\n\n## Pod-level Security Context\n\nPod-level Security Context 应用到Pod 内所有容器，并且还会影响Volume（包括fsGroup和selinuxOptions）。\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: hello-world\nspec:\n  containers:\n  # specification of the pod's containers\n  #…\n  securityContext:\n    fsGroup: 1234\n    supplementalGroups: [5678]\n    seLinuxOptions:\n      level: \"s0:c123,c456\"\n```\n\n## Pod Security Policies(PSP)\n\nPod Security Policies（PSP）是集群级的Pod安全策略，自动为集群内的Pod和Volume设置Security Context。\n\n![image.png](Assets/image_1667405053981_0.png)\n\n## PSP示例\n\n**限制容器的 host端口范围为 8000-8080：**\n\n```yaml\napiVersion: extensions/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: permissive\nspec:\n  selinux:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  runAsUser:\n    rule: RunAsAny\n  fsGroup:\n    rule: RunAsAny\n  hostPorts:\n  - min: 8000\n    max: 8080\n  volumes:\n  - '*'\n```\n\n# Taint\n\n## 为节点增加 taint\n\n**使用命令kubectl taint给节点增加一个Taint：**\n\n`kubectl taint nodes node1 key=value:NoSchedule`\n\n**运行如下命令删除Taint：**\n\n`kubectl taint nodes node1 key:NoSchedule-`\n\n**在 PodSpec中为容器设定容忍标签：**\n\n```yaml\ntolerations:\n- key: \"key\"\n  operator: \"Equal\"\n  value: \"value\"\n  effect: \"NoSchedule\"\n```\n\n```yaml\ntolerations:\n- key: \"key\"\n  operator: \"Exists'\n  effect: \"NoSchedule\"\n\n```\n\n## Taint\n\n可以以租户为粒度，为不同租户的节点增加 Taint，使得节点彼此隔离。\n\nTaint的作用是让租户独占节点，无对应Toleration的 Pod无法被调度到Taint节点上，实现了应用部署的隔离。\n\n# NetworkPolicy\n\n## NetworkPolicy\n\n如果你希望在 IP地址或端口层面（OSI第 3层或第 4 层）控制网络流量，则你可以考虑为集群中特定应用使用Kubernetes 网络策略（NetworkPolicy）。\n\nPod 可以通信的 Pod是通过如下三个标识符的组合来辩识的：\n\n- 其他被允许的 Pods；\n- 被允许的名字空间；\n- IP组块。\n\n网络策略通过网络插件来实现。要使用网络策略，你必须使用支持 NetworkPolicV 的网络解决方案。**创建一个NetworkPolicy资源对象而没有控制器来使它生效的话，是没有任何作用的。**\n\n## 隔离和非隔离的Pod\n\n默认情况下，Pod是非隔离的，它们接受任何来源的流量。\n\nPod在被某 NetworkPolicy选中时进入被隔离状态。\n\n一旦名字空间中有NetworkPolicy 选择了特定的 Pod，该 Pod 会拒绝该NetworkPolicy所不允许的连接。\n\n网络策略不会冲突，它们是累积的。如果任何一个或多个策略选择了一个Pod，则该 Pod 受限于这些策略的**入站（Ingress）/出站（Egress）规则**的并集。因此评估的顺序并不会影响策略的结果。\n\n为了允许两个Pods 之间的网络数据流，源端 Pod 上的出站（Earess）规则和目标端 Pod 上的入站（Ingress）规则都需要允许该流量。如果源端的出站（Egress）规则或目标端的入站（Ingress）规则拒绝该流量，则流量将被拒绝。\n\n## 安全策略属性\n\n**spec：** NetworkPolicy规约中包含了在一个名字空间中定义特定网络策略所需的所有信息。\n\n**podSelector：**\n\n- 每个 NetworkPolicy都包括一个 podSelector，它对该策略所适用的一组 Pod进行选择。\n- 空的 podSelector 选择名字空间下的所有 Pod。\n\n**policyTypes：**\n\n- 每个NetworkPolicy都包含一个policyTypes列表，其中包含Ingress或Egress 或两者兼具。\n- 如果 NetworkPolicy未指定 policyTypes 则默认情况下始终设置Ingress；\n- 如果 NetworkPolicy 有任何出口规则的话则设置 Egress。\n\n## 安全策略属性\n\nIngress：\n\n- 每个NetworkPolicy可包含一个Ingress规则的白名单列表。\n- 每个规则都允许同时匹配 from 和 ports 部分的流量。\n\nEgress：\n\n- 每个NetworkPolicy可包含一个Egress 规则的白名单列表。\n- 每个规则都允许匹配 to 和 port 部分的流量。\n\n## NetworkPolicy\n\n![image.png](Assets/image_1667463041667_0.png)\n\n## 默认策略\n\n![image.png](Assets/image_1667463441696_0.png)\n\n![image.png](Assets/image_1667463453193_0.png)\n\n# 依托于Calico 的NetworkPolicy\n\n## NetworkPolicy\n\nNetworkPolicy是命名空间级别资源。规则应用于与标签选择器匹配的endpoint 的集合。\n\n![image.png](Assets/image_1667464499256_0.png)\n\n## GlobalNetworkPolicy\n\nGlobalNetworkPolicy与 NetworkPolicy 功能一样，是整个集群级别的资源。\n\nGlobalNetworkPolicy 会在集群中所有 Namespace生效，并且能限制主机（HostEndpoint）。\n\n## 理解Calico 的防火墙规则\n\n![image.png](Assets/image_1667465894838_0.png)\n\n## 创建 NetworkPolicy并查看结果\n\n![image.png](Assets/image_1667465920840_0.png)\n\n## 创建规则允许 ICMP并查看规则变化\n\n![image.png](Assets/image_1667465957616_0.png)\n\n# 零信任架构（ZTA）\n\n## 传统安全模型\n\n**DMZ模式**\n\n传统的网络安全架构理念是基于边界的安全架构，企业构建网络安全体系时，首先寻找安全边界，把网络划分为外网、内网、DMZ（DeMilitarized Zone）区等不同的区域然后在边界上部署防火墙、入侵检测、WAF等产品。\n\n这种网络安全架构假设或默认了内网比外网更安全，在某种程度上预设了对内网中的人、设备和系统的信任，忽视加强内网安全措施。\n\n不法分子一旦突破企业的边界安全防护进入内网，会像进入无人之境，将带来严重的后果。\n\n传统的认证，即信任、边界防护、静态访问控制、以网络为中心。\n\n![image.png](Assets/image_1667468838210_0.png)\n\n## 零信任架构（Zero Trust Architecture， ZTA）\n\n随着云计算、大数据、物联网、移动办公等新技术与业务的深度融合，网络安全边界也逐渐变得更加模糊，传统边界安全防护理念面临巨大挑战。\n\n**零信任核心原则：从不信任，始终验证**\n\n动态安全架构：\n\n- 以身份为中心\n- 以识别、持续认证、动态访问控制、授权、审计以及监测为链条\n- 以最小化实时授权为核心\n- 以多维信任算法为基础\n- 认证达末端\n\n![image.png](Assets/image_1667468921971_0.png)\n\n---\n\n与边界模型的\"信任但验证\"不同，零信任的核心原则是\"从不信任、始终验证\"。\n\n根据Evan Gilman《Zero Trust Networks》书中所述，零信任网络建立在五个假设前提之下∶\n\n- 外部和内部威胁每时每刻都充斥着网络;\n- 不能仅仅依靠网络位置来确认信任关系;\n- 所有设备、用户、网络流量都应该被认证和授权;\n- 访问控制策略应该动态地基于尽量多的数据源进行计算和评估。\n\n## ZTA 安全模型\n\n- 零信任数据\n- 零信任人员\n- 零信任网络\n- 零信任工作负载\n- 零信任设备\n- 可视化和分析\n- 自动化和编排\n\n![image.png](Assets/image_1667469502375_0.png)\n\n## 零信任架构的三大技术\"SIM\"\n\n零信任架构的三大技术\"SIM\"，即\n\n- 软件定义边界（SDP，Software Defined Perimeter）\n- 身份识别与访问管理（IAM， Identity and Access Management）\n- 微隔离（MSG，Micro Segmentation）\n\n## 软件定义边界（SDP）\n\nSDP旨在使应用程序所有者能在需要时部署安全边界，以便将服务与不安全的网络隔离开。\n\nSDP将物理设备替换为在应用程序所有者控制下运行的逻辑组件，仅在设备验证和身份验证后才允许访问企业应用基础架构。\n\n基于 SDP的系统通常会实施控制层与数据层的分离：\n\n- 控制流阶段，用户及其设备进行预认证来获取丰富的属性凭据作为身份主体，以此结合基于属性的预授权策略，映射得到仅供目标访问的特定设备和服务;\n- 数据传输阶段直接建立相应安全连接并传输数据。\n\n## 身份识别与访问管理（IAM）\n\n零信任强调基于身份的信任链条，即该身份在可信终端，该身份拥有权限才可对资源进行请求。\n\n传统的 IAM系统可以协助解决身份唯一标识、身份属性、身份全生命周期管理的功能问题。\n\n通过IAM 将身份信息（身份吊销离职、身份过期、身份异常等）传递给零信任系统后.零信任系统可以通过IAM系统的身份信息来分配相应权限。\n\n通过 IAM系统对身份的唯一标识，可有利于零信任系统确认用户可信，通过唯一标识对用户身份建立起终端、资源的信任关系，并在发现风险时实施针对关键用户相关的访问连接进行阻断等控制。\n\n## 微隔离（MSG）\n\n**微隔离本质上是一种网络安全隔离技术**\n\n- 能够在逻辑上将数据中心划分为不同的安全段，一直到各个工作负载级别;\n- 为每个独立的安全段定义访问控制策略。\n\n**它主要聚焦在云平台东西向流量的隔离**\n\n- 一是区别传统物理防火墙的隔离作用;\n- 二是更加贴近云计算环境中的真实需求。\n\n微隔离将网络边界安全理念发挥到极致，将网络边界分割到尽可能的小.能够很好的缓解传统\n\n边界安全理念下的边界过度信任带来的安全风险。\n\n# 基于Istio的安全保证\n\n## 微服务架构下的安全挑战\n\n为了抵御中间人攻击，需要流量加密。\n\n为了提供灵活的服务访问控制，需要双向TLS和细粒度的访问策略。\n\n要确定谁在什么时候做了什么，需要审计工具。\n\n## Istio的安全保证\n\n**Istio安全功能提供：**\n\n- 身份识别；\n- 灵活策略；\n- 透明的TLS加密；\n- 认证，授权和审计（AAA）工具来保护你的服务和数据。\n\n**Istio安全的目标是：**\n\n- 默认安全：应用程序代码和基础设施无需更改。\n- 深度防御：与现有安全系统集成以提供多层防御。\n- 零信任网络：在不受信任的网络上构建安全解决方案。\n\n![image.png](Assets/image_1667470319529_0.png)\n\n## 高层架构\n\n用于密钥和证书管理的证书颁发机构（CA）。\n\n配置 API服务器分发给代理∶\n\n- 认证策略\n- 授权策略\n\n安全命名信息\n\nSidecar和边缘代理作为 Policy Enforcement Points（PEPs）以保护客户端和服务器之间的通信安全。\n\n一组 Envoy代理扩展，用于管理遥测和审计。\n\n## Istio安全架构\n\n![image.png](Assets/image_1667470608630_0.png)\n\n## Istio身份\n\n身份是任何安全基础架构的基本概念。\n\n在工作负载间通信开始时，双方必须交换包含身份信息的凭证以进行双向验证。\n\n在客户端，根据安全命名信息检查服务器的标识，以查看它是否是该服务的授权运行程序。\n\n在服务器端，服务器可以根据授权策略确定客户端可以访问哪些信息，审计谁在什么时间访问了什么，根据他们使用的工作负载向客户收费，并拒绝任何未能支付账单的客户访问工作负载。\n\nIstio 身份模型使用 service identity（服务身份）来确定一个请求源端的身份。\n\n---\n\n**Kubernetes：** Kubernetes 服务帐户\n\n**GKE/GCE：** 可以使用GCP服务帐户\n\n**GCP：** GCP服务帐户\n\n**AWS：** AWSIAM用户/角色帐户\n\n**On-premises（非Kubernetes）：** 用户帐户、自定义服务帐户、服务名称、Istio 服务帐户或 GCP服务帐户。\n\n**Istio安全与 SPIFFE：**\n\nIstio 和 SPIFFE共享相同的身份文件∶SVID（SPIFFE 可验证身份证件）。例如∶ 在 Kubernetes 中，X.509证书的 URI学段格式为 `spiffe∶//\u003cdomain\u003e/ns/\u003cnamespace\u003e/sa/\u003cserviceaccount\u003e`。这使 Istio 服务能够建立和接受与其他 SPIFFE兼容系统的连接。\n\n## SDS\n\n**Istio供应身份是通过secret discovery service（SDS）来实现的：**\n\n- istiod提供 gRPC服务以接受证书签名请求（CSRS）。\n- 当工作负载启动时，Envoy通过秘密发现服务（SDS）API向同容器内的istio-agent发送证书和密钥请求。\n- 在收到SDS请求后，istio-agent创建私钥和CSR，然后将CSR及其凭据发送到 istiod CA进行签名。\n- istiod CA 验证CSR中携带的凭据，成功验证后签署CSR以生成证书。\n- istio-agent 通过Envoy SDS API将私钥和从Istio CA 收到的证书发送给Envoy。\n- istio-agent 会监工作负载证书的有效期。上述CSR 过程会周期性地重复，以处理证书和密钥轮换。\n\n![image.png](Assets/image_1667470838581_0.png)\n\n# 认证\n\n## 基于Istio的认证\n\n- Istio 通过客户端和服务器端 Policy Enforcement Points（PEPs）建立服务到服务的通信通道。\n- PEPS在 Istio 架构中的实现是 Envoy。\n- **Peer authentication：**\n\t- 用于服务到服务的认证，以验证进行连接的客户端。\n- **Istio 提供双向 TLS作为传输认证的全栈解决方案，无需更改服务代码就可以启用它**。这个解决方案为：\n\t- 为每个服务提供强大的身份，表示其角色，以实现跨群集和云的互操作性。\n\t- 保护服务到服务的通信。\n\t- 提供密钥管理系统，以自动进行密钥和证书的生成，分发和轮换。\n- **Request authentication**\n\t- 用于最终用户认证，以验证附加到请求的凭据。\n\t- Istio 使用JSON Web Token（JWT）验证启用请求级认证，并使用自定义认证实现或任何OpenID Connect 的认证实现（例如下面列举的）来简化的开发人员体验。\n\n## Istio认证架构\n\n- 未设置模式的网格范围的peer认证策略默认使用PERMISSIVE模式。\n- 发送请求的客户端服务负责遵循必要的认证机制。\n- **RequestAuthentication**\n\t- 应用程序负责获取JWT凭证并将其附加到请求。\n- **PeerAuthentication**\n\t- Istio会自动将两个PEPS之间的所有流量升级为双向TLS。\n\t- 如果认证策略禁用了双向TLS模式，则Istio将继续在PEPS之间使用纯文本。\n\t- 要覆盖此行为，destination rules 显式禁用双向TLS模式。\n\n![image.png](Assets/image_1667471444924_0.png)\n\n## 双向TLS认证\n\n**当一个工作负载使用双向TLS认证向另一个工作负载发送请求时，该请求的处理方式如下：**\n- Istio 将出站流量从客户端重新路由到客户端的本地 sidecar Envoy。\n- 客户端 Envoy与服务器端 Envoy 开始双向TLS握手。在握手期间，客户端 Envoy还做了安全命名检查，以验证服务器证书中显示的服务帐户是否被授权运行目标服务。\n- 客户端 Envoy和服务器端 Envoy建立了一个双向的TLS连接，Istio 将流量从客户端 Envoy转发到服务器端 Envoy。\n- 授权后，服务器端 Envoy 通过本地 TCP连接将流量转发到服务器服务。\n\n## 宽容模式（permissive mode）\n\n允许服务同时接受纯文本流量和双向 TLS流量。\n\n这个功能极大地提升了双向TLS的入门体验。\n\n在运维人员希望将服务移植到启用了双向TLS 的Istio上时，许多非Istio客户端和非 Istio 服务端通信时会产生问题。\n\n通常情况下，运维人员无法同时为所有客户端安装Istio sidecar，甚至没有这样做的权限。即使在服务端上安装了Istio sidecar，运维人员也无法在不中断现有连接的情况下启用双向TLS。\n\n## 安全命名\n\n服务器身份（server identities）被编码在证书里，但服务名称（service names）通过服务发现或 DNS被检索。\n\n安全命名信息将服务器身份映射到服务名称。\n\n身份A到服务名称B的映射表示\"授权 A运行服务 B\"。\n\n控制平面监视 apiserver，生成安全命名映射，并将其安全地分发到PEPs。以下示例说明了为什么安全命名对身份验证至关重要。\n\n## 认证策略\n\n认证策略是对服务收到的请求生效的；\n\n要在双向TLS中指定**客户端**认证策略，需要在 DetinationRule 中设置 TLSSettings。\n\n![image.png](Assets/image_1667472040181_0.png)\n\n相应的需要通过PeerAuthentication配置服务端接受何种认证方式\n\n![image.png](Assets/image_1667472078286_0.png)\n\n## RequestAuthentication\n\n**Request 认证策略指定验证 JSON Web Token（JWT）所需的值。** 这些值包括：\n\n- token 在请求中的位置\n- 请求的 issuer\n- 共 JSON Web Key Set（JWKS）\n\nIstio 会根据request 认证策略中的规则检查提供的令牌（如果已提供），并拒绝令牌无效的请求。\n\n当请求不带有令牌时，默认情况下将接受它们。\n\n```yaml\napiVersion: security.istio.io/v1beta1\nkind: RequestAuthentication\nmetadata:\n  name: \"default\"\n  namespace: istio-system\nspec:\n  selector:\n    matchLabels:\n      istio: ingressgateway\n  jwtRules:\n  - issuer: \"testing@secure.istio.io\"\n    jwksUri: \"https://raw.githubusercontent.com/istio/istio/release-1.12/security/tools/jwt/samples/jwks.json\"\n```\n\n# 鉴权\n\n## 授权架构\n\n**每个Envoy代理都运行一个授权引擎，该引擎在运行时授权请求。**\n\n- 当请求到达代理时，授权引擎根据当前授权策略评估请求上下文，并返回授权结果ALLOW或DENY。\n- 授权策略支持ALLOW和DENY动作，拒绝策略优先于允许策略。\n- 如果将任何允许策略应用于工作负载，则默认情况下，不符合该策略的访问都将被禁止。\n\n![image.png](Assets/image_1667474428578_0.png)\n\n## 授权策略（AuthorizationPolicy）\n\n- selector字段指定策略的目标\n- action字段指定允许还是拒绝请求\n- rules 指定何时触发动作\n\t- rules下的from字段指定请求的来源\n\t- rules下的to字段指定请求的操作\n\t- rules下的when字段指定应用规则所需的条件\n\n![image.png](Assets/image_1667474516979_0.png)\n\n![image.png](Assets/image_1667474532819_0.png){:height 465, :width 458}\n\n## 策略目标\n\n可以通过 metadata/namespace 字段和可选的selector字段来指定策略的范围或目标。\n\nmetadata/namespace告诉该策略适用于哪个命名空间。如果将其值设置为根名称空间，则该策略将应用于网格中的所有名称空间。\n\n根命名空间的值是可配置的，默认值为 istio-system。\n\n您可以使用selector字段来进一步限制策略以应用于特定工作负载。\n\n如果未设置，则授权策略将应用于与授权策略相同的命名空间中的所有工作负载。\n\n![image.png](Assets/image_1667474660706_0.png)\n\n## 值匹配\n\n授权策略中的大多数字段都支持以下所有匹配模式：\n\n- 完全匹配即完整的字符串匹配。\n- 前缀匹配：\"\\*\"结尾的字符串。例如，\"test.abc.\\*\"匹配\"test.abc.com\"、\"test.abc.com.cn\"、\"test.abc.org\"等等。后缀匹配\"*\"开头的字符串。例如，\"*.abc.com\"匹配\"eng.abc.com\"、\"test.eng.abc.com\"等等。\n- 存在匹配：\\*用于指定非空的任意内容。您可以使用格式fieldname∶[\"\\*\"]指定必须存在的字段。这意味着该字段可以匹配任意内容，但是不能为空。\n\n![image.png](Assets/image_1667474807722_0.png)\n\n## 全部允许和默认全部拒绝授权策略\n\n![image.png](Assets/image_1667474840372_0.png)\n\n## 自定义条件\n\n![image.png](Assets/image_1667474870864_0.png)\n\n## 认证与未认证身份\n\n![image.png](Assets/image_1667474896234_0.png)\n\n## 在普通 TCP协议上使用 Istio 授权\n\n如果您授权策略中对TCP工作负载使用了任何只适用于HTTP 的字段，Istio 将会忽略它们。\n\n![image.png](Assets/image_1667474928562_0.png)\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/%E5%B0%86%E5%BA%94%E7%94%A8%E8%BF%81%E7%A7%BB%E8%87%B3Kubernetes%E5%B9%B3%E5%8F%B0":{"title":"将应用迁移至Kubernetes平台","content":"\n# 应用接入最佳实践\n\n## 应用容器化\n\n### 目标\n\n**稳定性、可用性、性能、安全**\n\n从多维度思考高可用的问题\n\n![image.png](Assets/image_1667144371254_0.png)\n\n### 应用容器化的思考\n\n![image.png](Assets/image_1667144500777_0.png)\n\n### 容器额外开销和风险\n\n- **Log driver**\n\t- Blocking mode\n\t- Non blocking mode\n- **共用 kernel 所以**\n\t- 系统参数配置共享\n\t- 进程数共享-Fork bomb\n\t- fd 数共享\n\t- 主机磁盘共享\n\n### 容器化应用的资源监控\n\n**容器中看到的资源是主机资源**\n\n- Top\n- Java runtime.GetAvailableProcesses()\n- cat /proc/cpuinfo\n- cat /proC/meminfo\n- df -k\n\n**解决方案**\n\n- 查询/proc/1/cgroup 是否包含kubepods关键字（docker关键字不可靠）。\n\t- 11:cpu,cpuacct:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort\u0002-pod521722c3_85a8_11e9_87fc_3cfdfe57c998.slice/9568cc0d8ae182395e1ce172e2cac723c4781a999e89e0f9f10d33af079a56e9\n- 包含此关键字，则表明是运行在Kubernetes 之上。\n\n### 内存开销\n\n**配额**\n\n- cat /sys/fs/cgroup/memory/memory.limit_in_bytes\n\t- 36854771712\n\n**用量**\n\n- cat /sys/fs/cgroup/memory/memory.usage_in_bytes\n\t- 448450560\n\n### CPU\n\n**配额，分配的 CPU个数=quota/ period，quota=-1代表 besteffort**\n\n- cat /sys/fs/cgroup/cpu/cpu.cfs_quota_us\n\t- -1\n- cat /sys/fs/cgroup/cpu/cpu.cfs_period_us\n\t- 100000\n\n**用量**\n\n- cat /sys/fs/cgroup/cpuacct/cpuacct.usage_percpu（按CPU区分）\n\t- 140669504971 148500278385149957919463 152786448674\n- cat /sys/fs/cgroup/cpuacct/cpuacct.usage\n\t- 12081100465458\n\n### 其他方案\n\n- xcfs\n\t- 通过 so 挂载的方式，使容器获得正确的资源信息\n- Kata\n\t- VM中跑container\n- Virtlet\n\t- 直接启动 VM\n\n## 将应用迁移至Kubernetes\n\n### Pod spec\n\n- 初始化需求（init container）\n- **需要几个主 container**\n- 权限? Privilege 和 SecurityContext（PSP）\n- **共享哪些Namespace（PID，IPC，NET，UTS，MNT）**\n- 配置管理\n- 优雅终止\n- 健康检查\n\t- Liveness Probe\n\t- Readiness Probe\n- DNS策略以及对 resolv.conf 的影响\n- imagePullPolicy Image 拉取策略\n\n### Probe 误用会造成严重后果\n\n![image.png](Assets/image_1667146432259_0.png)\n\n### 如何防止 PID泄露\n\n- 单进程容器\n- **合理的处理多进程容器**\n\t- 容器的初始化进程必须负责清理 fork 出来的所有子进程\n\t- 开源方案\n\t\t- Tini https://github.com/krallin/tini\n\t\t- 采用Tini作为容器的初始化进程（PID=1），容器中僵尸进程的父进程会被置为1\n- **如果不采用特殊初始化进程**\n\t- 建议采用HTTPCheck作为Probe\n\t- 为exec Probe 设置合理的超时时间\n\n![image.png](Assets/image_1667146638863_0.png)\n\n### 在 Kubernetes 上部署应用的挑战\n\n**资源规划**\n\n- 每个实例需要多少计算资源\n\t- CPU/GPU?\n\t- Memory\n- 超售需求\n- 每个实例需要多少存储资源\n\t- 大小\n\t- 本地还是网盘\n\t- 读写性能\n\t- DiskIO\n- 网络需求\n\t- 整个应用总体 QPS和带宽\n\n### Pod 的数据管理\n\nlocal-ssd∶独占的本地磁盘，独占IO，固定大小，读写性能高。\n\nLocal-dynamic∶基于LVM，动态分配空间，效率低。\n\n![image.png](Assets/image_1667146864819_0.png)\n\n### 我的数据应该保存在哪里\n\n![image.png](Assets/image_1667147111012_0.png)\n\n### 应用配置\n\n**传入方式**\n\n- Environment Variables\n- Volume Mount\n\n**数据来源**\n\n- Configmap\n- Secret\n- Downward API\n\n### 高可用部署\n\n- 需要多少实例?\n- 如何控制失败域，部署在几个地区，AZ，集群?\n- 如何进行精细的流量控制?\n- 如何做按地域的顺序更新?\n- 如何回滚?\n\n### 如何应对基础架构的影响\n\n![image.png](Assets/image_1667147403099_0.png)\n\n### PodDisruptionBudget\n\nPDB是为了自主中断时保障应用的高可用。\n\n在使用 PDB 时，你需要弄清楚你的应用类型以及你想要的应对措施：\n\n**无状态应用：**\n\n- 目标：至少有60%副本 Available。\n- 方案：创建 PDB Object，指定 minAvailable为60%或者maxUnavailable为40%\n\n**单实例的有状态应用∶**\n\n- 目标：终止这个实例之前必须提前通知客户并取得同意。\n- 方案：创建 PDB Object，并设置 maxUnavailable为0。\n\n**多实例的有状态应用：**\n\n- 目标最少可用的实例数不能少于某个数N，例如 etcd。\n- 方案：设置 maxUnavailable=1或者minAvailable=N，分别允许每次只删除一个实例和每次删除 expected replicas-minAvailable个实例。\n\n### 基础架构与应用团队的约束\n\n**基础架构团队**在移除一个节点时，应遵循如下流程\n\n将node置为不可调度\n\n`kubectl cordon \u003cnode name\u003e`\n\n执行node drain排空节点，将其上运行的\n\n`Pod平滑迁移至其他节点kubectl drain \u003cnode name\u003e`\n\n**应用开发人员**针对敏感应用，可定义PDB来确保应用不会被意外中断\n\n```yaml\napiVersion: policy/v1betal\nkind: PodDisruptionBudget\nmetadata:\n  name: zk-pdb\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: zookeeper\n```\n\n### 部署方式\n\n- 多少实例\n- 更新策略\n\t- MaxSurge\n\t- MaxUnavailable（需要考虑ResourceQuota 的限制）\n- 深入理解 PodTemplateHash 导致的应用的易变性\n\n### 服务发布\n\n- 需要把服务发布至集群内部或者外部，服务的不同类型：\n\t- ClusterIP(Headless)\n\t- NodePort\n\t- LoadBalancer\n\t- ExternalName\n- 证书管理和七层负载均衡的需求\n- 需要 gRPC负载均衡如何做?\n- DNS 需求\n- 与上下游服务的关系\n\n### 服务发布的挑战\n\n- **kube-dns**\n\t- DNS TTL 问题\n- **Service**\n\t- ClusterIP 只能对内\n\t- Kube-proxy 支持的 iptables/ipvs规模有限\n\t- IPVS的性能和生产化问题\n\t- kube-proxy 的 drift 问题\n\t- 频繁的 Pod变动（specchange，failover，crashLoop）导致LB频繁变更\n\t- 对外发布的 Service需要与企业 ELB即成\n\t- 不支持gRPC\n\t- 不支持自定义 DNS和高级路由功能\n- **Ingress**\n\t- Spec要 deprecate\n- **其他可选方案?**\n\n### 无状态应用管理\n\n- Replicaset副本集\n\t- 用什么Pod模版创建多少个实例。\n\t- replicas: 2\n- Deployment\n\t- 描述的是部署过程\n \n版本管理\n\n```yaml\nannotations:\n  deployment.kubernetes.io/revision: \"1\"\nspec:\n  revisionHistoryLimit: 10\n```\n\n滚动升级策略\n\n```yaml\nstrategy:\n  rollingUpdate:\n  \tmaxSurge: 25%\n    maxUnavailable: 1\n  type: RollingUpdate\n```\n\n### 有状态应用管理\n\n**Statefulset**\n\n与 deployment相比，多了\n\n`serviceName: nginx-ss`\n\n**Volume claim template**\n\n```yaml\nvolumeClaimTemplates:\n  metadata:\n    creationTimestamp: null\n    name: www\n  spec:\n    accessModes:\n\nReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n    volumeMode: Filesystem\n```\n\n### 有状态应用-Operator\n\n**创建 Operator 的关键是CRD（自定义资源）的设计。**\n\nKubernetes 对象是可扩展的，扩展的方式有\n\n- **基于原生对象**\n\t- 生成 types 对象，并通过client-go生成相应的clientset， lister， informer。\n\t- 实现对象的registerybackend，即定义对象任何存储进etcd。\n\t- 注册对象的scheme至apiserver。\n\t- 创建该对象的 apiservice 生命，注册该对象所对应的api handler。\n\t- **基于原生对象往往需要通过 aggregation apiserver 把不同对象组合起来。**\n- **基于CRD**\n\t- 在不同应用业务环境下，对于平台可能有一些特殊的需求，这些需求可以抽象为Kubernetes的扩展资源，而Kubernetes的CRD（CustomResourceDefinition）为这样的需求提供了轻量级的机制，保证新的资源的快速注册和使用。\n\t- 在更老的版本中，TPR（ThirdPartyResource）是与CRD类似的概念，但是在1.9以上的版本中被弃用，而CRD则进入的beta状态。\n\n### 如何使用CRD\n\n用户向Kubernetes API服务注册一个带特定 schema 的资源，并定义相关 API\n\n- 注册一系列该资源的实例\n- 在 Kubernetes 的其它资源对象中引用这个新注册资源的对象实例\n- 用户自定义的controller例程需要对这个引用进行释义和实施，让新的资源对象达到预期的状态\n\n### 基于CRD的开发过程\n\n**借助Kubernetes RBAC和 authentication机制来保证该扩展资源的 security、access control、authentication 和 multitenancy。**\n\n将扩展资源的数据存储到Kubernetes 的 etcd 集群。\n\n借助Kubernetes提供的controller模式开发框架，实现新的 controller，并借助 APIServer 监听etcd 集群关于该资源的状态并定义状态变化的处理逻辑。\n\n该功能可以让开发人员扩展添加新功能，更新现有的功能，并且可以自动执行一些管理任务，这些自定义的控制器就像Kubernetes原生的组件一样，Operator 直接使用Kubernetes API进行开发，也就是说**他们可以根据这些控制器内部编写的自定义规则来监控集群、更改 Pods/Services、对正在运行的应用进行扩缩容。**\n\n### 控制器模式\n\n![image.png](Assets/image_1667153037444_0.png)\n\n## 有状态应用的复杂性讨论\n\n### 有状态应用部署示例-mysqL\n\n**1. 高可用部署**\n\n- 构建 Galara cluster，提供多活高可用 mysql集群\n- 多实例跨集群/跨机架/跨主机\n\n**2. 持久化存储**\n\n- 需要为每个Pod 创建 PVC并 mount\n- 读写性能保证\n- local dynamic作为数据盘，cephfs 作为备份盘\n\n**3. 有状态应用的复杂配置**\n\n- 与无状态应用不一样，mysql 需要复杂配置以完成 galara集群的构建/etc/mysql/conf.d/galera.c\n\n### 启动顺序\n\n- **在PrimaryComponent节点上运行**\n\t- mysqld_bootstrap\n- **在其他节点上运行**\n\t- systemctl start mysql\n- **发生了什么**\n\t- 当节点第一次启动时，会自动生成UUID以代表当前节点身份\n\t- 启动后，garala会在数据目录生成gvwstate.dat文件，该文件内容记录Primary Component 的UUID以及连接到当前Primary Component的节点的UUID\n\t- 如果Primary Component出现故障，则剩余节点会重新选择新的Primary Component\n\t- 若该文件已经存在，则无需额外执行bootstrap命令启动PrimaryCompont，可依此规则编写Operator在多个节点构建此文件\n\n### 健康检查\n\n- **mysql 提供健康检查 API**\n\t- 检查集群成员是否能接受查询请求\n\t\t- SHOW GLOBAL STATUS LIKE 'wsrep_ready';\n\t- 检查节点是否与其他节点网络互通\n\t\t- SHOW GLOBAL STATUS LIKE 'wsrep connected';\n\t- 检查节点自场次查询结束后接收到的查询请求数量，如果结果为非0，意味着写请求不能立即处理\n\t\t- SHOW STATUS LIKE 'wsrep_local_recv_queue_avg';\n- **健康检查应该影响 Pod 的readiness probe，在进行版本升级时，确保大多数集群节点状态一致。**\n\n### 数据备份\n\n- **推荐为 mysql 创建不同类型的 volume**\n\t- Local volume 用来做数据盘\n\t- Network volume 用来做数据备份\n- **创建 cronjob，每天将数据备份至backup目录**\n\t- 备份文件为\n- **一键恢复能力**\n\t- 导入备份目录的 sql file\n\n### 版本发布和故障转移\n\n针对配置了Local disk的 Pod，**当发生因版本变更而引发的Pod 重建时**，新Pod在进行调度时，调度器会查询 Pod 挂载的volume 所在节点，并将新 Pod 优先调度至该节点，此场景不涉及到数据恢复。\n\n其开销与 mysql进程重启相差不大。\n\n---\n\n**如果节点出现故障，如硬件故障，Kubernetes 的Evict Manager 会将该Pod从故障节点驱逐，Operator 应确保新 Pod 会被重新构建。** 而新 Pod会被调度至新节点，此场景等价于替换mysql中的少数节点。\n\ngalara 集群中的少数 mysql节点替换，不涉及到数据迁移，galara 会确保新节点的数据同步\n\n**若整个mysql 集群需要做数据恢复，则应该从backup 目录对应的网络 volume恢复数据。** 此时\n\n- 可选择：\n\t- 只恢复单一节点数据：配置简单\n\t- 恢复所有节点数据：恢复速度快\n- 与基础架构的Contract\n\t- PodDisruptionBudget\n\n# Spec管理神器-Helm\n\n## 什么是Helm\n\n**Helm 特性**\n\n- Helm chart是创建一个应用实例的必要的配置组，也就是一堆 Spec。\n- 配置信息被归类为模版（Template）和值（Value），这些信息经过渲染生成最终的对象。\n- 所有配置可以被打包进一个可以发布的对象中。\n- 一个release 就是一个有特定配置的chart 的实例。\n\n## Helm 的组件\n\n**Helm client**\n\n- 本地chart开发\n- 管理 repository\n- 管理 release\n- 与helm library交互\n- 发送需要安装的chart\n- 请求升级或着卸载存在的 release\n\n**Helm library**\n\n- 负责与 APIserver交互，并提供以下功能\n\t- 基于chart和 configuration 创建一个release\n\t- 把 chart安装进kubernetes，并提供相应的 release 对象\n\t- 升级和卸载\n\t- Helm 采用Kubernetes存储所有配置信息，无需自己的数据库\n\n# metrics-server\n\n## Aggregated APIServer\n\n![image.png](Assets/image_1667209374158_0.png)\n\n## Metrics-Server\n\n**metrics-server 是Kubernetes 监控体系中的核心组件之一**，它负责从kubelet 收集资源指标，然后对这些指标监控数据进行聚合（依赖kube-aggregator），并在Kubernetes Apiserver中通过 MetricS API/apis/metrics.k8s.io/）公开暴露它们，但是 **metrics-server只存储最新的指标数据（CPU/Memory）**。\n\n你的 kube-apiserver要能访问到metrics-server;\n\n需要kube-apiserver启用聚合层;\n\n组件要有认证配置并且绑定到 metrics-server;\n\nPod / Node 指标需要由 Summary API通过 kubelet公开。\n\n![image.png](Assets/image_1667212041763_0.png)\n\n# 自动扩容缩容-HPA\n\n## 横向伸缩和纵向伸缩\n\n- **应用扩容是指在应用接收到的并发请求已经处于其处理请求极限边界的情形下，扩展处理能力而确保应用高可用的技术手段**\n- Horizontal Scaling\n\t- 所谓横向伸缩是指通过增加应用实例数量分担负载的方式来提升应用整体处理能力的方式\n- Vertical Scaling\n\t- 所谓纵向伸缩是指通过增加单个应用实例资源以提升单个实例处理能力，进而提升应用整体处理能力的方式\n\n![image.png](Assets/image_1667212325432_0.png)\n\n## 理解云原生的弹性能力\n\n![image.png](Assets/image_1667212517047_0.png)\n\n## HPA\n\n**HPA（Horizontal Pod Autoscaler）是Kubernetes 的一种资源对象**，能够根据某些指标对在 statefulSet、replicaSet、deployment 等集合中的 Pod 数量进行横向动态伸缩，使运行在上面的服务对指标的变化有一定的自适应能力。\n\n因节点计算资源固定，当 Pod 调度完成并运行以后，动态调整计算资源变得较为困难，因为横向扩展具有更大优势，HPA是扩展应用能力的第一选择。规则。\n\nHPA依赖于Metrics-Server。\n\n## HPA Spec\n\n```yaml\napiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: mynginx\nspec:\n  #HPA的伸缩对象描述，HPA会动态修改该对象的Pod数量\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: mynginx\n  # HPA 的最小 Pod 数量和最大 Pod数量\n  minReplicas: 1\n  maxReplicas: 3\n#监控的指标数组，支持多种类型的指标共存\nmetric:\n```\n\n## HPA 支持的指标类型\n\n对于按 Pod统计的资源指标（如 CPU），控制器从资源指标 API中获取每一个**HorizontalPodAutoscaler指定的 Pod 的度量值**，如果设置了目标使用率，控制器获取每个Pod 中的容器资源使用情况，并计算资源使用率。 如果设置了target 值，将直接使用原始数据（不再计算百分比）。\n\n如果 Pod使用自定义指示，控制器机制与资源指标类似，区别在于自定义指标只使用原始值，而不是使用率。\n\n如果 Pod使用对象指标和外部指标（每个指标描述一个对象信息）。 这个指标将直接根据目标设定值相比较，并生成一个上面提到的扩缩比例。**在 autoscaling/v2beta2版本 API中，这个指标也可以根据 Pod数量平分后再计算。**\n\n## HPA指标\n\n```yaml\n#Resource类型的指标\n- type: Resource\n  resource:\n    name: cpu\n    #Utilization类型的目标值，Resource类型的指标只支持Utilization和AverageValue类型的目标值\n    target:\n      type: Utilization\n      averageUtilization: 50\n \n#Pods类型的指标\n- type: Pods\n  pods:\n    metric:\n      name: packets-per-second\n      #AverageValue类型的目标值，Pods指标类型下只支持AverageValue类型的目标值\n      target:\n        type: AverageValue\n        averageValue: 1k\n```\n\n## 算法细节\n\nHPA算法非常简单\n\n**期望副本数=ceil\\[当前副本数*(当前指标/期望指标)\\]**\n\n当前度量值为 200m，目标设定值为100m，那么由于200.0/100.0 == 2.0，副本数量将会翻倍。如果当前指标为50m，副本数量将会洞半，因为 50.0/100.0== 0.5。如果计算出的扩缩上例接近 1.0（根据--horizontal-pod-autoscaler-tolerance参数全局配置的容忍值，默认为 0.1），将会放弃本次扩缩。\n\n## 滚动升级时扩缩\n\n- 当你为一个Deployment 配置自动扩缩时，你要为每个 Deployment 绑定一个HorizontalPodAutoscaler。\n- HorizontalPodAutoscaler 管理 Deployment 的 replicas字段。\n- Deployment Controller 负责设置下层ReplicaSet 的 replicas字段，以便确保在上线及后续过程副本个数合适。\n- **想一想：为什么 deploymentSpec中的 replicas字段的类型为\\*int，而不是int?**\n\n## 冷却/延迟支持\n\n当使用Horizontal Pod AutosCaler 管理一组副本扩缩时，有可能因为指标动态的变化造成副本数量频繁的变化，有时这被称为 **抖动（Thrashing）**。\n\n--horizontal-pod-autoscaler-downscale-stabilization∶设置缩容冷却时间窗口长度。水平 Pod扩缩器能够记住过去建议的负载规模，并仅对此时间窗口内的最大规模执行操作。**默认值是5分钟（5m0s）。**\n\n## 扩缩策略\n\n在 Spec字段的 behavior 部分可以指定一个或多个扩缩策略。 当指定多个策略时，默认选择允许更改最多的策略。下面的例子展示了缩容时的行为：\n\n```yaml\nbehavior:\n  scaleDOown:\n    policies:\n    - type: Pods\n      value: 4\n      periodSeconds: 60\n    - type: Percent\n    value: 10\n    periodSeconds: 60\n```\n\n## HPA 存在的问题\n\n- 基于指标的弹性有滞后效应，因为弹性控制器操作的链路过长。\n- 从应用负载超出阈值到HPA完成扩容之间的时间差包括∶\n\t- 应用指标数据已经超出阈值；\n\t- HPA定期执行指标收集滞后效应；\n\t- HPA控制 Deployment 进行扩容的时间；\n\t- Pod调度，运行时启动挂载存储和网络的时间；\n\t- 应用启动到服务就绪的时间。\n- **很可能在突发流量出现时，还没完成弹性扩容，既有的服务实例已经被流量击垮。**\n\n# 自动扩容缩容-VPA\n\n## VPA\n\n**VPA全称Vertical Pod Autoscaler，即垂直 Pod 自动扩缩容**，它根据容器资源使用率自动设置CPU 和 内存的 requests，从而允许在节点上进行适当的调度，以便为每个 Pod提供适当的资源。它既可以缩小过度请求资源的容器，也可以根据其使用情况随时提升资源不足的容量。\n\n使用VPA 的意义：\n\n- Pod 资源用其所需，提升集群节点使用效率；\n- 不必运行基准测试任务来确定CPU 和内存请求的合适值；\n- VPA 可以随时调整CPU和内存请求，无需人为操作，因此可以减少维护时间。\n\n**注意：VPA 目前还没有生产就绪，在使用之前需要了解资源调节对应用的影响。**\n\n## VPA架构图\n\n![image.png](Assets/image_1667218774861_0.png)\n\n## VPA组件\n\nVPA 引入了一种新型的 API资源：**VerticalPodAutoscaler**。\n\n**VPA Recommender**监视所有 Pod，不断为它们计算新的推荐资源，并将推荐值存储在VPA对象中。它使用来自Metrics-Server 的集群中所有 Pod 的利用率和 OOM事件。\n\n所有 Pod 创建请求都通过 **VPA Admission Controller**。\n\n**VPAUpdater** 是负责 Pod 实时更新的组件。如果 Pod在\"Auto\"模式下使用VPA，则Updater 可以决定使用推荐器资源对其进行更新。\n\n**History Storage**是一个存储组件（如Prometheus），它使用来自 API Server 的利用率信息和 OOM（与推荐器相同的数据）并将其持久存储。\n\nVPA 更新模式：\n- Off\n- Auto\n\n## VPA 工作原理\n\n![image.png](Assets/image_1667218863362_0.png)\n\n## Recommender 设计理念\n\n**推荐模型（MVP）假设内存和 CPU利用率是独立的随机变量，其分布等于过夫 N 天观察到的变量（推荐值为N=8以捕获每周峰值）。**\n\n对于 CPU，目标是将容器使用率超过请求的高百分比（例如 95%）的时间部分保持存某个阈值（例如1%的时间）以下。在此模型中，**\"CPU使用率\"** 被定义为在短时间间隔内测量的平均使用率。测量间隔越短，针对尖峰、延迟敏感的工作负载的建议质量就越高。**最小合理分辨率为1/min，推荐为1/sec。**\n\n对于**内存**，目标是将特定时间窗口内容器使用率超过请求的概率保持在某个阈值以下（例如，**24小时内低于1%**）。**窗口必须很长（≥24 小时）** 以确保由OOM 引起的驱逐不会明显影响（a）服务应用程序的可用性（b）批处理计算的进度（更高级的模型可以允许用户指定 SLO来控制它）。\n\n## 主要流程\n\n![image.png](Assets/image_1667219529173_0.png)\n\n## 滑动窗口与半衰指数直方图\n\nRecommender的资源推荐算法主要受Google AutoPilot moving window 推荐器的启发，**假设CPU和Memory消耗是独立的随机变量，其分布等于过去N天观察到的变量分布（推荐值为N=8以捕获每周业务容器峰值）。**\n\nRecommender组件获取资源消耗实时数据，存到相应资源对象CheckPoint中。CheckPoint CRD 资源本质上是一个直方图。\n\n![image.png](Assets/image_1667219600773_0.png)\n\n## 直方图统一对外提供的接口\n\n![image.png](Assets/image_1667219640780_0.png)\n\n## 半衰期和权重系数\n\n为每个样本数据权重乘上**指数2^（（sampleTime-referenceTimestamp）/ halfLife）**，以保证较新的样本被赋予更高的权重，而较老的样本随时间推移权重逐步衰减。\n\n默认情况下，**每24h 为一个半衰期**，即每经过 24h，直方图中所有样本的权重（重要性）衰减为原来的一半。\n\n**当指数过大时，referenceTimestamp 就需要向前调整，以避免浮点乘法计算时向上溢出。**\n\n- CPU使用量样本对应的权重是基于容器 CPU request值确定的。当 CPUrequest 增加时，对应的权重也随之增加。\n- 而Memory 使用量样本对应的权重固定为1.0。\n\n## VPA\n\n**VPA 的成熟度还不足**\n\n更新正在运行的 Pod 资源配置是VPA的一项试验性功能，会导致 Pod 的重建和重启，而且有可能被调度到其他的节点上。\n\nVPA不会驱逐没有在副本控制器管理下的 Pod。**目前对于这类 Pod，Auto 模式等同于Initial 模式。**\n\n目前VPA不能和监控CPU和内存度量的 **Horizontal Pod Autoscaler（HPA）**同时运行，除非 HPA 只监控其他定制化的或者外部的资源度量。\n\nVPA使用 admission webhook 作为其准入控制器。如果集群中有其他的 admission webhook，需要确保它们不会与VPA发生冲突。准入控制器的执行顺序定义在 APIServer的配置参数中。\n\nVPA会处理出现的绝大多数 **OOM（Out Of Memory）** 的事件，但不保证所有的场景下都有效。\n\nVPA的性能还没有在大型集群中测试过。\n\nVPA 对 Pod 资源 requests 的修改值可能超过实际的资源上限，例如节点资源上限、空闲资源或资源配额，从而造成， Pod 处于 Pending状态无法被调度。同时使用集群自动伸缩（ClusterAutoScaler）可以一定程度上解决这个问题。\n\n多个VPA同时匹配同一个 Pod 会造成未定义的行为。\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4":{"title":"常用命令","content":"\n# Kubectl\n\n## Kubectl 命令和 kubeconfig\n\n* kubectl 是一个 Kubernetes 的命令行工具，它允许Kubernetes 用户以命令行的方式与 Kubernetes 交互，其默认读取配置文件 ~/.kube/config。\n* kubectl 会将接收到的用户请求转化为 rest 调用以rest client 的形式与 apiserver 通讯。\n* apiserver 的地址，用户信息等配置在 kubeconfig。\n\n## 常用命令\n\n* kubectl get po –oyaml -w\n\t* kubectl    可查看对象。\n\t* -oyaml    输出详细信息为 yaml 格式。\n\t* -w watch    该对象的后续变化。\n\t* -owide    以详细列表的格式查看对象。\n\n## 在终端通过stdin读取inline YAML\n\n```bash\n\n$ kubectl apply -f - \u003c\u003cEOF\n\u003c-- insert YAML content here --\u003e\nEOF\n```\n\nOR\n\n```bash\n$ cat file.yaml | kubectl apply -f -\n```\n\n## 跨 Namespace 同步 Secret 和 ConfigMap\n\n### 手动\n\n```bash\nkubectl get secret \u003csecret-name\u003e -n \u003csource_ns\u003e -o json \\\n | jq 'del(.metadata[\"namespace\",\"creationTimestamp\",\"resourceVersion\",\"selfLink\",\"uid\"])' \\\n | kubectl apply -n \u003ctarget_ns\u003e -f -\n```\n\n### More\n\nhttps://zhuanlan.zhihu.com/p/519168101\n\n## finalizer导致namespace Terminating\n\n### namespace资源对象的spec.finalizer[] 列表中不为空\n\n```bash\nNAMESPACE=delete-me\nkubectl get ns $NAMESPACE -o json | jq '.spec.finalizers=[]' \u003e ns.json\nkubectl proxy --port=8899 \u0026\nPID=$!\ncurl -X PUT http://localhost:8899/api/v1/namespaces/$NAMESPACE/finalize -H \"Content-Type: application/json\" --data-binary @ns.json\nkill $PID\n```\n\n### namespace资源对象的metadata.finalizer[] 列表不为空\n\n```bash\n$ kubelet edit ns \u003cname\u003e #将metadata.finalizer[]列表删除\n```\n\n# Secret\n\n## 创建secret\n\n`kubectl create secret generic \u003csecret name\u003e --from-literal=\u003ckey\u003e=\u003cvalue\u003e -n \u003cnamespace\u003e`\n\n## 获取secret（password）\n\n```sh\nkubectl get secret -n \u003cnamespace\u003e \u003csecret name\u003e -o jsonpath='{.data}'\nkubectl get secret -n \u003cnamespace\u003e \u003csecret name\u003e -o jsonpath='{.data.password}' | base64 -d\n```\n\n## 删除secret\n\n`kubectl -n \u003cnamespace\u003e delete secrets \u003csecret name\u003e`\n\n## 创建imagePullSecrets\n\n### 通过命令行创建\n\n```bash\n$ kubectl create -n \u003cnamespace\u003e secret docker-registry \u003csecret-name\u003e \\\n\t\t  --docker-server=\u003cdocker-server\u003e \\\n          --docker-username=\u003cusername\u003e \\\n          --docker-password=\u003cpassword\u003e \\\n          --docker-email=\u003cemail\u003e\n```\n\n### 基于现有凭证创建\n\n```basg\nkubectl create -n \u003cnamespace\u003e secret generic \u003csecret-name\u003e \\\n    --from-file=.dockerconfigjson=\u003cpath/to/.docker/config.json\u003e \\\n    --type=kubernetes.io/dockerconfigjson\n```\n\n# kubecm\n\n\u003e https://kubecm.cloud/zh-cn/introduction\n\n交互式管理kubeconfig工具。\n\n```bash\n# 查看 $HOME/.kube/config 中所有的 context\nkubecm list\n  \n# 添加 example.yaml 到 $HOME/.kube/config.yaml，该方式不会覆盖源 kubeconfig，只会在当前目录中生成一个 config.yaml 文件\nkubecm add -f example.yaml\n  \n# 功能同上，但是会将 example.yaml 中的 context 命名为 test\nkubecm add -f example.yaml -n test\n  \n# 添加 -c 会覆盖源 kubeconfig\nkubecm add -f example.yaml -c\n  \n# 交互式删除\nkubecm delete\n  \n# 删除指定 context\nkubecm delete my-context\n  \n# 合并 test 目录中的 kubeconfig,该方式不会覆盖源 kubeconfig，只会在当前目录中生成一个 config.yaml 文件\nkubecm merge -f test \n  \n# 添加 -c 会覆盖源 kubeconfig\nkubecm merge -f test -c\n  \n# 交互式重命名\nkubecm rename\n  \n# 将 dev 重命名为 test\nkubecm rename -o dev -n test\n  \n# 重命名 current-context 为 dev\nkubecm rename -n dev -c\n  \n# 交互式切换 namespace\nkubecm namespace\n  \n# 或者\nkubecm ns\n  \n# 切换默认 namespace 为 kube-system\nkubecm ns kube-system\n```\n\n# 调试命令\n\n## crictl查找容器PID\n\n```bash\n# 0.设置runtime-endpoint\ncrictl -r unix:///run/k0s/containerd.sock\n  \n# 1.查找容器id\ncrictl ps 或通过k8s查找\n  \n# 2.通过inspect查找PID\ncrictl inspect --output go-template --template '{{.info.pid}}' $containerid\n```\n\n## docker查找容器PID\n\n```bash\n# 1.查找容器id\ndocker ps\n  \n# 2.通过inspect查找PID\ndocker inspect $containerid -f '{{ .State.Pid }}'\n```\n\n## nsenter\n\n\u003e https://qingwave.github.io/k8s-debug-nsenter\n\n通过`nsenter`可以轻松在宿主机进入容器的网络命令空间。之后便可以使用宿主机各种工具`tcpdump`, `netstat`等命令。\n\n```bash\n# 进入容器networker namespace\nnsenter -n --target $pid\n```\n\n# kubectl debug\n\n\u003e https://kubernetes.io/zh-cn/docs/tasks/debug/debug-application/debug-running-pod/\n\n### 使用临时容器来调试\n\n当由于容器崩溃或容器镜像不包含调试程序（例如[无发行版镜像](https://github.com/GoogleContainerTools/distroless)等） 而导致 `kubectl exec` 无法运行时，[临时容器](https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/ephemeral-containers/)对于排除交互式故障很有用。\n\n\u003e **说明：** [容器运行时](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes)必须支持 `--target` 参数。 如果不支持，则临时容器可能不会启动，或者可能使用隔离的进程命名空间启动， 以便 `ps` 不显示其他容器内的进程。\n\n```bash\nkubectl debug -it $podName --image=busybox:1.28 --target=$containerName\n```\n\n### 通过 Pod 副本调试\n\n有些时候 Pod 的配置参数使得在某些情况下很难执行故障排查。 例如，在容器镜像中不包含 shell 或者你的应用程序在启动时崩溃的情况下， 就不能通过运行 `kubectl exec` 来排查容器故障。 在这些情况下，你可以使用 `kubectl debug` 来创建 Pod 的副本，通过更改配置帮助调试。\n\n#### 在添加新的容器时创建 Pod 副本\n\n当应用程序正在运行但其表现不符合预期时，你会希望在 Pod 中添加额外的调试工具， 这时添加新容器是很有用的。\n\n例如，应用的容器镜像是建立在 `busybox` 的基础上， 但是你需要 `busybox` 中并不包含的调试工具。 你可以使用 `kubectl run` 模拟这个场景：\n\n```bash\n\nbectl run myapp --image=busybox:1.28 --restart=Never -- sleep 1d\n```\n\n通过运行以下命令，建立 `myapp` 的一个名为 `myapp-debug` 的副本， 新增了一个用于调试的 Ubuntu 容器，\n\n```bash\nkubectl debug myapp -it --image=ubuntu --share-processes --copy-to=myapp-debug\n```\n\n\u003e` --share-processes` 允许在此 Pod 中的其他容器中查看该容器的进程。 参阅[在 Pod 中的容器之间共享进程命名空间](https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/share-process-namespace/) 获取更多信息。\n\n#### 在改变 Pod 命令时创建 Pod 副本\n\n有时更改容器的命令很有用，例如添加调试标志或因为应用崩溃。\n\n为了模拟应用崩溃的场景，使用 `kubectl run` 命令创建一个立即退出的容器：\n\n```bash\n\nbectl run --image=busybox:1.28 myapp -- false\n```\n\n你可以使用 `kubectl debug` 命令创建该 Pod 的一个副本， 在该副本中命令改变为交互式 shell：\n\n```bash\n\nbectl debug myapp -it --copy-to=myapp-debug --container=myapp -- sh\n```\n\n现在你有了一个可以执行类似检查文件系统路径或者手动运行容器命令的交互式 shell。\n\n\u003e 要更改指定容器的命令，你必须用 `--container` 命令指定容器的名字， 否则 `kubectl debug` 将建立一个新的容器运行你指定的命令。\n\n#### 在更改容器镜像时拷贝 Pod\n\n在某些情况下，你可能想要改动一个行为异常的 Pod，即从其正常的生产容器镜像更改为包含调试构建程序或其他实用程序的镜像。\n\n下面的例子，用 `kubectl run` 创建一个 Pod：\n\n```bash\n\nkubectl run myapp --image=busybox:1.28 --restart=Never -- sleep 1d\n```\n\n现在可以使用 `kubectl debug` 创建一个拷贝并将其容器镜像更改为 `ubuntu`：\n\n```bash\nkubectl debug myapp --copy-to=myapp-debug --set-image=*=ubuntu\n```\n\n`--set-image` 与 `container_name=image` 使用相同的 `kubectl set image` 语法。 `*=ubuntu` 表示把所有容器的镜像改为 `ubuntu`。\n\n### 在节点上通过 shell 来进行调试\n\n如果这些方法都不起作用，你可以找到运行 Pod 的节点，然后创建一个 Pod 运行在该节点上。 你可以通过 `kubectl debug` 在节点上创建一个交互式 Shell：\n\n```bash\nkubectl debug node/mynode -it --image=ubuntu\n```\n\n当在节点上创建调试会话，注意以下要点：\n* `kubectl debug` 基于节点的名字自动生成新的 Pod 的名字。\n* 节点的根文件系统会被挂载在 `/host`。\n* 新的调试容器运行在主机 IPC 名字空间、主机网络名字空间以及主机 PID 名字空间内， Pod 没有特权，因此读取某些进程信息可能会失败，并且 `chroot /host` 也会失败。\n* 如果你需要一个特权 Pod，需要手动创建。\n\n当你完成节点调试时，不要忘记清理调试 Pod：\n\n```bash\nkubectl delete pod node-debugger-mynode-pdx84\n```\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0":{"title":"服务发现","content":"\n# 服务发布\n\n* 需要把服务发布至集群内部或者外部，服务的不同类型\n\t* ClusterlP(Headless)\n\t* NodePort\n\t* LoadBalancer\n\t* ExternalName\n* 证书管理和七层负载均衡的需求\n* 需要gRPC负载均衡如何做?\n* DNS需求\n* 与上下游服务的关系\n\n## 服务发布的挑战\n\n- kube-dns\n\t- DNS TTL问题\n- Service\n\t- ClusterIP只能对内\n\t- kube-proxy支持的iptables/ipvs规模有限\n\t- IPVS的性能和生产化问题\n\t- kube-proxy的drift问题\n\t- 频繁的Pod变动（spec change，failover，crashloop）导致LB频繁变更\n\t- 对外发布的Service需要与企业ELB即成\n\t- 不支持gRPC\n\t- 不支持自定义DNS和高级路由功能\n- Ingress\n\t- Spec的成熟度?\n- 其他可选方案?\n\n# 微服务架构下的高可用挑战\n\n## 服务发现\n\n微服务架构是由一系列职责单一的细粒度服务构成的分布式网状结构，服务之间通过轻量机制进行通信，这时候必然引入一个服务注册发现问题，也就是说服务提供方要注册通告服务地址，服务的调用方要能发现目标服务。\n\n同时服务提供方一般以集群方式提供服务，也就引入了负载均衡和健康检查问题。\n\n## 互联网架构发展历程\n\n![image.png](Assets/image_1666185055076_0.png)\n\n## 理解网络包格式\n\n![image.png](Assets/image_1666185072055_0.png)\n\n## 集中式LB服务发现\n\n在服务消费者和服务提供者之间有一个独立的LB。\n\nLB上有所有服务的地址映射表，通常由运维配置注册。\n\n当服务消费方调用某个目标服务时，它向LB发起请求，由LB以某种策略（比如Round-Robin）做负载均衡后将请求转发到目标服务。\n\nLB一般具备健康检查能力，能自动摘除不健康的服务实例。\n\n服务消费方通过DNS发现LB，运维人员为服务配置一个DNS域名，这个域名指向LB。\n\n![image.png](Assets/image_1666185251776_0.png){:height 238, :width 657}\n\n---\n\n集中式LB方案实现简单，在LB上也容易做集中式的访问控制，这一方案目前还是业界主流。\n\n集中式LB的主要问题是单点问题，所有服务调用流量都经过LB，当服务数量和调用量大的时候，LB容易成为瓶颈，且一旦LB发生故障对整个系统的影响是灾难性的。\n\nLB在服务消费方和服务提供方之间增加了一跳（hop），有一定性能开销。\n\n## 进程内LB服务发现\n\n进程内LB方案将LB的功能以库的形式集成到服务消费方进程里头，该方案也被称为客户端负载方案。\n\n服务注册表（SerViCe ReqistrV）配合支持服务自注册和自发现，服务提供方启动时.首先将服务地址注册到服务注册表（同时定期报心跳到服务注册表以表明服务的存活状态）。\n\n服务消费方要访问某个服务时，它通过内置的LB组件向服务注册表查询（同时缓存并定期刷新）目标服务地址列表，然后以某种负载均衡策略选择一个目标服务地址，最后向目标服务发起请求。\n\n这一方案对服务注册表的可用性（Availability）要求很高，一般采用能满足高可用分布式一致的组件（例如ZooKeeper， Consul， etcd等）来实现。\n\n![image.png](Assets/image_1666185729019_0.png)\n\n---\n\n进程内LB是一种分布式模式，LB和服务发现能力被分散到每一个服务消费者的进 •程内部，同时服务消费方和服务提供方之间是直接调用，没有额外开销，性能比较好。该方案以客户库（Client Librarv）的方式集成到服务调用方进程里头，如果企业内有多种不同的语言栈，就要配合开发多种不同的客户端，有一定的研发和维护成本。\n\n一旦客户端跟随服务调用方发布到生产环境中，后续如果要对客户库进行升级，势必要求服务调用方修改代码并重新发布，所以该方案的升级推广有不小的阻力。\n\n## 独立LB进程服务发现\n\n针对进程内LB模式的不足而提出的一种折中方案，原理和第二种方案基本类似。\n\n不同之处是，将LB和服务发现功能从进程内移出来，变成主机上的一个独立进程，主机上的一个或者多个服务要访问目标服务时，他们都通过同一主机上的独立LB进程做服务发现和负载均衡。\n\nLB独立进程可以进一步与服务消费方进行解耦，以独立集群的形式提供高可用的负载均衡服务。\n\n这种模式可以称之为真正的\"软负载（Soft Load Balancing）\"。\n\n![image.png](Assets/image_1666185836227_0.png)\n\n---\n\n独立LB进程也是一种分布式方案，没有单点问题，一个LB进程挂了只影响该主机上的服务调用方。\n\n服务调用方和LB之间是进程间调用，性能好。\n\n简化了服务调用方，不需要为不同语言开发客户库，LB的升级不需要服务调用方改代码。\n\n不足是部署较复杂，环节多，出错调试排查问题不方便。\n\n## 负载均衡\n\n系统的扩展可分为纵向（垂直）扩展和横向（水平）扩展。\n\n* 纵向扩展，是从单机的角度通过增加硬件处理能力，比如CPU处理能力，内存容量，磁盘等方面，实现服务器处理能力的提升，不能满足大型分布式系统（网站），大流量，高并发，海量数据的问题；\n* 横向扩展，通过添加机器来满足大型网站服务的处理能力。比如∶一台机器不能满足，则增加两台或者多台机器，共同承担访问压力，这就是典型的集群和负载均衡架构。\n\n负载均衡的作用（解决的问题）：\n\n* 解决并发压力，提高应用处理性能，增加吞吐量，加强网络处理能力；\n* 提供故障转移，实现高可用；\n* 通过添加或减少服务器数量，提供网站伸缩性，扩展性；\n* 安全防护，负载均衡设备上做一些过滤，黑白名单等处理。\n\n## DNS负载均衡\n\n最早的负载均衡技术，利用域名解析实现负载均衡，在DNS服务器.配置多个A记录，这些A记录对应的服务器构成集群。\n\n![image.png](Assets/image_1666186231947_0.png)\n\n![image.png](Assets/image_1666186257815_0.png)\n\n## 负载均衡技术概览\n\n![image.png](Assets/image_1666186318831_0.png)\n\n## 网络地址转换\n\n网络地址转换（Network Address Translation，NAT）通常通过修改数据包的源地址（Source NAT）或目标地址（Destination NAT）来控制数据包的转发行为\n\n![image.png](Assets/image_1666186604654_0.png)\n\n## 新建TCP连接\n\n为记录原始客户端IP地址，负载均衡功能不仅需要进行数据包的源目标地址修改，同时要记录原始客户端IP地址，基于简单的NAT无法满足此需求，于是衍生出了基于传输层协议的负载均衡的另一种方案—TCP/UDPTermination方案\n\n![image.png](Assets/image_1666186661092_0.png)\n\n## 链路层负载均衡\n\n在通信协议的数据链路层修改MAC地址进行负载均衡。\n\n数据分发时，不修改IP地址，指修改目标MAC地址，配置真实物理服务器集群所有机器虚拟IP和负载均衡服务器IP地址一致，达到不修改数据包的源地址和目标地址，进行数据分发的目的。实际处理服务器IP和数据请求目的IP一致，不需要经过负载均衡服务器进行地址转换，可将响\n\n应数据包直接返回给用户浏览器，避免负载均衡服务器网卡带宽成为瓶颈。也称为直接路由模式（DR模式）。\n\n![image.png](Assets/image_1666186804807_0.png)\n\n## 隧道技术\n\n负载均衡中常用的隧道技术是IP over IP，其原理是保持原始数据包IP头不变，在IP头外层增加额外的IP包头后转发给上游服务器。\n\n上游服务器接收IP数据包，解开外层IP包头后，剩下的是原始数据包。\n\n同样的，原始数据包中的目标IP地址要配置在上游服务器中，上游服务器处理完数据请求以后，响应包通过网关直接返回给客户端。\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80":{"title":"架构基础","content":"\n# 主要组件\n\n![image.png](Assets/image_1665768740767_0.png)\n\n# Kubernetes 的主节点（Master Node）\n\n- API服务器：API Server\n\t- 这是 Kubernetes 控制面板中唯一带有用户可访问 API 以及用户可交互的组件。API 服务器会暴露一个 RESTful 的 Kubernetes API 并使用 JSON 格式的清单文件（manifest files）。\n- 群的数据存储：Cluster Data Store\n\t- Kubernetes 使用“etcd”。这是一个强大的、稳定的、高可用的键值存储，被Kubernetes 用于长久储存所有的 API 对象。\n- 控制管理器：Controller Manager\n\t- 被称为“kube-controller manager”，它运行着所有处理集群日常任务的控制器。包括了节点控制器、副本控制器、端点（endpoint）控制器以及服务账户等。\n- 调度器：Scheduler\n\t- 调度器会监控新建的 pods（一组或一个容器）并将其分配给节点。\n\n# Kubernetes 的工作节点（Worker Node）\n\n- Kubelet\n\t- 负责调度到对应节点的 Pod 的生命周期管理，执行任务并将 Pod 状态报告给主节点的渠道，通过容器运行时（拉取镜像、启动和停止容器等）来运行这些容器。它还会定期执行被请求的容器的健康探测程序。\n- Kube-proxy\n\t- 它负责节点的网络，在主机上维护网络规则并执行连接转发。它还负责对正在服务的 pods 进行负载平衡。\n\n# etcd\n\netcd 是 CoreOS 基于 Raft 开发的分布式 key-value 存储，可用于服务发现、共享配置以及一致性保障（如数据库选主、分布式锁等）。\n\n- 基本的 key-value 存储；\n- 监听机制；\n- key 的过期及续约机制，用于监控和服务发现；\n- 原子 CAS 和 CAD，用于分布式锁和 leader 选举\n\n![image.png](Assets/image_1665770514940_0.png)\n\n# APIServer\n\nKube-APIServer 是 Kubernetes 最重要的核心组件之一，主要提供以下功能：\n\n- 提供集群管理的 REST API 接口，包括:\n\t- 认证 Authentication；\n\t- 授权 Authorization；\n\t- 准入 Admission（Mutating \u0026 Valiating）。\n- 提供其他模块之间的数据交互和通信的枢纽（其他模块通过 APIServer 查询或修改数据，只有 APIServer 才直接操作 etcd）。\n\t- APIServer 提供 etcd 数据缓存以减少集群对 etcd 的访问。\n\n## APIServer展开\n\n![image.png](Assets/image_1665770845377_0.png)\n\n# Controller Manager\n\nController Manager 是集群的大脑，是确保整个集群动起来的关键；\n\n作用是确保 Kubernetes 遵循声明式系统规范，确保系统的真实状态（ActualState）与用户定义的期望状态（Desired State）一致；\n\nController Manager 是多个控制器的组合，每个 Controller 事实上都是一个control loop，负责侦听其管控的对象，当对象发生变更时完成配置；\n\nController 配置失败通常会触发自动重试，整个集群会在控制器不断重试的机制下确保最终一致性（ **Eventual Consistency**）。\n\n## 控制器的协同工作原理\n\n![image.png](Assets/image_1665819441936_0.png)\n\n# Scheduler\n\n特殊的 Controller，工作原理与其他控制器无差别。\n\nScheduler 的特殊职责在于监控当前集群所有未调度的 Pod，并且获取当前集群所有节点的健康状况和资源使用情况，为待调度 Pod 选择最佳计算节点，完成调度。\n\n**调度阶段分为：**\n\n- Predict：过滤不能满足业务需求的节点，如资源不足、端口冲突等。\n- Priority：按既定要素将满足调度需求的节点评分，选择最佳节点。\n- Bind：将计算节点与 Pod 绑定，完成调度。\n\n# Kubelet\n\nKubernetes 的初始化系统（init system）\n\n- 从不同源获取 Pod 清单，并按需求启停 Pod 的核心组件：\n\t- Pod 清单可从本地文件目录，给定的 HTTPServer 或 Kube-APIServer 等源头获取；\n\t- Kubelet 将运行时，网络和存储抽象成了 CRI，CNI，CSI。\n- 负责汇报当前节点的资源信息和健康状态；\n- 负责 Pod 的健康检查和状态汇报。\n\n![image.png](Assets/image_1665820588387_0.png)\n\n# Kube-Proxy\n\n监控集群中用户发布的服务，并完成负载均衡配置。\n\n每个节点的 Kube-Proxy 都会配置相同的负载均衡策略，使得整个集群的服务发现建立在分布式负载均衡器之上，服务调用无需经过额外的网络跳转（Network Hop）。\n\n负载均衡配置基于不同插件实现：\n\n- userspace。\n- 操作系统网络协议栈不同的 Hooks 点和插件：\n\t- iptables；\n\t- ipvs。\n\n![image.png](Assets/image_1665820733260_0.png)\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3":{"title":"深入理解","content":"\n# API 设计原则\n\n**所有 API 都应是声明式的**\n\n相对于命令式操作，声明式操作对于重复操作的效果是稳定的，这对于容易出现数据丢失或重复的分布式环境来说是很重要的。\n\n声明式操作更易被用户使用，可以使系统向用户隐藏实现的细节，同时也保留了系统未来持续优化的可能性。\n\n此外，声明式的 API 还隐含了所有的 API 对象都是名词性质的，例如 Service、Volume 这些 API 都是名词，这些名词描述了用户所\n\n**期望得到的一个目标对象。**\n\nAPI 对象是彼此互补而且可组合的\n\n这实际上鼓励 API 对象尽量实现面向对象设计时的要求，即“高内聚，松耦合”，对业务相关的概念有一个合适的分解，提高分解出来的对象的可重用性。\n\n**高层 API 以操作意图为基础设计**\n\n如何能够设计好 API，跟如何能用面向对象的方法设计好应用系统有相通的地方，高层设计一定是从业务出发，而不是过早的从技术实现出发。\n\n因此，针对 Kubernetes 的高层 API 设计，一定是以 Kubernetes 的业务为基础出发，也就是以系统调度管理容器的操作意图为基础设计\n\n**低层 API 根据高层 API 的控制需要设计**\n\n设计实现低层 API 的目的，是为了被高层 API 使用，考虑减少冗余、提高重用性的目的，低层 API的设计也要以需求为基础，要尽量抵抗受技术实现影响的诱惑。\n\n**尽量避免简单封装，不要有在外部 API 无法显式知道的内部隐藏的机制**\n\n简单的封装，实际没有提供新的功能，反而增加了对所封装 API 的依赖性。\n\n例如 StatefulSet 和 ReplicaSet，本来就是两种 Pod 集合，那么 Kubernetes 就用不同 API 对象来定义它们，而不会说只用同一个 ReplicaSet，内部通过特殊的算法再来区分这个 ReplicaSet 是有状态的还是无状态\n\n**API 操作复杂度与对象数量成正比**\n\nAPI 的操作复杂度不能超过 O(N)，否则系统就不具备水平伸缩性了。\n\n**API 对象状态不能依赖于网络连接状态**\n\n由于众所周知，在分布式环境下，网络连接断开是经常发生的事情，因此要保证 API 对象状态能应对网络的不稳定，API 对象的状态就不能依赖于网络连接状态。\n\n**尽量避免让操作机制依赖于全局状态**\n\n因为在分布式系统中要保证全局状态的同步是非常困难的。\n\n# Kubernetes 如何通过对象的组合完成业务描述\n\n![image.png](Assets/image_1665825319357_0.png)\n\n# 架构设计原则\n\n只有 APIServer 可以直接访问 etcd 存储，其他服务必须通过 Kubernetes API 来访问集群状态；\n\n单节点故障不应该影响集群的状态；\n\n在没有新请求的情况下，所有组件应该在故障恢复后继续执行上次最后收到的请求（比如网络分区或服务重启等）；\n\n所有组件都应该在内存中保持所需要的状态，APIServer 将状态写入 etcd 存储，而其他组件则通过 APIServer 更新并监听所有的变化；\n\n优先使用事件监听而不是轮询。\n\n# 引导（Bootstrapping）原则\n\n- Self-hosting 是目标。\n- 减少依赖，特别是稳态运行的依赖。\n- 通过分层的原则管理依赖。\n- 循环依赖问题的原则：\n\t- 同时还接受其他方式的数据输入（比如本地文件等），这样在其他服务不可用时还可以手动配置引导服务；\n\t- 状态应该是可恢复或可重新发现的；\n\t- 支持简单的启动临时实例来创建稳态运行所需要的状态，使用分布式锁或文件锁等来协调不同状态的切换（通常称为 pivoting 技术）；\n\t- 自动重启异常退出的服务，比如副本或者进程管理器等。\n\n# 核心技术概念和 API 对象\n\nAPI 对象是 Kubernetes 集群中的管理操作单元。\n\nKubernetes 集群系统每支持一项新功能，引入一项新技术，一定会新引入对应的 API 对象，支持对该功能的管理操作。\n\n每个 API 对象都有四大类属性：\n\n- TypeMeta\n- MetaData\n- Spec\n- Status\n\n## TypeMeta\n\nKubernetes对象的最基本定义，它通过引入GKV（Group，Kind，Version）模型定义了一个对象的类型。\n\n**1. Group**\n\nKubernetes 定义了非常多的对象，如何将这些对象进行归类是一门学问，**将对象依据其功能范围归入不同的分组，比如把支撑最基本功能的对象归入 core 组，把与应用部署有关的对象归入 apps 组，** 会使这些对象的可维护性和可理解性更高。\n\n**2. Kind**\n\n定义一个对象的基本类型，比如 Node、Pod、Deployment 等。\n\n**3. Version**\n\n社区每个季度会推出一个 Kubernetes 版本，随着 Kubernetes 版本的演进，对象从创建之初到能够完全生产化就绪的版本是不断变化的。与软件版本类似，通常社区提出一个模型定义以后，随着该对象不断成熟，其版本可能会从 v1alpha1 到 v1alpha2，或者到 v1beta1，最终变成生产就绪版本 v1。\n\n## MetaData\n\nMetadata 中有两个最重要的属性：**Namespace和Name**，分别定义了对象的Namespace 归属及名字，**这两个属性唯一定义了某个对象实例。**\n\n1. Label\n\n顾名思义就是给对象打标签，一个对象可以有任意对标签，其存在形式是键值对。Label 定义了对象的可识别属性，Kubernetes API 支持以 Label 作为过滤条件查询对象。\n\n2. Annotation\n\nAnnotation 与 Label 一样用键值对来定义，但 Annotation 是作为属性扩展，更多面向于系统管理员和开发人员，因此需要像其他属性一样做合理归类。\n\n3. Finalizer\n\nFinalizer 本质上是一个资源锁，Kubernetes 在接收某对象的删除请求时，会检查 Finalizer 是否为空，如果不为空则只对其做逻辑删除，即只会更新对象中的metadata.deletionTimestamp 字段。\n\n4. ResourceVersion\n\nResourceVersion 可以被看作一种乐观锁，每个对象在任意时刻都有其ResourceVersion，当 Kubernetes 对象被客户端读取以后，ResourceVersion信息也被一并读取。此机制确保了分布式系统中任意多线程能够无锁并发访问对象，极大提升了系统的整体效率。\n\n### Label\n\nLabel 是识别 Kubernetes 对象的标签，以 key/value 的方式附加到对象上。\n\nkey 最长不能超过 63 字节，value 可以为空，也可以是不超过 253 字节的字符串。\n\nLabel 不提供唯一性，并且实际上经常是很多对象（如 Pods）都使用相同的 label 来标志具体的应用。\n\nLabel 定义好后其他对象可以使用 Label Selector 来选择一组相同 label 的对象\n\n![image.png](Assets/image_1665835918391_0.png)\n\nLabel Selector 支持以下几种方式：\n\n- 等式，如 app=nginx 和 env!=production；\n- 集合，如 env in (production, qa)；\n- 多个 label（它们之间是 AND 关系），如 app=nginx,env=test。\n\n### Annotation\n\nAnnotations 是 key/value 形式附加于对象的注解。\n\n不同于 Labels 用于标志和选择对象，Annotations 则是用来记录一些附加信息，用来辅助应用部署、安全策略以及调度策略等。\n\n比如 deployment 使用 annotations 来记录 rolling update 的状态。\n\n## Spec 和 Status\n\nSpec 和 Status 才是对象的核心。\n\nSpec 是用户的期望状态，由创建对象的用户端来定义。\n\nStatus 是对象的实际状态，由对应的控制器收集实际状态并更新。\n\n与 TypeMeta 和 Metadata 等通用属性不同，Spec 和 Status 是每个对象独有的。\n\n## 常用 Kubernetes 对象及其分组\n\n![image.png](Assets/image_1665836531393_0.png)\n\n# 核心对象概览\n\n## Node\n\nNode 是 Pod 真正运行的主机，可以物理机，也可以是虚拟机。\n\n为了管理 Pod，每个 Node 节点上至少要运行 container runtime（比如 Docker 或者 Rkt）、Kubelet 和 Kube-proxy 服务。\n\n## Namespace\n\n**Namespace 是对一组资源和对象的抽象集合**，比如可以用来将系统内部的对象划分为不同的项目组或用户组。\n\n常见的 pods, services, replication controllers 和 deployments 等都是属于某一个 Namespace 的（默认是 default），而 Node, persistentVolumes等则不属于任何 Namespace。\n\n## 什么是 Pod\n\nPod 是一组紧密关联的容器集合，它们共享 PID、IPC、Network 和 UTS namespace，是 Kubernetes调度的基本单位。\n\nPod 的设计理念是支持多个容器在一个 Pod 中共享网络和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。\n\n同一个 Pod 中的不同容器可共享资源：\n\n- 共享网络 Namespace；\n- 可通过挂载存储卷共享存储；\n- 共享 Security Context。\n\n## 如何通过 Pod 对象定义支撑应用运行\n\n环境变量：\n\n- 直接设置值；\n- 读取 Pod Spec 的某些属性；\n- 从 ConfigMap 读取某个值；\n- 从 Secret 读取某个值。\n\n![image.png](Assets/image_1665837588972_0.png)\n\n## 存储卷\n\n通过存储卷可以将外挂存储挂载到 Pod 内部使用。\n\n存储卷定义包括两个部分: Volume 和 VolumeMounts。\n\n- Volume：定义 Pod 可以使用的存储卷来源；\n- VolumeMounts：定义存储卷如何 Mount 到容器内部。\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: hello-volume\nspec:\ncontainers:\n- image: nginx:1.15\nname: nginx\nvolumeMounts: - name: data\nmountPath: /data\nvolumes:\n- name: data\nemptyDir: {}\n```\n\n## Pod 网络\n\nPod的多个容器是共享网络 Namespace 的，这意味着：\n\n同一个 Pod 中的不同容器可以彼此通过 Loopback 地址访问：\n\n- 在第一个容器中起了一个服务 http://127.0.0.1 。\n- 在第二个容器内，是可以通过 httpGet http://127.0.0.1 访问到该地址的。\n\n这种方法常用于不同容器的互相协作。\n\n## 资源限制\n\nKubernetes 通过 Cgroups 提供容器资源管理的功能，可以限制每个容器的CPU 和内存使用，比如对于刚才创建的 deployment，可以通过下面的命令限制nginx 容器最多只用 50% 的 CPU 和 128MB 的内存：\n\n```bash\n$ kubectl set resources deployment nginx-app -c=nginx --\nlimits=cpu=500m,memory=128Mi\ndeployment \"nginx\" resource requirements updated\n```\n\n## 健康检查\n\nKubernetes 作为一个面向应用的集群管理工具，需要确保容器在部署后确实处在正常的运行状态。\n\n1. 探针类型：\n\t- LivenessProbe\n\t\t- 探测应用是否处于健康状态，如果不健康则删除并重新创建容器。\n\t- ReadinessProbe\n\t\t- 探测应用是否就绪并且处于正常服务状态，如果不正常则不会接收来自 Kubernetes Service 的流量。\n\t- StartupProbe\n\t\t- 探测应用是否启动完成，如果在 failureThreshold\\*periodSeconds 周期内未就绪，则会应用进程会被重启。\n1. 探活方式：\n\t- Exec\n\t- TCP socket\n\t- HTTP\n\n## Configmap\n\nConfigMap 用来将非机密性的数据保存到键值对中。\n\n使用时， Pods 可以将其用作环境变量、命令行参数或者存储卷中的配置文件。\n\nConfigMap 将环境配置信息和 容器镜像解耦，便于应用配置的修改\n\n## 密钥对象（Secret）\n\nSecret 是用来保存和传递密码、密钥、认证凭证这些敏感信息的对象。\n\n使用 Secret 的好处是可以避免把敏感信息明文写在配置文件里。\n\nKubernetes 集群中配置和使用服务不可避免的要用到各种敏感信息实现登录、认证等功能，例如访问 AWS 存储的用户名密码。\n\n为了避免将类似的敏感信息明文写在所有需要使用的配置文件中，可以将这些信息存入一个 Secret 对象，而在配置文件中通过 Secret 对象引用这些敏感信息。\n\n这种方式的好处包括：意图明确，避免重复，减少暴漏机会。\n\n## 用户（User Account）\u0026 服务帐户（Service Account）\n\n顾名思义，用户帐户为人提供账户标识，而服务账户为计算机进程和 Kubernetes 集群中运行的 Pod 提供账户标识。\n\n**用户帐户和服务帐户的一个区别是作用范围：**\n\n- 用户帐户对应的是人的身份，人的身份与服务的 Namespace 无关，所以用户账户是跨Namespace 的；\n- 而服务帐户对应的是一个运行中程序的身份，与特定 Namespace 是相关的。\n\n## Service\n\nService 是应用服务的抽象，通过 labels 为应用提供负载均衡和服务发现。匹配 labels 的 Pod IP 和端口列表组成 endpoints，由 Kube-proxy 负责将服务IP 负载均衡到这些 endpoints 上。\n\n每个 Service 都会自动分配一个 cluster IP（仅在集群内部可访问的虚拟地址）和 DNS 名，其他容器可以通过该地址或 DNS 来访问服务，而不需要了解后端容器的运行。\n\n![image.png](Assets/image_1665839447998_0.png)\n\n## 副本集（Replica Set）\n\nPod 只是单个应用实例的抽象，要构建高可用应用，通常需要构建多个同样的副本，提供同一个服务。\n\nKubernetes 为此抽象出副本集 ReplicaSet，其允许用户定义 Pod 的副本数，每一个 Pod 都会被当作一个无状态的成员进行管理，Kubernetes 保证总是有用户期望的数量的 Pod 正常运行。\n\n当某个副本宕机以后，控制器将会创建一个新的副本。\n\n当因业务负载发生变更而需要调整扩缩容时，可以方便地调整副本数量。\n\n## 部署（Deployment）\n\n部署表示用户对 Kubernetes 集群的一次更新操作。\n\n部署是一个比 RS 应用模式更广的 API 对象，可以是创建一个新的服务，更新一个新的服务，也可以是滚动升级一个服务。\n\n滚动升级一个服务，实际是创建一个新的 RS，然后逐渐将新 RS 中副本数增加到理想状态，将旧 RS 中的副本数减小到 0 的复合操作。\n\n这样一个复合操作用一个 RS 是不太好描述的，所以用一个更通用的 Deployment 来描述。\n\n以 Kubernetes 的发展方向，未来对所有长期伺服型的的业务的管理，都会通过 Deployment 来管理。\n\n![image.png](Assets/image_1665839670775_0.png)\n\n## 有状态服务集（StatefulSet）\n\n对于 StatefulSet 中的 Pod，每个 Pod 挂载自己独立的存储，如果一个 Pod 出现故障，从其他节点启动一个同样名字的Pod，要挂载上原来 Pod 的存储继续以它的状态提供服务。\n\n适合于 StatefulSet 的业务包括数据库服务 MySQL 和 PostgreSQL，集群化管理服务 ZooKeeper、etcd 等有状态服务。\n\n使用 StatefulSet，Pod 仍然可以通过漂移到不同节点提供高可用，而存储也可以通过外挂的存储来提供高可靠性，StatefulSet 做的只是将确定的 Pod 与确定的存储关联起来保证状态的连续性。\n\n![image.png](Assets/image_1665839742250_0.png)\n\n## Statefulset 与 Deployment 的差异\n\n- 身份标识\n\t- StatefulSet Controller 为每个 Pod 编号，序号从0开始。\n- 数据存储\n\t- StatefulSet 允许用户定义 volumeClaimTemplates，Pod 被创建的同时，Kubernetes 会以volumeClaimTemplates 中定义的模板创建存储卷，并挂载给 Pod。\n- StatefulSet 的升级策略不同\n\t- onDelete\n\t- 滚动升级\n\t- 分片升级\n\n## 任务（Job）\n\nJob 是 Kubernetes 用来控制批处理型任务的 API 对象。\n\nJob 管理的 Pod 根据用户的设置把任务成功完成后就自动退出。\n\n成功完成的标志根据不同的 spec.completions 策略而不同：\n\n- 单 Pod 型任务有一个 Pod 成功就标志完成；\n- 定数成功型任务保证有 N 个任务全部成功；\n- 工作队列型任务根据应用确认的全局成功而标志成功。\n\n## 后台支撑服务集（DaemonSet）\n\n长期伺服型和批处理型服务的核心在业务应用，可能有些节点运行多个同类业务的 Pod，有些节点上又没有这类 Pod 运行；\n\n而后台支撑型服务的核心关注点在 Kubernetes 集群中的节点（物理机或虚拟机），要保证每个节点上都有一个此类 Pod 运行。\n\n节点可能是所有集群节点也可能是通过 nodeSelector 选定的一些特定节点。\n\n典型的后台支撑型服务包括存储、日志和监控等在每个节点上支撑 Kubernetes 集群运行的服务。\n\n![image.png](Assets/image_1665839934475_0.png)\n\n## 存储 PV 和 PVC\n\nPersistentVolume（PV）是集群中的一块存储卷，可以由管理员手动设置，或当用户创建 PersistentVolumeClaim（PVC）时根据 StorageClass 动态设置。\n\nPV 和 PVC 与 Pod 生命周期无关。也就是说，当 Pod 中的容器重新启动、Pod 重新调度或者删除时，PV 和 PVC 不会受到影响，Pod 存储于 PV 里的数据得以保留。\n\n对于不同的使用场景，用户通常需要不同属性（例如性能、访问模式等）的 PV。\n\n## CustomResourceDefinition\n\nCRD 就像数据库的开放式表结构，允许用户自定义 Schema。\n\n有了这种开放式设计，用户可以基于 CRD 定义一切需要的模型，满足不同业务的需求。\n\n社区鼓励基于 CRD 的业务抽象，众多主流的扩展应用都是基于 CRD 构建的，比如 Istio、Knative。\n\n甚至基于 CRD 推出了 Operator Mode 和 Operator SDK，可以以极低的开发成本定义新对象，并构建新对象的控制器。\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Pod%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F":{"title":"深入理解Pod的生命周期","content":"\n# 如何优雅的管理Pod的完整生命周期\n\n![image.png](Assets/image_1666168768853_0.png)\n\n# Pod状态机\n\n![image.png](Assets/image_1666168798844_0.png)\n\n# Pod Phase\n\n- Pod Phase\n\t- Pending\n\t- Running\n\t- Succeeded\n\t- Failed\n\t- Unknown\n- kubectl get pod显示的状态信息是由podstatus 的conditions和phase计算出来的\n- 查看pod细节\n\t- `kubectl get pod $podname-oyaml`\n- 查看pod相关事件\n\t- `kubectl describe pod`\n\n![image.png](Assets/image_1666169145175_0.png)\n\n# Pod状态计算细节\n\n![image.png](Assets/image_1666169178690_0.png)\n\n# 如何确保Pod的高可用\n\n避免容器进程被终止避免Pod被驱逐\n\n- 设置合理的 resources.memory limits 防止容器进程被OOMKill\n- 设置合理的 emptydir.sizeLimit并且确保数据写入不超过emptyDir的限制，防止Pod被驱逐\n\n# Pod的QoS分类\n\n**\\[QoS\u0026Quota\\] Guaranteed, Burstable and BestEffort**\n\n当计算节点检测到内存压力时，Kubernetes会按BestEffort-\u003eBurstable-\u003eGuaranteed的顺序依次驱逐Pod\n\n![image.png](Assets/image_1666169701618_0.png)\n\n![image.png](Assets/image_1666169722428_0.png)\n\n```bash\n$ kubectl get pod test-vfgg6-oyaml lgrep qosClass\nqosClass: Burstable\n```\n\n# 质量保证Guaranteed，Burstable and BestEffort\n\n定义Guaranteed类型的资源需求来保护你的重要Pod。\n\n认真考量Pod需要的真实需求并设置Limit和resource，这有利干将集群资源利用率控制在合理范围并减少Pod被驱逐的现象。\n\n尽量避免将生产Pod设置为BestEffort，但是对测试环境来讲，BestEffort Pod能确保大多数应用不会因为资源不足而处于Pending状态。\n\nBurstable适用于大多数场景。思考∶为什么?\n\n# 基于Taint的Evictions\n\n![image.png](Assets/image_1666170094232_0.png)\n\n# 健康检查探针\n\n- 健康探针类型分为\n\t- livenessProbe\n\t\t- 探活，当检查失败时，意味着该应用进程已经无法正常提供服务，kubelet会终止该容器进程并按照restartPolicy决定是否重启\n\t- readinessProbe\n\t\t- 就绪状态检查，当检查失败时，意味着应用进程正在运行，但因为某些原因不能提供服务，Pod状态会被标记为NotReady\n\t- startupProbe\n\t\t- 在初始化阶段（Ready之前）进行的健康检查，通常用来避免过于频繁的监测影响应用启动\n- 探测方法包括\n\t- ExecAction：在容器内部运行指定命令，当返回码为0时，探测结果为成功\n\t- TCPSocketAction：由kubelet起，通过TCP协议检查容器IP和端口，当端口可达时，探测结果为成功\n\t- HTTPGetAction：由kubelet发起，对Pod的IP和指定端口以及路径进行HTTPGet操作，当返回码为200-400之间时，探测结果为成功\n\n# 探针属性\n\n![image.png](Assets/image_1666170883220_0.png)\n\n# ReadinessGates\n\nReadiness允许在Kubernetes自带的Pod Conditions之外引入自定义的就绪条件\n\n新引入的readinessGates condition需要为True状态后，加上内置的Conditions，Pod才可以为就绪状态\n\n该状态应该由某控制器修改\n\n# Post-start和Pre-Stop Hook\n\n![image.png](Assets/image_1666174698494_0.png)\n\n# terminationGracePeriodSeconds的分解\n\n![image.png](Assets/image_1666174728593_0.png)\n\n# Terminating Pod的误用\n\nbash/sh 会忽略SIGTERM信号量，因此kill-SIGTERM会永远超时，若应用使用bash/sh作为Entrypoint，则应避免过长的grace period\n\n![image.png](Assets/image_1666174795919_0.png)\n\n# Terminating Pod的经验分享\n\n- terminationGracePeriodSeconds默认时长30秒\n- 如果不关心Pod的终止时长，那么无需采取特殊措施\n- 如果希望快速终止应用进程，那么可采取如下方案\n\t- 在preStop Script中主动退出进程\n\t- 在主容器进程中使用特定的初始化进程\n- 优雅的初始化进程应该\n\t- 正确处理系统信号量，将信号量转发给子进程\n\t- 在主进程退出之前，需要先等待并确保所有子进程退出\n\t- 监控并清理孤儿子进程\n\n# 存储带来的挑战\n\n- 多容器之间共享存储，最简方案是emptyDir\n- 带来的挑战：\n\t- emptyDir需要控制 size limt，否则无限扩张的应用会撑爆主机磁盘导致主机不可用，进而导致大规模集群故障\n\t- emptyDir size limit生效以后，kubelet会定期对容器目录执行du操作，会导致些许的性能影响\n\t- size limit达到以后，Pod会被驱逐，原Pod的日志配置等信息会消失\n\n# 应用配置\n\n- 传入方式\n\t- Environment Variables\n\t- Volume Mount\n- 数据来源\n\t- ConfigMap\n\t- Secret\n\t- Downward API\n\n# 数据应该如何保存\n\n![image.png](Assets/image_1666175473162_0.png)\n\n![image.png](Assets/image_1666175513153_0.png)\n\n# 容器应用可能面临的进程中断\n\n![image.png](Assets/image_1666176184774_0.png)\n\n# 高可用部署方式\n\n- 多少实例\n- 更新策略\n\t- maxSurge\n\t- maxUnavailable（需要考虑ResourceQuota的限制\n- 深入理解PodTemplateHash导致的应用的易变性\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/%E7%94%9F%E4%BA%A7%E5%8C%96%E8%BF%90%E7%BB%B4":{"title":"生产化运维","content":"\n# 镜像仓库\n\n## 镜像仓库\n\n- 镜像仓库（Docker Registry）负责存储、管理和分发镜像。\n- 镜像仓库管理多个Repository，Repository通过命名来区分。每个Repository包含一个或多个镜像，镜像通过镜像名称和标签（Tag）来区分。\n- 客户端拉取镜像时，要指定三要素：\n\t- 镜像仓库：要从哪一个镜像仓库拉取镜像，通常通过DNS或IP地址来确定一个镜像仓库，如hub.docker.com\n\t- Repository：组织名，如cncamp\n\t- 镜像名称+标签：如nginx∶latest\n\n![image.png](Assets/image_1667053630959_0.png)\n\n---\n\n**公有镜像仓库优势**\n\n开放：任何开发者都可以上传、分享镜像到公有镜像仓库中。\n\n便捷：开发者可以非常方便地搜索、拉取其他开发者的镜像，避免重复造轮子。\n\n免运维：开发者只需要关注应用开发，不必关心镜像仓库的更新、升级、维护等。\n\n成本低：企业或开发者不需要购买硬件、解决方案来搭建镜像仓库，也不需要团队来维护。\n\n**私有镜像仓库优势**\n\n隐私性：企业的代码和数据是企业的私有资产，不允许随意共享到公共平台。\n\n敏感性：企业的镜像会包含一些敏感信息，如密钥信息、令牌信息等。这些敏感信息严禁暴露到企业外部。\n\n网络连通性：企业的网络结构多种多样，并非所有环境都可以访问互联网。\n\n安全性：而在企业环境中，若使用一些含有漏洞的依赖包，则会引入安全隐患。\n\n## 镜像仓库遵循OCI的Distribution Spec\n\n![image.png](Assets/image_1667053850649_0.png)\n\n## 数据和块文件\n\n镜像由元数据和块文件两部分组成，镜像仓库的核心职能就是管理这两项数据。\n\n- 元数据\n\t- 元数据用于描述一个镜像的核心信息，包含镜像的镜像仓库、仓库、标签、校验码、文件层，镜像构建描述等信息。\n\t- 通过这些信息，可以从抽象层面完整地描述一个镜像∶它是如何构建出来的、运行过什么构建命令、构建的每一个文件层的校验码、打的标签、镜像的校验码等。\n- 块文件（blob）\n\t- 块文件是组成镜像的联合文件层的实体，每一个块文件是一个文件层，内部包含对应文件层的变更。\n\n![image.png](Assets/image_1667053866547_0.png)\n\n## Harbor\n\nHarbor是VMware开源的企业级镜像仓库，目前已是CNCF的毕业项目。它拥有完整的仓库管理、镜像管理、基于角色的权限控制、镜像安全扫描集成、镜像签名等。\n\n![image.png](Assets/image_1667054088588_0.png)\n\n### Harbor提供的服务\n\nHarbor核心服务：提供Harbor的核心管理服务API，包括仓库管理，认证管理，授权管理，配置管理、项目管理、配额管理、签名管理、副本管理等。\n\nHarbor Portal：Harbor的Web界面。\n\nRegistry：Registry负责接收客户端的pull/push请求，其核心为docker/Distribution。\n\n副本控制器：Harbor可以以主从模式来部署镜像仓库，副本控制器将镜像从主镜像服务分发到从镜像服务。\n\n日志收集器：收集各模块的日志。\n\n垃圾回收控制器：回收日常操作中删除镜像记录后遗留在块存储中的孤立块文件。\n\n### Harbor架构\n\n![image.png](Assets/image_1667054290568_0.png)\n\n### Harbor高可用架构\n\n![image.png](Assets/image_1667054764792_0.png)\n\n## 本地镜像加速Dragonfly\n\n- Dragonfly是一款基于P2P的智能镜像和文件分发工具\n- 它旨在提高文件传输的效率和速率，最大限度地利用网络带宽，尤其是在分发大量数据时\n\t- 应用分发\n\t- 缓存分发\n\t- 日志分发\n\t- 镜像分发\n\n### 优势\n\n- 基于P2P的文件分发\n- 非侵入式支持所有类型的容器技术\n- 机器级别的限速\n- 被动式CDN\n- 高度一致性\n- 磁盘保护和高效 IO\n- 高性能\n- 自动隔离异常\n- 对文件源无压力\n- 支持标准 HTTP 头文件\n- 有效的 Registry鉴权并发控制\n- 简单易用\n\n### 镜像下载流程\n\ndfget proxy也称为 dfdaemon，会拦截来自 docker pull或docker push的HTTP请求，然后使用 dfget 来处理那些跟镜像分层相关的请求\n\n![image.png](Assets/image_1667055180018_0.png)\n\n每个文件会被分成多个分块，并在对等节点之间传输\n\n![image.png](Assets/image_1667055195716_0.png)\n\n# 镜像安全\n\n## 镜像安全的最佳实践\n\n- 构建指令问题\n\t- 避免在构建竟像时，添加密钥，Token等敏感信息（配置与代码应分离）\n- 应用依赖问题\n\t- 应尽量避免安装不必要的依赖\n\t- 确保依赖无安全风险，一些长时间不更新的基础镜像的可能面临安全风险，比如基于openssl1.0，只支持tls1.0等\n- 文件问题\n\t- 在构建镜像时，除应用本身外，还会添加应用需要的配置文件、模板等，在添加这些文件时，会无意间添加一些包含敏感信息或不符合安全策略的文件到镜像中。\n\t- 当镜像中存在文件问题时，需要通过引入该文件的构建指令行进行修复，而不是通过追加一条删除指令来修复。\n\n![image.png](Assets/image_1667055854221_0.png)\n\n## 镜像扫描（Vulnerability Scanning）\n\n镜像扫描通过扫描工具或扫描服务对镜像进行扫描，来确定镜像是否安全\n\n- 分析构建指令、应用、文件、依赖包\n- 查询CVE库、安全策略\n- 检测镜像是否安全，是否符合企业的安全标准。\n\n## 镜像策略准入控制\n\n镜像准入控制是在部署Pod、更新Pod时，对Pod 中的所有镜像进行安全验证以放行或拦截对Pod的操作：\n\n- 放行：Pod中所有的镜像都安全，允许此次的操作，Pod成功被创建或更新。\n- 拦截：Pod中的镜像未扫描，或已经扫描但存在安全漏洞，或不符合安全策略，Pod无法被创建或更新。\n\n![image.png](Assets/image_1667056069430_0.png)\n\n## 扫描镜像\n\n1. 镜像扫描服务从镜像仓库拉取镜像。\n2. 解析镜像的元数据。\n3. 解压镜像的每一个文件层。\n4. 提取每一层所包含的依赖包、可运行程序、文件列表、文件内容扫描。\n5. 将扫描结果与CVE字典、安全策略字典进行匹配，以确认最终镜像是否安全。\n\n## 镜像扫描服务\n\n![image.png](Assets/image_1667056153240_0.png)\n\n# 基于Kubernetes的DevOps\n\n## 传统运维模式\n\n- 缺乏一致性环境\n- 平台与应用部署相互割裂\n- 缺乏工具链支持\n- 缺乏统一的灰度发布管理\n- 缺乏统一监控能力和持续运维能力\n\n![image.png](Assets/image_1667058218102_0.png)\n\n## 建立持续交付的服务体系\n\n传统的开发运维模式下，存在的问题：\n\n- 从需求到版本上线中间是个黑箱子，风险不可控；\n- 开发设计时未过多考虑运维，导致后续部署及维护的困难；\n- 开发各自为政，烟囱式开发，未考虑共享重用、联调，开发的资产积累不能快速交移到运维手中；\n\n应对这样的问题，我们通常倡导的解决之道是：运维前移，统一运维，建立持续交付服务体系。\n\n## 基于Docker的开发模式驱动持续集成\n\n![image.png](Assets/image_1667058564537_0.png)\n\n## DevOps流程定义\n\n![image.png](Assets/image_1667058593881_0.png)\n\n## Dev和Ops的边界定义\n\nProgramming vs. Engineering\n\nProgramming更多的是系统设计和编码实现\n\nEngineering包含更大范围概念，除了功能层面的实现，还需为运维服务\n\n## 定义production readiness\n\nFunction ready vs. production ready\n- Function Ready只是交付的软件产品从功能层面满足需求定义\n- Production Ready除了功能就绪还包含\n\t- LnP测试通过，满足性能需求\n\t- 用户手册完成，用户可按照用户手册使用既定功能\n\t- 管理手册完成，运维人员可以依照管理手册部署，升级产品并解决现网问题\n\t- 监控，包括\n\t\t- 组件健康状态检查（UP）\n\t\t- 性能指标（Metrics）\n\t\t- 基于性能指标，定义alert rule，在系统故障或缓慢时，发送告警信息给运维人员\n\t\t- Assertion，定期测试某功能并检查结果，比如每小时创建service，测试vip连通性\n\n## 单体架构下的人员配置\n\n![image.png](Assets/image_1667059342424_0.png)\n\n## 微服务架构下的人员配置\n\n![image.png](Assets/image_1667059375806_0.png)\n\n## DevOps下的人员划分\n\n![image.png](Assets/image_1667059434125_0.png)\n\n## 此组织结构的优缺点\n\n### 优势\n\n- 一个架构师负责整个产品的规划，使得产品更规范，产品进化不同的Program由不同PO负责端到端，从需求到生产系统部署，保证solution质量\n- 研发和运营统一，一个最大的作用是研发可以深刻理解现网痛苦，对功能需求的设计，和优先级定义有极大帮助。\n- Function ready vs. production ready\n- Programming vs. Engineering\n- 更具连续性\n\n### 问题\n\n- 传统运营和开发的冲突并非消失，而是转变为了运营经理和架构以及PO之间的冲突。\n- 运营轮值导致生产系统权责不明确\n- 轮值期间期待不出故障\n- 对于出现的故障缺少端对端的跟踪，可能问题还没处理结束，已经要交接给下一个人了\n- 对于生产系统的积累问题，如故障节点，无人专职处理，导致比较多的故障节点堆积\n- 对于生产系统的配置，无统一规划，每个人可能用不同的配置达到相同的目的\n- Dev自主权过高导致Dev可以偷偷加功能并部署到生产\n\n## 我眼中理想的DevOps\n\nDev和Ops需要责权划分，可以有overlap，同时做部署，同时做计划，但Dev应侧重功能开发，Ops偏重生产系统运维\n\nOps参与到版本规划流程中，并为一个功能能不能release和deploy把关\n\n- Dev\n\t- Plan\n\t- Code\n\t- Build\n\t- Test\n\t- Release\n\t- Deploy\n- Ops\n\t- Deploy\n\t- Operate\n\t- Monitor\n\t- Plan\n\n![image.png](Assets/image_1667060313830_0.png)\n\n## DevOps流程概览\n\n![image.png](Assets/image_1667060846273_0.png)\n\n## 代码分支管理\n\n![image.png](Assets/image_1667060900749_0.png)\n\n## 持续集成\n\n![image.png](Assets/image_1667061198936_0.png)\n\n## 持续部署\n\n![image.png](Assets/image_1667061220025_0.png)\n\n## Gitops\n\n![image.png](Assets/image_1667061243780_0.png)\n\n# 基于Jenkins的自动化流水线\n\n## Kubernetes CI\u0026CD完整流程\n\n![image.png](Assets/image_1667115885998_0.png)\n\n# Tekton\n\n## Jenkins的不足\n\n- 基于脚本的Job配置复用率不足\n\t- Jenkins等工具的流水线作业通常基于大量不可复用的脚本语言，如何提高代码复用率\n- 代码调试困难\n\t- 如何让流水线作业的配置更好地适应云原生场景的需求越来越急迫\n\n## 基于声明式API的流水线-Tekton\n\n自定义：Tekton对象是高度自定义的，可扩展性极强。平台工程师可预定义可重用模块以详细的模块目录提供，开发人员可在其他项目中直接引用。\n\n可重用：Tekton对象的可重用性强，组件只需—次定义，即可被组织内的任何人在任何流水线都可重用。使得开发人员无需重复造轮子即可构建复杂流水线。\n\n可扩展性：Tekton组件目录（Tekton Catalog）是一个社区驱动的Tekton组件的存储仓库。任何用户可以直接从社区获取成熟的组件并在此之上构建复杂流水线，也就是当你要构建一个流水线时，很可能你需要的所有代码和配置都可以从Tekton Catalog直接拿下来复用，而无需重复开发。\n\n标准化：Tekton作为Kubernetes集群的扩展安装和运行，并使用业界公认的Kubernetes资源模型；Tekton作业以Kubernetes容器形态执行。\n\n规模化支持：只需增加Kubernetes节点，即可增加作业处理能力。Tekton的能力可依照集群规模随意扩充，无需重新定义资源分配需求或者重新定义流水线。\n\n## Tekton核心组件\n\nPipeline：对象定义了一个流水线作业，一个Pipeline对象由一个或数个Task对象组成。\n\nTask：一个可独立运行的任务，如获取代码，编译，或者推送镜像等等，当流水线被运行时，Kubernetes会为每个Task创建一个Pod。一个Task由多个Step组成，每个Step体现为这个Pod中的一个容器。\n\n![image.png](Assets/image_1667119295352_0.png)\n\n## 输入输出资源\n\nPipeline和Task对象可以接收git reposity，pull request等资源作为输入，可以将Image，Kubernetes Cluster，Storage，CloudEvent等对象作为输出\n\n![image.png](Assets/image_1667119345327_0.png)\n\n## 事件触发的自动化流水线\n\n![image.png](Assets/image_1667120405330_0.png)\n\n## EventListener\n\nEventListener:\n\n事件监听器，该对象核心属性是interceptors拦截器，该拦截器可监听多种类型的事件，比如监听来自GitLab的Push事件。\n\n当该EventListener对象被创建以后，Tekton 控制器会为该EventListener创建Kubernetes Pod和Service，并启动一个HTTP服务以监听Push事件。\n\n当用户在GitLab项目中设置webhook并填写该EventListener的服务地址以后，任何人针对被管理项目发起的Push操作，都会被EventListener捕获。\n\n# Argocd\n\n## Argocd\n\nArgo CD是用于Kubernetes的声明性GitOps连续交付工具。\n\n**为什么选择Argo CD?**\n\n应用程序定义，配置和环境应为声明性的，并受版本控制。\n\n应用程序部署和生命周期管理应该是自动化的，可审核的且易于理解的。\n\n## argo cd 架构\n\nArgo CD被实现为kubernetes控制器，该控制器连续监视正在运行的应用程序，并将当前的活动状态与所需的目标状态（在Git存储库中指定）进行比较。\n\n其活动状态偏离目标状态的已部署应用程序被标记为Outofsync。\n\nArgo CD报告并可视化差异，同时提供了自动或手动将实时状态同步回所需目标状态的功能。\n\n在Git存储库中对所需目标状态所做的任何修改都可以自动应用并反映在指定的目标环境中。\n\n![image.png](Assets/image_1667121172178_0.png)\n\n## argocd的适用场景\n\n1. 低成本的Gitops利器\n2. 多集群管理\n\t1. 不同目的集群∶测试，集成，预生产，生产\n\t2. 多生产集群管理\n\n# 监控和日志\n\n## 数据系统构建\n\n日志收集与分析\n\n监控系统构建\n\n## 日志系统的价值\n\n分布式系统的日志查看比较复杂，因为多对节点的系统，要先找到正确的节点，才能看到想看的日志。日志系统把整个集群的日志汇总在一起，方便查看\n\n因为节点上的日志滚动机制，如果有应用打印太多日志，如果没有日志系统，会导致关键日志 •丢失。\n\n日志系统的重要意义在于解决，节点出错导致不可访问，进而丢失日志的情况。\n\n## 常用数据系统构建模式\n\n![image.png](Assets/image_1667134573485_0.png)\n\n## 日志收集系统Loki\n\nGrafana Loki是可以组成功能齐全的日志记录堆栈的一组组件。\n\n- 与其他日志记录系统不同，Loki是基于仅索引有关日志的元数据的想法而构建的：标签。\n- 日志数据本身被压缩并存储在对象存储（例如S3或GCS）中的块中，甚至存储在文件系统本地。\n- 小索引和高度压缩的块简化了操作，并大大降低了Loki的成本。\n\n## 基于Loki的日志收集系统\n\n![image.png](Assets/image_1667135054996_0.png)\n\n## Loki-stack子系统\n\n- Promtail\n\t- 将容器日志发送到 Loki或者Grafana 服务上的日志收集工具\n\t- 发现采集目标以及给日志流添加上Label，然后发送给Loki\n\t- Promtail 的服务发现是基于Prometheus 的服务发现机制实现的，可以查看configmap loki-promtail了解细节\n- Loki\n\t- Loki是可以水平扩展、高可用以及支持多租户的日志聚合系统\n\t- 使用和 Prometheus 相同的服务发现机制，将标签添加到日志流中而不是构建全文索引\n\t- Promtail 接收到的日志和应用的 metrics指标就具有相同的标签集\n- Grafana\n\t- Grafana 是一个用于监控和可视化观测的开源平台，支持非常丰富的数据源\n\t- 在 Loki技术栈中它专门用来展示来自 Prometheus和 Loki 等数据源的时间序列数据\n\t- 允许进行查询、可视化、报警等操作，可以用于创建、探索和共享数据 Dashboard\n\n## Loki架构\n\n![image.png](Assets/image_1667135213971_0.png)\n\n## Loki组件\n\n- Distributor（分配器）\n\t- 分配器服务负责处理客户端写入的日志。\n\t- 一旦分配器接收到日志数据，它就会把它们分成若干批次，并将它们并行地发送到多个采集器去。\n\t- 分配器通过 gRPC和采集器进行通信。\n\t- 它们是无状态的， 基于一致性哈希，我们可以根据实际需要对他们进行扩缩容。\n- Ingester（采集器）\n\t- 采集器服务负责将日志数据写入长期存储的后端（DynamoDB、S3、Cassandra 等等）。\n\t- 采集器会校验采集的日志是否乱序。\n\t- 采集器验证接收到的日志行是按照时间戳递增的顺序接收的，否则日志行将被拒绝并返回错误。\n- Querier（查询器）\n\t- 查询器服务负责处理 LogQL 查询语句来评估存储在长期存储中的日志数据。\n\n## 在生产中的问题\n\n存在的问题\n\n- 利用率低\n\t- 日志大多数目的是给管理员做问题分析用的，但管理员更多的是登陆到节点或者pod里做分析，因为日志分析只是整个分析过程中的一部分，所以很多时候顺手就把日志看了\n- Beats出现过锁住文件系统，docker container无法删除的情况\n- 与监控系统相比，日志系统的重要度稍低\n- 出现过多次因为日志滚动太快而使得日志收集占用太大网络带宽的情况\n\n## 监控系统\n\n- 为什么监控，监控什么内容\n\t- 对自己系统的运行状态了如指掌，有问题及时发现，而不让用户先发现我们系统不能使用。\n\t- 我们也需要知道我们的服务运行情况。例如，slowsql处于什么水平，平均响应时间超过200ms的占比有百分之多少\n- 我们为什么需要监控我们的服务\n\t- 需要监控工具来提醒我服务出现了故障，比如通过监控服务的负载来决定扩容或缩容。如果机器普遍负载不高，则可以考虑是否缩减一下机器规模，如果数据库连接经常维持在一个高位水平，则可以考虑一下是否可以进行拆库处理，优化一下架构。\n\t- 监控还可以帮助进行内部统制，尤其是对安全比较敏感的行业，比如证券银行等。比如服务器受到攻击时，我们需要分析事件，找到根本原因，识别类似攻击，发现没有发现的被攻击的系统，甚至完成取证等工作。\n- 监控目的\n\t- 减少宕机时间\n\t- 扩展和性能管理\n\t- 资源计划\n\t- 识别异常事件\n\t- 故障排除、分析\n\n## 在Kubernetes集群中的监控系统\n\n![image.png](Assets/image_1667136576658_0.png)\n\n## Prometheus中的指标类型\n\n- Counter （计数器器）\n\t- Counter 类型代表一种样本数据单调递增的指标，即只增不减，除非监控系统发生了重置。\n- Gauge（仪表盘）\n\t- Guage 类型代表一种样本数据可以任意变化的指标，即可增可减。\n- Histogram（直方图）\n\t- Histogram 在一段时间范围内对数据进行采样（通常是请求持续时间或响应大小等），并将其计入可配置的存储桶（bucket）中，后续可通过指定区间筛选样本，也可以统计样本总数，最后一般将数据展示为直方图\n\t- 样本的值分布在 bucket 中的数量，命名为 `\u003cbasename\u003e_bucket{le=\"\u003c上边界\u003e\"}`\n\t- 所有样本值的大小总和，命名为 `\u003cbasename\u003e_sum`\n\t- 样本总数，命名为 `\u003cbasename\u003e_count`。值和 `\u003cbasename\u003e_bucket{le=\"+Inf\"}` 相同\n- Summary（摘要）\n\t- 与 Histogram 类型类似，用于表示一段时间内的数据采样结果（通常是请求持续时间或响应大小等），但它直接存储了分位数（通过客户端计算，然后展示出来），而不是通过区间来计算\n\t- 它们都包含了 `\u003cbasename\u003e_sum` 和 `\u003cbasename\u003e_count` 指标\n\t- Histogram 需要通过 `\u003cbasename\u003e_bucket` 来计算分位数，而 Summary 则直接存储了分位数的值。\n\n## Prometheus Query Language\n\n- histogram_quantile(0.95, sum(rate(httpserver_execution_latency_seconds_bucket\\[5m\\])) by (le))\n- Histogram是直方图， httpserver_execution_latency_seconds_bucke是直方图指标，是将httpserver处理请求的时间放入不同的桶内，其表达的是落在不同时长区间的响应次数\n- by (le)，是将采集的数据按桶的上边界分组\n- rate(httpserver_execution_latency_seconds_bucket\\[5m\\])，计算的是五分钟内的变化率\n- Sum()，是将所有指标的变化率总计\n- 0.95，是取95分位\n- 综上：\n\t- 上述表达式计算的是httpserver处理请求时，95%的请求在五分钟内，在不同响应时间区间的处理的数量的变化情况\n\n## 构建支撑生产的监控系统\n\n- Metrics\n\t- 收集数据\n- Alert\n\t- 创建告警规则，如果告警规则被触发，则按不同眼中程度来告警\n- Assertion\n\t- 以一定时间间隔，模拟客户行为，操作kubernetes对象，并断言成功，如果不成功则按眼中程度告警\n\n## 来自生产系统的经验分享\n\n- Prometheus需要大内存和存储\n\t- 最初prometheus经常发生OOM kill\n\t- 在提高指定的资源以后，如果发生crash或者重启，prometheus需要30分钟以上的时间来读取数据进行初始化\n- Prometheus是运营生产系统过程中最重要的模块\n\t- 如果prometheus down机，则没有任何数据和告警，管理员两眼一黑，什么都不知道了\n\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%9A%84%E7%AE%A1%E7%90%86":{"title":"生产化集群的管理","content":"\n# 计算节点\n\n## 生产化集群的考量\n\n- 计算节点：\n\t- 如何批量安装和升级计算节点的操作系统?\n\t- 如何管理配置计算节点的网络信息?\n\t- 如何管理不同SKU（StockKeeping Unit）的计算节点?\n\t- 如何快速下架故障的计算节点?\n\t- 如何快速扩缩集群的规模?\n- 控制平面：\n\t- 如何在主节点上下载、安装和升级控制平面组件及其所需的配置文件?\n\t- 如何确保集群所需的其他插件，例如CoreDNS、监控系统等部署完成?\n\t- 如何准备控制平面组件的各种安全证书?\n\t- 如何快速升级或回滚控制平面组件的版本?\n\n# 操作系统的选择\n\n## 操作系统的评估与选择\n\n- 通用操作系统\n\t- Ubuntu\n\t- Centos\n\t- Fedora\n- 专为容器优化的操作系统\n\t- 最小化操作系统\n\t\t- CoreOS\n\t\t- RedHat Atomic\n\t\t- Snappy Ubuntu Core\n\t\t- RancherOS\n\n## 操作系统的评估与选择\n\n操作系统评估和选型的标准\n\n- 是否有生态系统\n- 成熟度\n- 内核版本\n- 对运行时的支持\n- Init System\n- 包管理和系统升级\n- 安全\n\n## 生态系统与成熟度\n\n容器优化操作系统的优势\n\n- 小\n- 原子级升级和回退\n- 更高的安全性\n\n![image.png](Assets/image_1666270636230_0.png)\n\n## 云原生的原则\n\n可变基础设施的风险\n\n- 在灾难发生的时候，难以重新构建服务。持续过多的手工操作，缺乏记录，会导致很难由标准初始化后的服务器来重新构建起等效的服务。\n- 在服务运行过程中，持续的修改服务器，就犹如程序中的可变变量的值发生变化而引入的状态不一致的并发风险。这些对于服务器的修改，同样会引入中间状态，从而导致不可预知的问题。\n\n不可变基础设施（immutable infrastructure）\n\n- 不可变的容器镜像\n- 不可变的主机操作系统\n\n## Atomic\n\n- 由Red Hat支持的软件包安装系统\n- 多种Distro\n\t- Fedora\n\t- CentOS\n\t- RHEL\n- 优势\n\t- 不可变操作系统，面向容器优化的基础设施\n\t\t- 灵活和安全性较好\n\t\t- 只有/etc和/var可以修改，其他目录均为只读\n\t- 基于rpm-ostree管理系统包\n\t\t- rpm-ostree是一个开源项目，使得生产系统中构建镜像非常简单\n\t\t- 支持操作系统升级和回滚的原子操作\n\n## 最小化主机操作系统\n\n- 原则：\n- 最小化主机操作系统\n- 只安装必要的工具\n\t- 必要：支持系统运行的最小工具集\n\t- 任何调试工具，比如性能排查，网络排查工具，均可以后期以容器形式运行\n- 意义\n\t- 性能\n\t- 稳定性\n\t- 安全保障\n\n## 操作系统构建流程\n\n![image.png](Assets/image_1666271335385_0.png)\n\n## ostree\n\n提供一个共享库（libostree）和一些列命令行\n\n提供与git命令行一致的体验，可以提交或者下载一个完整的可启动的文件系统树\n\n提供将ostree部署进bootloader的机制\n\n```bash\nhttps://github.com/ostreedev/ostree/blob/main/src/boot/dracut/module-setup.sh\ninstall() {\n\tdracut_install /usr/lib/ostree/ostree-prepare-root\n\tinst simple \"$systemdsystemunitdir}/ostree-prepare-root.service\"\n\tmkdir-p\"${initdir}${systemdsystemconfdir}/initrd-root-fs.target.wants\"\n\tln_r\"${systemdsystemunitdir}/ostree-prepare-root.service\"\\\n\t\t\"$systemdsystemconfdiry/initrd-root-fs.target.wants/ostree-prepare-root.service\"\n}\n```\n\n## 构建ostree\n\n- rpm-ostree\n\t- 基于treefile将rpm包构建成为ostree 管理ostree以及bootloader配置\n- treefile\n\t- refer：分支名（版本，cpu架构）\n\t- repo:：rpm package repositories\n\t- packages：待安装组件\n- 将rpm构建成ostree\n\t- rpm-ostree compose tree --unified-core --cachedir=cache--repo=./build-repo /path/to/treefile.json\n\n![image.png](Assets/image_1666271634113_0.png)\n\n## 加载ostree\n\n初始化项目\n\nostree admin os-init centos-atomic-host\n\n导入ostree repo\n\nostree remote add atomic http://ostree.svr/ostree\n\n![image.png](Assets/image_1666271736326_0.png)\n\n拉取ostree\n\nostree pull atomic centos-atomic-host/8/x86_64/standard\n\n部署os\n\nostree admin deploy--os=centos-atomic-host centos-atomic-host/8/x86_64/standard--karg='root=/dev/atomicos/root'\n\n## 操作系统加载\n\n- 物理机\n\t- 物理机通常需要通过foreman启动，foreman通过pxe boot，并加载kickstart\n\t- kickstart通过ostree deploy即可完成操作系统的部署\n- 虚拟机\n\t- 需要通过镜像工具将ostree构建成qcow2格式，vhd，raw等模式\n\n# 节点资源管理\n\n## NUMA Node\n\nNon-Uniform Memory Access是一种内存访问方式，是为多处理器计算机设计的内存架构\n\n![image.png](Assets/image_1666276412414_0.png)\n\n## 节点资源管理\n\n- 状态汇报\n- 资源预留\n- 防止节点资源耗尽的防御机制驱逐\n- 容器和系统资源的配置\n\n## 状态上报\n\nkubelet周期性地向API Server进行汇报，并更新节点的相关健康和资源使用信息\n\n- 节点基础信息，包括IP地址、操作系统、内核、运行时、kubelet、kube-proxy版本信息。\n- 节点资源信息包括CPU、内存、HugePage、临时存储、GPU等注册设备，以及这些资源中可以分配给容器使用的部分。\n- 调度器在为Pod选择节点时会将机器的状态信息作为依据。\n\n![image.png](Assets/image_1666276786048_0.png)\n\n## Lease\n\n在早期版本kubelet的状态上报直接更新node对象，而上报的信息包含状态信息和资源信息，因此需要传输的数据包较大，给APIServer和etcd造成的压力较大。\n\n后引入Lease对象用来保存健康信息，在默认40s的nodeleaseDurationSeconds周期内，若Lease对象没有被更新，则对应节点可以被判定为不健康。\n\n```yaml\napiVersion: coordination.k8s.io/v1\nkind: Lease\nmetadata:\n  creationTimestamp: \"2021-08-19T02:50:09Z\"\n  name: k8snode\n  namespace: kube-node-lease\n  ownerReferences:\n  - apiVersion: V1\n    kind: Node\n    name: k8snode\n    uid: 58679942-e2dd-4ead-aada-385f099d5f56\n  resourceVersion: \"1293702\"\n  uid: 1bf51951-b832-49da-8708-4b224b1ec3ed\nspec:\n  holderldentity: k8snode\n  leaseDurationSeconds: 40\n  renewTime: \"2021-09-08T01:34:16.489589Z\"\n```\n\n## 资源预留\n\n计算节点除用户容器外，还存在很多支撑系统运行的基础服务，譬如systemd、journald、sshd、dockerd、Containerd、kubelet等。\n\n为了使服务进程能够正常运行，要确保它们在任何时候都可以获取足够的系统资源，所以我们要为这些系统进程预留资源。\n\nkubelet可以通过众多启动参数为系统预留CPU、内存、PID等资源，比如SystemReserved、KubeReserved等。\n\n## Capacity和Allocatable\n\n容量资源（Capacity）是指kubelet获取的计算节点当前的资源信息。\n\n- CPU是从/proc/cpuinfo文件中获取的节点CPU核数；\n- memory是从/proc/memoryinfo中获取的节点内存大小；\n- ephemeral-storage是指节点根分区的大小。\n\n资源可分配额（Allocatable）是用户Pod可用的资源，是资源容量减去分配给系统的资源的剩余部分。\n\n![image.png](Assets/image_1666277432159_0.png)\n\n## 节点磁盘管理\n\n- 系统分区 nodefs\n\t- 工作目录和容器日志\n- 容器运行时分区 imagefs\n\t- 用户镜像和容器可写层\n\t- 容器运行时分区是可选的，可以合并到系统分区中\n\n## 驱逐管理\n\n- kubelet会在系统资源不够时中止一些容器进程，以空出系统资源，保证节点的稳定性。\n- 但由kubelet发起的驱逐只停止Pod的所有容器进程，并不会直接删除Pod。\n\t- Pod的status.phase会被标记为Failed\n\t- status.reason会被设置为Evicted\n\t- status.message则会记录被驱逐的原因\n\n## 资源可用额监控\n\nkubelet依赖内嵌的开源软件cAdvisor，周期性检查节点资源使用情况\n\nCPU是可压缩资源，根据不同进程分配时间配额和权重，CPU可被多个进程竞相使用\n\n驱逐策略是基于磁盘和内存资源用量进行的，因为两者属于不可压缩的资源，当此类资源使用耗尽时将无法再申请\n\n![image.png](Assets/image_1666277809205_0.png)\n\n## 驱逐策略\n\nkubelet获得节点的可用额信息后，会结合节点的容量信息来判断当前节点运行的Pod是否满足驱逐条件。\n\n驱逐条件可以是绝对值或百分比，当监控资源的可使用额少干设定的数值或百分比时，kubelet就会发起驱逐操作。\n\nkubelet参数evictionMinimumReclaim可以设置每次回收的资源的最小值，以防止小资源的多次回收。\n\n![image.png](Assets/image_1666278219341_0.png)\n\n## 基于内存压力的驱逐\n\nmemory.avaiable表示当前系统的可用内存情况。\n\nkubelet默认设置了memory.avaiable\u003c100Mi的硬驱逐条件\n当kubelet检测到当前节点可用内存资源紧张并满足驱逐条件时，会将节点的MemoryPressure状\n态设置为True，调度器会阻止BestEffort Pod调度到内存承压的节点。\n\nkubelet启动对内存不足的驱逐操作时，会依照如下的顺序选取目标Pod：\n\n（1）判断Pod所有容器的内存使用量总和是否超出了请求的内存量，超出请求资源的Pod会成为\n备选目标。\n\n（2）查询Pod的调度优先级，低优先级的Pod被优先驱逐。\n\n（3）计算Pod所有容器的内存使用量和Pod请求的内存量的差值，差值越小，越不容易被驱逐。\n\n## 基于磁盘压力的驱逐\n\n* 以下任何一项满足驱逐条件时，它会将节点的DiskPressure状态设置为True，调度器不会再调度任何Pod到该节点上\n\t* nodefs.available\n\t* nodefs.inodesFree\n\t* imagefs.available\n\t* imagefs.inodesFree\n* 驱逐行为\n\t* 有容器运行时分区\n\t\t* nodefs达到驱逐阈值，那么kubelet删除已经退出的容器\n\t\t* Imagefs达到驱逐阈值，那么kubelet删除所有未使用的镜像。\n* 无容器运行时分区\n\t* kubelet同时删除未运行的容器和未使用的镜像。\n\n---\n\n* 回收已经退出的容器和未使用的镜像后，如果节点依然满足驱逐条件，kubelet就会开始驱逐正在运行的Pod，进一步释放磁盘空间。\n* 判断Pod的磁盘使用量是否超过请求的大小，超出请求资源的Pod会成为备选目标。\n* 查询Pod的调度优先级，低优先级的Pod优先驱逐。\n* 根据磁盘使用超过请求的数量进行排序，差值越小，越不容易被驱逐。\n\n## 容器和资源配置\n\n针对不同QoS Class的Pod，Kubneretes按如下Hierarchy组织cgroup中的CPU子系统\n\n![image.png](Assets/image_1666279135958_0.png)\n\n## CPU CGroup配置\n\n![image.png](Assets/image_1666279354888_0.png)\n\n## 内存CGroup配置\n\n![image.png](Assets/image_1666279387571_0.png)\n\n## OOM Killer行为\n\n系统的OOMKiller可能会采取OOM的方式来中止某些容器的进程，进行必要的内存回收操作而系统根据进程的oom score来进行优先级排序，，选择待终止的进程，且进程的oom score，越高，越容易被终止。\n\n进程的oom score是根据当前进程使用的内存占节点总内存的比例值乘以10，再加上oom_SCore_adj综合得到的\n\n而容器进程的oom score adj正是kubelet根据memory.request进行设置的\n\n![image.png](Assets/image_1666279569545_0.png)\n-\n\n## 日志管理\n\n节点上需要通过运行logrotate的定时任务对系统服务日志进行rotate清理，以防止系统服务日志占用大量的磁盘空间。\n\n* logrotate的执行周期不能过长，以防日志短时间内大量增长。\n* 同时配置日志的rotate条件，在日志不占用太多空间的情况下，保证有足够的日志可供查看。\n* Docker\n\t* 除了基于系统logrotate管理日志，还可以依赖Docker自带的日志管理功能来设置容器日志的数量和每个日志文件的大小。\n\t* Docker写入数据之前会对日志大小进行检查和rotate操作，确保日志文件不会超过配置的数量和大小。\n* Containerd\n\t* 日志的管理是通过kubelet定期（默认为10s）执行du命令，来检查容器日志的数量和文件的大小的。\n\t* 每个容器日志的大小和可以保留的文件个数，可以通过kubelet的配置参数container-log-max\u0002size 和container-log-max-files进行调整。\n\n## Docker卷管理\n\n在构建容器镜像时，可以在Dockerfile中通过VOLUME指令声明一个存储卷，目前Kubernetes尚未将其纳入管控范围，不建议使用。\n\n如果容器进程在可写层或emptyDir卷进行大量读写操作，就会导致磁盘I/O过高，从而影响其他容器进程甚至系统进程。\n\nDocker和Containerd运行时都基于CGroup v1。对于块设备，只支持对DirectI/O限速，而对于Buffer I/O还不具备有效的支持。因此，针对设备限速的问题，目前还没有完美的解决方案，对于有特殊I/O需求的容器，建议使用独立的磁盘空间。\n\n## 网络资源\n\n由网络插件通过Linux Traffic Control为Pod限制带宽\n\n可利用CNI社区提供的bandwidth插件\n\n```yaml\napiversion: v1\nkind: Pod\nmetadata:\n\tannotations:\n\t\tkubernetes.io/ingress-bandwidth: 10MB\n    \tkubernetes.io/egress-bandwidth: 10MB\n··\n```\n\n## 进程数\n\nkubelet默认不限制Pod可以创建的子进程数量，但可以通过启动参数podPidsLimit开启限制，还可以由reserved参数为系统进程预留进程数。\n\n- kubelet通过系统调用周期性地获取当前系统的PID的使用量，并读取/proc/sys/kernel/pid_max，获取系统支持的PID上限。\n- 如果当前的可用进程数少于设定阈值，那么kubelet会将节点对象的PIDPressure标记为True\n- kube-scheduler在进行调度时，会从备选节点中对处于NodeUnderPIDPressure状态的节点进行过滤。\n\n# 节点异常检测\n\n## Kubernetes集群可能存在的问题\n\n- 基础架构守护程序问题：NTP服务关闭；\n- 硬件问题：CPU，内存或磁盘损坏；\n- 内核问题：内核死锁，文件系统损坏；\n- 容器运行时问题：运行时守护程序无响应\n- ...\n\n当kubernetes中节点发生上述问题，在整个集群中，k8s服务组件并不会感知以上问题，就会导致pod仍会调度至问题节点。\n\n## node-problem-detector\n\n为了解决这个问题，社区引入了守护进程node-problem-detector，从各个守护进程收集节点问题，并使它们对上游层可见。\n\nKubernetes节点诊断的工具，可以将节点的异常，例如\n\n- Runtime无响应\n- Linux Kernel无响应\n- 网络异常\n- 文件描述符异常\n- 硬件问题如CPU，内存或者磁盘故障\n\n## 故障分类\n\n![image.png](Assets/image_1666282939883_0.png)\n\n## 问题汇报手段\n\nnode-problem-detector通过设置NodeCondition或者创建Event对象来汇报问题\n\nNodeCondition：针对永久性故障，会通过设置NodeCondition来改变节点状态\n\nEvent：临时故障通过Event来提醒相关对象，比如通知当前节点运行的所有Pod\n\n## 使用插件 pod 启用NPD\n\n如果你使用的是自定义集群引导解决方案，不需要覆盖默认配置，可以利用插件 Pod进一步自动化部署。\n\n创建 node-strick-detector.yaml，并在控制平面节点上保存配置到插件 Pod 的目录/etc/kubernetes/addons/node-problem-detector。\n\n## NPD的异常处理行为\n\nNPD只负责获取异常事件，并修改node condition，不会对节点状态和调度产生影响\n\n```yaml\nlastHeartbeatTime: \"2021-11-06T15:44:46Z\"\nlastTransitionTime: \"2021-11-06T15:29:43Z\"\nmessage:'kernel: INFO:task docker:20744 blocked for more than 120 seconds.'\nreason: DockerHung\nstatus: \"True\"\ntype: KernelDeadlock\n```\n\n需要自定义控制器，监听NPD汇报的condition，taint node，阻止pod调度到故障节点\n\n问题修复后，重启NPD Pod来清理错误事件\n\n# 常用节点问题排查手段\n\n## ssh到内网节点\n\n创建一个支持ssh的pod\n\n并通过负载均衡器转发ssh请求\n\n## 查看日志\n\n- 针对使用systemd拉起的服务\n\t- `journalctl-afu kubelet-S \"2019-08-26 15:00:00\"`\n\t\t- -u unit，对应的systemd拉起的组件，如kubelet\n\t\t- -f folloW，跟踪最新日志\n\t\t- -a show all，现实所有日志列\n\t\t- -S since，从某一时间开始-S \"2019-08-26 15∶00∶00\"\n- 对于标准的容器日志\n\t- `kubectl logs -f -c \u003ccontainername\u003e \u003cpodname\u003e`\n\t- `kubectl logs -f --all -containers \u003cpodname\u003e`\n\t- `kubectl logs -f -c \u003cpodname\u003e -- previous`\n- 如果容器日志被shell转储到文件，则需通过exec\n\t- `kubectl exec -it xxx --tail -f /path/to/log`\n\n# 基于extended resource扩展节点资源\n\n## 扩展资源\n\n扩展资源是 kubernetes.io 域名之外的标准资源名称。 它们使得集群管理员能够颁布非Kubernetes 内置资源，而用户可以使用他们。\n\n自定义扩展资源无法使用kubernetes.io作为资源域名\n\n## 管理扩展资源\n\n- 节点级扩展资源\n\t- 节点级扩展资源绑定到节点\n- 设备插件管理的资源\n\t- 发布在各节点上由设备插件所管理的资源，如GPU，智能网卡等\n\n## 为节点配置资源\n\n集群操作员可以向 API服务器提交 PATCH HTTP请求，以在集群中节点的 status.capacity 中为其配置可用数量。\n\n完成此操作后，节点的 status.capacity 字段中将包含新资源。\n\nkubelet会异步地对 status.allocatable字段执行自动更新操作，使之包含新资源。\n\n调度器在评估 Pod 是否适合在某节点上执行时会使用节点的 status.allocatable 值，在更新节点容量使之包含新资源之后和请求该资源的第一个 Pod 被调度到该节点之间，可能会有短暂的延迟。\n\n```bash\ncurl --key admin.key --cert admin.crt --header \"Content-Type: application/json\u0002patch+ison\"\\\n--request PATCH-k\\\n--data '[{\"op\":\"add\",\"path\":\"/status/capacity/cncamp.com~1reclaimed-cpu\", \"Value\":\"2\"}]'\nhttps://192.168.34.2:6443/api/v1/nodes/cadmin/status\n```\n\n## 使用扩展资源\n\n```yaml\nspec:\n  containers:\n    - name: nginx\n      image: nginx\n      resources:\n        limits:\n          cncamp.com/reclaimed-cpu: 3\n        requests:\n          cncamp.com/reclaimed-cpu: 3\n```\n\n## 集群层面的扩展资源\n\n- 可选择由默认调度器管理资源，默认调度器像管理其他资源一样管理扩展资源\n\t- Request与Limit必须一致，因为Kubernetes无法确保扩展资源的超售\n- 更常见的场景是，由调度器扩展程序（Scheduler Extenders）管理，这些程序处理资源消耗和资源配额\n\t- 修改调度器策略配置ignoredByScheduler字段可配置调度器不要检查自定义资源\n\n```json\n{\n    \"kind\": \"Policy\",\n    \"apiVersion\": \"v1\",\n    \"extenders\": [\n        {\n            \"urlPrefix\": \"\u003cextender-endpoint\u003e\",\n            \"bindVerb\": \"bind\",\n            \"managedResources\": [\n                {\n                    \"name\": \"example.com/foo\",\n                    \"ignoredByScheduler\": true\n                }\n            ]\n        }\n    ]\n}\n```\n\n# 构建和管理高可用集群\n\n## Kubernetes高可用层级\n\n![image.png](Assets/image_1667027450698_0.png)\n-\n\n## 高可用的数据中心\n\n- 多地部署\n- 每个数据中心需要划分成具有独立供电、制冷、网络设备的高可用区\n- 每个高可用区管理独立的硬件资产，包括机架、计算节点、存储、负载均衡器、防火墙等硬件设备\n\n## Node的生命周期管理\n\n运营Kubernetes集群，不仅仅是集群搭建那么简单，运营需要对集群中所有节点的完整申明周期负责。\n\n- 集群搭建\n- 集群扩容/缩容\n- 集群销毁（很少）\n- 无论是集群搭建还是扩容，核心是Node的生命周期管理\n\t- Onboard\n\t\t- 物理资产上架\n\t\t- 操作系统安装\n\t\t- 网络配置\n\t\t- Kubernetes组件安装\n\t\t- 创建Node 对象\n\t- 故障处理\n\t\t- 临时故障?重启大法好\n\t\t- 永久故障?机器下架\n\t- Offboard\n\t\t- 删除Node对象\n\t\t- 物理资产下架，送修/报废\n\n## 主机管理\n\n选定哪个版本的系统内核、哪个发行版、安装哪些工具集、主机网络如何规划等。\n\n日常的主机镜像升级更新也可能是造成服务不可用的因素之一。\n\n主机镜像更新可以通过A/B系统OTA（Over The Air）升级方式进行。\n\n分别使用A、B两个存储空间，共享一份用户数据。在升级过程中，OTA更新即往其中一个存储空间写入升级包，同时保证了另一个系统可以正常运行，而不会打断用户。如果OTA失败，那么设备会启动到OTA之前的磁盘分区，并且仍然可以使用。\n\n## 生产化集群管理\n\n- 如何设定单个集群规模\n\t- 社区声明单一集群可支持5000节点，在如此规模的集群中，大规模部署应用是有诸多挑战的。应该更多还是更少?如何权衡?\n- 如何根据地域划分集群\n\t- 不同地域的计算节点划分到同一集群\n\t- 将同一地域的节点划分到同一集群\n- 如何规划集群的网络\n\t- 企业办公环境、测试环境、预生产环境和生产环境应如何进行网络分离\n\t- 不同租户之间应如何进行网络隔离\n- 如何自动化搭建集群\n\t- 如何自动化搭建和升级集群，包括自动化部署控制平面和数据平面的核心组件\n\t- 如何与企业的公共服务集成\n\n## 企业公共服务\n\n需要与企业认证平台集成，这样企业用户就能通过统一认证平台接入Kubernetes集群，而无须重新设计和管理一套用户系统。\n\n集成企业的域名服务、负载均衡服务，提供集群服务对企业外发布的访问入口\n\n在与企业的公共服务集成时，需要考虑它们的服务是否可靠\n\n对于不能异步调用的请求，采用同步调用需要设置合理的超时时间\n\n过长的超时时间，，会延迟结果等待时间，导致整体的链路调用时间延长，从而降低整体的TPS\n\n有些失败是短暂的、偶然的（比如网络抖动），进行重试即可。而有些失败是必然的，重试反而会造成调用请求量放大，加重对调用系统的负担\n\n## 控制平面的高可用保证\n\n针对大规模的集群，应该为控制平面组件划分单独节点，减少业务容器对控制平面容器或守护进程的干扰和资源抢占\n\n控制平面所在的节点，应确保在不同机架上，以防止因为某些机架的交换机或电源出问题，造成所有的控制面节点都无法工作\n\n保证控制平面的每个组件有足够的CPU、内存和磁盘资源，过于严苛的资源限制会导致系统效率低下，降低集群可用性\n\n应尽可能地减少或消除外部依赖。在Kubneretes初期版本中存在较多Cloud Provider API的调用，导致在运营过程中，当Cloud Provider API出现故障时，会使得Kubernetes集群也无法正常工作。\n\n应尽可能地将控制平面和数据平面解耦，确保控制平面组件出现故障时，将业务影响降到最低。\n\nKubernetes还有一些核心插件，是以普通的Pod形式加载运行的，可能会被调度到任意工作节点，与普通应用竞争资源。这些插件是否正常运行也决定了集群的可用性。\n\n## 高可用集群\n\n![image.png](Assets/image_1667030270184_0.png)\n\n## 集群安装方法比较\n\n![image.png](Assets/image_1667030574489_0.png)\n\n## 用Kubespray搭建高可用集群搭建\n\n![image.png](Assets/image_1667037652776_0.png)\n\n## 基于声明式API管理集群\n\n- 集群管理不仅仅包括集群搭建，还有更多功能需要支持\n\t- 集群扩缩容\n\t- 节点健康检查和自动修复\n\t- Kubernetes升级\n\t- 操作系统升级\n- 云原生场景中集群应该按照我们的期望的状态运行，这意味着我们应该将集群管理建立在声明式API的基础之上\n\n## Kubernetes Cluster API\n\n![image.png](Assets/image_1667038756175_0.png)\n\n## 参与角色\n\n- 管理集群\n\t- 管理workload 集群的集群，用来存放Cluster API对象的地方\n- Workload集群\n\t- 真正开放给用户用来运行应用的集群，由管理集群管理\n- Infrastructure provider\n\t- 提供不同云的基础架构管理，包括计算节点，网络等。目前流行的公有云多与Cluster API集成了。\n- Bootstrap provider\n\t- 证书生成\n\t- 控制面组件安装和初始化，监控节点的创建\n\t- 将主节点和计算节点加入集群\n- Control plane\n\t- Kubernetes控制平面组件\n\n## 涉及模型\n\n- Machine\n\t- 计算节点，用来描述可以运行Kubernetes组件的机器对象（注意与Kubernetes Node）的差异\n\t- 一个新Machine被创建以后，对应的控制器会创建一个计算节点，安装好操作系统并更新Machine的状态\n\t- 当一个Machine被删除后，对应的控制器会删除掉该节点并回收计算资源。\n\t- 当Machine属性被更新以后（比如Kubernetes版本更新），对应的控制器会删除旧节点并创建新节点\n- Machine Immutability (In-place Upgrade vs. Replace)\n\t- 不可变基础架构\n- MachineDeployment\n\t- 提供针对Machine和MachineSet的声明式更新，类似于Kubernetes Deployment\n- MachineSet\n\t- 维护一个稳定的机器集合，类似Kubernetes ReplicaSet\n- MachineHealthCheck\n\t- 定义节点应该被标记为不可用的条件\n\n## 日常运营中的节点问题归类\n\n- 可自动修复的问题\n\t- 计算节点down\n\t\t- Ping不通\n\t\t- TCP probe失败\n\t\t- 节点上的所有应用都不可达\n- 不可自动修复的问题\n\t- 文件系统坏\n\t- 磁盘阵列故障\n\t- 网盘挂载问题\n\t- 其他硬件故障\n\t- Kernel出错，core dumps\n- 其他问题\n\t- 软件Bug\n\t- 进程锁死，或者memory/CPU竞争问题\n\t- Kubernetes组件出问题\n\t\t- Kubelet/Kube-proxy/Docker/Salt\n\n## 故障监测和自动恢复\n\n当创建Compute节点时，允许定义Liveness Probe\n\n当livenessProbe失败时，ComputeNode的ProbePassed设置为false\n\n在Prometheus中，已经有Node level的alert，抓取Prometheus中的alert\n\n![image.png](Assets/image_1667045860645_0.png)\n\n设定自动恢复规则\n\n- 大多数情况下，重启大法好（人人都是Restart operator)。\n- 如果重启不行就重装。（reprovision）\n- 重装不行就重修。（breakfix）\n\n# Cluster Autoscaler\n\n## 工作机制\n\n- 扩容\n\t- 由于资源不足，pod调度失败，即有pod一直处于Pending状态\n- 缩容\n\t- node的资源利用率较低时，持续10分钟低于50%\n\t- 此node上存在的pod都能被重新调度到其他node上运行\n\n![image.png](Assets/image_1667046078752_0.png)\n\n## Cluster AutoScaler架构\n\n- Autoscaler：核心模块，负责整体扩缩容功能\n- Estimator：负责评估计算扩容节点\n- Simulator：负责模拟调度，计算缩容节点\n- Cloud-Provider：与云交互进行节点的增删操作，每个支持CA的主流厂商都实现自己的plugin实现动态缩放\n\n![image.png](Assets/image_1667046243581_0.png)\n\n## Cluster Autoscaler的扩展机制\n\n- 为了自动创建和初始化Node，Cluster Autoscaler要求Node必须属于某个Node Group，比如\n\t- GCE/GKE 中的 Managed instance groups（MIG）\n\t- AWS中的 Autoscaling Groups\n\t- Cluster API Node\n- 当集群中有多个Node Group 时，可以通过`--expander=\u003coption\u003e` 选项配置选择Node Group 的策略，支持如下四种方式\n\t- random：随机选择\n\t- most-pods：选择容量最大（可以创建最多Pod）的Node Group\n\t- least-waste：以最小浪费原则选择，即选择有最少可用资源的 Node Group\n\t- price：选择最便宜的 Node Group\n\n# 多租户集群管理\n\n## 租户\n\n租户是指一组拥有访问特定软件资源权限的用户集合，在多租户环境中，它还包括共享的应用、服务、数据和各项配置等\n\n多租户集群必须将租户彼此隔离，以最大限度地减少租户与租户、租户与集群之间的影响\n\n集群须在租户之间公平地分配集群资源。通过多租户共享集群资源，可以有效地降低集群管理成本，提高整体集群的资源利用率\n\n## 认证-实现多租户的基础\n\n租户管理首先需要识别访问的用户是谁，因此用户身份认证是多租户的基础\n\n权限控制，如允许合法登录的用户访问、拒绝非法登录的用户访问或提供有限的匿名访问\n\nKubernetes可管理两类用户\n\n用来标识和管理系统组件的ServiceAccount\n\n外部用户的认证，需要通过Kubernetes的认证扩展来对接企业、供应商的认证服务，为用户验证、操作授权、资源隔离等提供基础\n\n## 隔离\n\n除认证、授权这些基础条件外，还要能够保证用户的工作负载彼此之间有尽可能安全的隔离，减少用户工作负载之间的影响。通常从权限、网络、数据三个方面对不同用户进行隔离\n\n权限隔离\n\n普通用户的容器默认不具有priviledged、sys admin、net admin等高级管理权限，以阻止对宿主机及其他用户的容器进行读取、写入等操作。\n\n网络隔离\n\n不同的Pod，运行在不同的Network Namespace中，拥有独立的网络协议栈。Pod之间只能通过容器开放的端口进行通信，不能通过其他方式进行访问。\n\n数据隔离\n\n容器之间利用Namespace进行隔离，在第2章中我们已经对不同的Namespace进行了详细描述。不同Pod的容器，运行在不同的MNT、UTS、PID、IPC Namespace上，相互之间无法访问对方的文件系统、进程、IPC等信息;同一个Pod的容器，其mnt、PID Namespace也不共享。\n\n## 租户隔离手段\n\nNamespace：Namespace属于且仅属于一个租户。权限定义∶定义内容包括命名空间中的Role与RoleBinding。这些资源表示目前租户在归属于自己的命名空间中定义了什么权限、授权给了哪些租户的成员。\n\nPod安全策略：特殊权限指集群级的特定资源定义——PodSecurityPolicy。它定义了一系列工作负载与基础设施之间、工作负载与工作负载之间的关联关系，并通过命名空间的RoleBinding完成授权。\n\n网络策略：基础设施层面为保障租户网络的隔离机制提供了一系列默认策略，以及租户自己定制的用于租户应用彼此访问的策略。\n\nPod、Service、PersistentVolumeClaim等命名空间资源：这些定义表示租户的应用落地到Kubernetes中的实体。\n\n![image.png](Assets/image_1667048020921_0.png)\n\n## 权限隔离\n\n- 基于Namespace的权限隔离\n\t- 创建一个namespace-admin ClusterRole，拥有所有对象的所有权限\n\t- 为用户开辟新namespace，并在该namespace创建rolebinding绑定namespace-admin ClusterRole，用户即可拥有当前namespace所有对象操作权限\n- 自动化解决方案\n\t- 当Namespace创建时，通过mutatingwebhook将namespace变形，将用户信息记录至namespace annotation\n\t- 创建一个控制器，监控namespace，创建rolebinding为该用户绑定namespace-admin 的权限\n\n![image.png](Assets/image_1667048158659_0.png)\n\n## Quota管理\n\n开启ResourceQuota准入插件。\n\n在用户namespace创建ResourceQuota对象进行限额配置。\n\n```yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: high-qos-limit-requests\nSpec:\n  hard:\n    limits.cpu: 8\n    limits.memory: 24Gi\n    pods: 10\n    requests.cpu: 4\n    requests.memory: 12Gi\n  scopes:\n    - NotBestEffort\n```\n\n![image.png](Assets/image_1667048868059_0.png)\n\n## 节点资源隔离\n\n通过为节点设置不同taint来识别不同租户的计算资源。\n\n不同租户在创建Pod时，增加Toleration关键字，确保其能调度至某个taint的节点。\n\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/%E8%B0%83%E5%BA%A6":{"title":"调度","content":"\n# kube-scheduler\n\nkube-scheduler负责分配调度Pod 到集群内的节点上，它监听 kube-apiserver，查询还未分配\n\nNode 的Pod，然后根据调度策略为这些 Pod 分配节点（更新 Pod 的 NodeName 字段）。\n\n调度器需要充分考虑诸多的因素∶\n\n* 公平调度；\n* 资源高效利用；\n* QoS；\n* affinity和 anti-affinity；\n* 数据本地化（data locality）；\n* 内部负载干扰（inter-workload interference）；\n* deadlines。\n\n# 调度器\n\nkube-scheduler 调度分为两个阶段，predicate 和 priority：\n\n* predicate：过滤不符合条件的节点；\n* priority：优先级排序，选择优先级最高的节点。\n\n# Predicates策略\n\n![image.png](Assets/image_1666026644919_0.png)\n\n![image.png](Assets/image_1666026667493_0.png)\n\n还有很多其他策略，你也可以编写自己的策略。\n\n# Predicates plugin工作原理\n\n![image.png](Assets/image_1666026900870_0.png)\n\n# Priorities策略\n\n![image.png](Assets/image_1666026930477_0.png)\n\n![image.png](Assets/image_1666026940091_0.png)\n\n# 资源需求\n\n* CPU\n\t* requests\n\t\t* Kubernetes调度Pod时，会判断当前节点正在运行的Pod的CPURequest的总和，再加上当前调度Pod的CPU request，计算其是否超过节点的CPU的可分配资源\n* limits\n\t* 配置cgroup以限制资源上限\n* 内存\n\t* requests\n\t\t* 判断节点的剩余内存是否满足Pod的内存请求量，以确定是否可以将Pod调度到该节点\n\t* limits\n\t\t* 配置cgroup以限制资源上限\n\n# 磁盘资源需求\n\n容器临时存储（ephemeral storage）包含日志和可写层数据，可以通过定义Pod Spec中的limits.ephemeral-storage和requests.ephemeral-storage来申请。\n\nPod调度完成后，计算节点对临时存储的限制不是基于CGroup的，而是由kubelet定时获取容器的日志和容器可写层的磁盘使用情况，如果超过限制，则会对Pod进行驱逐。\n\n# Init Container的资源需求\n\n当kube-Scheduler调度带有多个init容器的Pod时，只计算cpu.request最多的init容器，而不是计算所有的init容器总和。\n\n由于多个init容器按顺序执行，并且执行完成立即退出，所以申请最多的资源init容器中的所需资源，即可满足所有init容器需求。\n\nkube-Scheduler在计算该节点被占用的资源时，init容器的资源依然会被纳入计算。因为init容器在特定情况下可能会被再次执行，比如由于更换镜像而引起Sandbox重建时。\n\n# 把 Pod 调度到指定 Node 上\n\n可以通过 nodeSelector、nodeAffinity、podAfinity 以及Taints 和 tolerations 等来将Pod 调度到需要的 Node 上。\n\n也可以通过设置 nodeName 参数，将Pod 调度到指定node节点上。\n\n比如，使用nodeSelector，首先给Node加上标签∶ `kubectl label nodes \u003cyour-node-name\u003e disktype=ssd`\n\n接着，指定该Pod只想运行在带有disktype=ssd标签的Node上。\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n    - name: nginx\n      image: nginx\n      imagePullPolicy: IfNotPresent\n  nodeSelector:\n    disktype: ssd\n```\n\n## nodeSelector\n\n首先给 Node打上标签∶\n\nkubectl label nodes node-01 disktype=ssd\n\n然后在 daemonset 中指定nodeSelector为 disktype=ssd：\n\n```yaml\nSpec:  \n  nodeSelector:  \n    disktype: ssd \n```\n\n## NodeAffinity\n\nNodeAffinity 目前支持两种∶requiredDuringSchedulinglgnoredDuringExecution 和preferredDuringSchedulinglgnoredDuringExecution，分别代表必须满足条件和优选条件。\n\n比如下面的例子代表调度到包含标签Kubernetes.io/e2e-az-name并且值为e2e-az1或e2e-az2的Node上，并且优选还带有标签another-node-label-key=another-node-label-value 的 Node。\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: with-node-affinity\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulinglgnoredDuringExecution:\n        nodeSelectorTerms:\n          - matchExpressions:\n            - key: kubernetes.io/e2e-az-name\n              operator: In\n              values:\n              - e2e-az1\n              - e2e-az2\n      preferredDuringSchedulinglgnoredDuringExecution:\n        - weight: 1\n          preference:\n            matchExpressions:\n            - key: another-node-label-key\n              operator: In\n              values:\n              - another-node-label-value\n  containers:\n  - name: with-node-affinity\n    image: gcr.io/google_containers/pause:2.0\n```\n\n## podAffinity\n\npodAffinity 基于 Pod 的标签来选择Node，仅调度到满足条件 Pod 所在的 Node 上，支持podAffinity 和 podAntiAffinity。这个功能比较绕，以下面的例子为例∶\n\n如果一个\"Node 所在 Zone 中包含至少一个带有security=S1标签且运行中的 Pod\"，那么可以调度到该 Node，不调度到\"包含至少一个带有 security=S2标签且运行中 Pod\"的Node 上。\n\n```yaml\nkind: Pod\nmetadata:\n  name: with-pod-affinity\nspec:\n  afinity:\n    podAffinity:\n      requiredDuringSchedulinglgnoredDuringExecution:\n      - labelSelector:\n        matchExpressions:\n        - key: security\n          operator: In\n          values:\n            - S1\n      topologyKey: failure-domain.beta.kubernetes.io/zone\n    podAntiAffinity:\n      preferredDuringSchedulinglgnoredDuringExecution:\n      - weight: 100\n        podAffityTerm:\n          labelSelector:\n            matchExpressions:\n            - key: security\n              operator: In\n              values:\n              - 52\n          topologyKey: kubernetes.io/hostname\n  containers:\n  - name: with-pod-affinity\n    image: gcr.io/google_containers/pause:2.0\n```\n\n## Taints 和 Toleration\n\nTaints和Tolerations 用于保证 Pod 不被调度到不合适的 Node 上，其中Taint 应用于Node 上，而Toleration 则应用于 Pod 上。\n\n目前支持的Taint类型：\n\n- **NoSchedule**：新的 Pod不调度到该Node 上，不影响正在运行的 Pod；\n- **PreferNoSchedule**：soft版的NoSchedule，尽量不调度到该 Node 上；\n- **NoExecute**：新的 Pod 不调度到该Node上，并且删除（evict）已在运行的 Pod。Pod 可以增加一个时间（tolerationSeconds）。\n\n然而，当 Pod 的 Tolerations 匹配 Node 的所有Taints 的时候可以调度到该Node上;当 Pod 是已经运行的时候，也不会被删除（evicted）。另外对于NoExecute，如果 Pod 增加了一个tolerationSeconds，则会在该时间之后才删除 Pod。\n\n## 多租户Kubernetes集群-计算资源隔离\n\nKubernetes集群一般是通用集群，可被所有用户共享，用户无需关心计算节点细节。\n\n但往往某些自带计算资源的客户要求：\n\n* 带着计算资源加入Kubernetes集群；\n* 要求资源隔离。\n\n实现方案：\n\n* 将要隔离的计算节点打上Taints；\n* 在用户创建创建 Pod 时，定义 tolerations来指定要调度到 node taints。\n\n**该方案有漏洞吗?如何堵住?**\n\n* 其他用户如果可以get nodes 或者 pods，可以看到taints信息，也可以用相同的tolerations占用资源。\n* 不让用户get node detail？\n* 不让用户get别人的pod detail？\n* 企业内部，也可以通过规范管理，通过统计数据看谁占用了哪些node；\n* 数据平面上的隔离还需要其他方案配合。\n\n## 优先级调度\n\n从v1.8开始，kube-scheduler支持定义Pod 的优先级，从而保证高优先级的 Pod 优先调度。开启方法为：\n\n* apiserver 配置 `--feature-gates=PodPriority=true` 和 `--runtime\u0002config=scheduling.k8s.io/v1alpha1=true`\n* kube-Scheduler 配置 `--feature-gates=PodPriority=true`\n\n### PriorityClass\n\n在指定 Pod 的优先级之前需要先定义一个PriorityClass（非namespace资源），如：、\n\n```yaml\npiVersion: v1\nkind: PriorityClass\nmetadata:\n  name: high-priority\nvalue: 1000000\nglobalDefault: false\ndescription: \"This priority class should be used for XYZ service pods only.\"\n```\n\n### 为 pod 设置 priority\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nlabels:\n  env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\npriorityClassName: high-priority\n```\n\n### 多调度器\n\n如果默认的调度器不满足要求，还可以部署自定义的调度器。并且，在整个集群中还可以同时运行多个调度器实例，通过 podSpec.schedulerName 来选择使用哪一个调度器（默认使用内置的调度器）。\n\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/%E8%BF%9C%E7%A8%8B%E8%B0%83%E8%AF%95":{"title":"远程调试","content":"\n\u003e 原理：https://learn.microsoft.com/zh-cn/visualstudio/bridge/overview-bridge-to-kubernetes\n\n\u003e 教程：https://learn.microsoft.com/zh-cn/visualstudio/bridge/bridge-to-kubernetes-vs-code\n\n\u003e 示例：https://learn.microsoft.com/zh-cn/visualstudio/bridge/bridge-to-kubernetes-sample\n\n## 安装\n\n1. 安装vscode\n2. 安装vscode扩展Kubernetes和Bridge to Kubernetes\n3. 安装go扩展\n\n## 配置\n\n1. 打开项目\n2. 选择kubernetes namespace\n3. 打开命令面板，运行命令“Bridge to Kubernetes: 配置”\n4. 选择需要重定向到本地的服务\n5. 输入本地运行的程序的端口号\n6. 选择或创建调试的启动配置（与本地开发的配置相同）\n7. 选择在隔离或非隔离模式下运行，隔离模式需要转发header kubernetes-route-as\n8. 在调试中选择刚才创建的配置启动\n\n## 限制\n\n- 要使 Bridge to Kubernetes 成功连接，一个 pod 只能有一个容器在该 pod 中运行。\n- 目前，Bridge to Kubernetes pod 必须是 Linux 容器。 不支持 Windows 容器。\n- Bridge to Kubernetes 需要提升的权限才能在开发计算机上运行，以便编辑主机文件。\n- Bridge to Kubernetes 不能用于已启用 Azure Dev Spaces 的群集。\n\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/%E9%83%A8%E7%BD%B2KubeSphere":{"title":"部署KubeSphere","content":"\n# 安装KubeSphere\n\n## 直接在k8s中安装\n\n```sh\nkubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.3.0/kubesphere-installer.yaml\n  \nkubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.3.0/cluster-configuration.yaml\n```\n\n## 检查安装日志\n\n`kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l 'app in (ks-install, ks-installer)' -o jsonpath='{.items[0].metadata.name}') -f`\n\n# 访问 KubeSphere 控制台\n\n执行 `kubectl edit ks-console` 将 service 类型 `NodePort`  更改为 `LoadBalancer`\n\n执行 `kubectl get svc -n kubesphere-system` 获取 EXTERNAL-IP\n\n使用默认帐户和密码（ `admin/P@88w0rd` ）登录控制台\n\n# 删除自带的prometheus\n\n## 官方教程\n\n### 卸载堆栈\n\n```sh\n\nbectl -n kubesphere-system exec $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- kubectl delete -f /kubesphere/kubesphere/prometheus/alertmanager/ 2\u003e/dev/null\n\nbectl -n kubesphere-system exec $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- kubectl delete -f /kubesphere/kubesphere/prometheus/devops/ 2\u003e/dev/null\n\nbectl -n kubesphere-system exec $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- kubectl delete -f /kubesphere/kubesphere/prometheus/etcd/ 2\u003e/dev/null\n\nbectl -n kubesphere-system exec $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- kubectl delete -f /kubesphere/kubesphere/prometheus/grafana/ 2\u003e/dev/null\n\nbectl -n kubesphere-system exec $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- kubectl delete -f /kubesphere/kubesphere/prometheus/kube-state-metrics/ 2\u003e/dev/null\n\nbectl -n kubesphere-system exec $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- kubectl delete -f /kubesphere/kubesphere/prometheus/node-exporter/ 2\u003e/dev/null\n\nbectl -n kubesphere-system exec $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- kubectl delete -f /kubesphere/kubesphere/prometheus/upgrade/ 2\u003e/dev/null\n\nbectl -n kubesphere-system exec $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- kubectl delete -f /kubesphere/kubesphere/prometheus/prometheus-rules-v1.16\\+.yaml 2\u003e/dev/null\n\nbectl -n kubesphere-system exec $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- kubectl delete -f /kubesphere/kubesphere/prometheus/prometheus-rules.yaml 2\u003e/dev/null\n\nbectl -n kubesphere-system exec $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- kubectl delete -f /kubesphere/kubesphere/prometheus/prometheus 2\u003e/dev/null\n# Uncomment this line if you don't have Prometheus managed by Prometheus Operator in other namespaces.\n\nkubectl -n kubesphere-system exec $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- kubectl delete -f /kubesphere/kubesphere/prometheus/init/ 2\u003e/dev/null\n```\n\n### 删除PVC\n\n```sh\nkubectl -n kubesphere-monitoring-system delete pvc `kubectl -n kubesphere-monitoring-system get pvc | grep -v VOLUME | awk '{print $1}' |  tr '\\n' ' '`\n```\n\n## 网友教程\n\n```sh\n\nbectl -n kubesphere-system exec $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- kubectl delete -f /kubesphere/kubesphere/prometheus/alertmanager/ 2\u003e/dev/null\n\nbectl -n kubesphere-system exec $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- kubectl delete -f /kubesphere/kubesphere/prometheus/devops/ 2\u003e/dev/null\n\nbectl -n kubesphere-system exec $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- kubectl delete -f /kubesphere/kubesphere/prometheus/etcd/ 2\u003e/dev/null\n\nbectl -n kubesphere-system exec $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- kubectl delete -f /kubesphere/kubesphere/prometheus/grafana/ 2\u003e/dev/null\n\nbectl -n kubesphere-system exec $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- kubectl delete -f /kubesphere/kubesphere/prometheus/kube-state-metrics/ 2\u003e/dev/null\n\nbectl -n kubesphere-system exec $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- kubectl delete -f /kubesphere/kubesphere/prometheus/node-exporter/ 2\u003e/dev/null\n\nbectl -n kubesphere-system exec $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- kubectl delete -f /kubesphere/kubesphere/prometheus/upgrade/ 2\u003e/dev/null\n\nbectl -n kubesphere-system exec $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- kubectl delete -f /kubesphere/kubesphere/prometheus/prometheus-rules-v1.16\\+.yaml 2\u003e/dev/null\n\nbectl -n kubesphere-system exec $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- kubectl delete -f /kubesphere/kubesphere/prometheus/prometheus-rules.yaml 2\u003e/dev/null\n\nbectl -n kubesphere-system exec $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- kubectl delete -f /kubesphere/kubesphere/prometheus/prometheus 2\u003e/dev/null\nkubectl delete deploy -n  kubesphere-monitoring-system prometheus-operator\nkubectl delete svc -n kubesphere-monitoring-system prometheus-operator\nkubectl delete prometheusrules.monitoring.coreos.com -n kubesphere-monitoring-system  prometheus-operator-rules  prometheus-k8s-rules\nkubectl delete servicemonitor -n kubesphere-monitoring-system coredns kube-apiserver  kube-controller-manager  kube-scheduler kubelet prometheus-operator\n```\n\n# 配置KubeSphere\n\n## 配置 `monitoring endpoint`\n\n```sh\nkubectl edit cm -n kubesphere-system kubesphere-config      #集群重启后会失效\n \n    monitoring:\n      endpoint: http://prometheus-operated.monitoring.svc:9090\n  \nkubectl edit cc -n kubesphere-system ks-installer          #集群重启后不会失效\n    monitoring:\n      endpoint: http://prometheus-operated.monitoring.svc:9090\n      \n# 果您启用了 KubeSphere 的告警组件，请搜索 alerting 的 prometheusEndpoint 和 thanosRulerEndpoint，并参照如下示例修改。KubeSphere Apiserver 将自动重启使设置生效。\n...\n   alerting:\n     ...\n     prometheusEndpoint: http://prometheus-operated.monitoring.svc:9090\n     thanosRulerEndpoint: http://thanos-ruler-operated.monitoring.svc:10902\n     ...\n...\n```\n\n## 修复dashboard的图表\n\n```sh\n# 下载kubernetes-prometheusRule.yaml，此与apiserver指标有关\nwget https://raw.githubusercontent.com/kubesphere/ks-installer/master/roles/ks-monitor/files/prometheus/kubernetes/kubernetes-prometheusRule.yaml\n  \n# 应用kubernetes-prometheusRule.yaml\nkubectl apply -f kubernetes-prometheusRule.yaml\n  \n# kube-prometheus-stack-node.rules会与kubernetes-prometheusRule.yaml中的’node_namespace_pod:kube_pod_info:'和node:node_num_cpu:sum冲突。\nkubectl edit prometheusrules.monitoring.coreos.com -n monitoring kube-prometheus-stack-node.rules\n# 删除以下内容\n\nexpr: |-\n        topk by(cluster, namespace, pod) (1,\n          max by (cluster, node, namespace, pod) (\n            label_replace(kube_pod_info{job=\"kube-state-metrics\",node!=\"\"}, \"pod\", \"$1\", \"pod\", \"(.*)\")\n        ))\n      record: 'node_namespace_pod:kube_pod_info:'\n\nexpr: |-\n        count by (cluster, node) (sum by (node, cpu) (\n          node_cpu_seconds_total{job=\"node-exporter\"}\n        * on (namespace, pod) group_left(node)\n          topk by(namespace, pod) (1, node_namespace_pod:kube_pod_info:)\n        ))\n      record: node:node_num_cpu:sum\n```\n\n## 重启 KubeSphere APIserver\n\n```sh\nkubectl -n kubesphere-system rollout restart deployment/ks-apiserver\n# 或者\nkubectl rollout restart deploy -n kubesphere-system ks-installer\n```\n\n# 完全删除KubeSphere\n\n使用脚本[kubesphere-delete.sh](https://github.com/kubesphere/ks-installer/blob/release-3.1/scripts/kubesphere-delete.sh)\n\n# 参考文章\n\n1. [kubesphere集成自有的kube-prometheus-stack](https://blog.csdn.net/u010533742/article/details/125554002)\n2. [集成您自己的 Prometheus](https://kubesphere.io/zh/docs/v3.3/faq/observability/byop/)\n\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/%E9%9B%86%E7%BE%A4%E8%81%94%E9%82%A6%E5%92%8CIstio%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86":{"title":"集群联邦和Istio多集群管理","content":"\n# 集群治理的驱动力\n\n## 分布式云是未来\n\n- **成本优化**（Cost Effective）\n- 更好的弹性及灵活性（**Elasticity\u0026Flexibility**）\n- **避免厂商锁定**（Avoid Vendor Lock-in）\n- 第一时间获取云上的新功能（**Innovation**）\n- **容灾**（Resilience \u0026Recovery）\n- 数据保护及风险管理（**Data Protection\u0026Risk Management**）\n- **提升响应速度**（NetworkPerformance Improvements）\n\n## 分布式云的挑战\n\n- Kubernetes单集群承载能力有限\n- 异构的基础设施\n- 存量资源接入\n- 配置变更及下发\n- 跨地域、跨机房应用部署及管理\n- 容灾与隔离性，异地多活\n- 弹性调度及自动伸缩\n- 监控告警\n\n## 如何应对\n\n- 通过Kubernetes 屏蔽底层基础设施，提供统一的接入层\n- 多云架构\n- 多集群+多云\n- 多集群管控\n- 统一的管控面\n- 方便接入，降低使用门槛\n\n## 跨地域的集群管理\n\n![image.png](Assets/image_1667318499437_0.png)\n\n## 集群联邦\n\n### 集群联邦的必要性\n\n#### 单一集群的管理规模有上限\n\n**数据库存储**\n\netcd 作为 Kubernetes集群的后端存储数据库，对空间大小的要求比较苛刻，这限制了集群能存储的对象数量和大小。\n\n**内存占用**\n\n为提高系统效率，Kubernetes 的 API Server 作为 API网关，会对该集群的所有对象做缓存。集群越大，缓存需要的内存空间就越大。\n\n其他Kubernetes 控制器也需要对侦听的对象构建客户端缓存，这些都需要占用系统内存。这些内存需求都要求对系统的规模有所限制。\n\n**控制器复杂度**\n\nKubernetes 的一个业务流程是由多个对象和控制器联动完成的，即使控制器遵循了设计原则，随着对象数量的增长，控制器的处理耗时也会越来越长。\n\n#### 单个计算节点资源上限\n\n单个计算节点的资源，不仅仅是CPU、内存等可量化资源。\n\n还有端口、进程数量等不可量化资源。比如 Linux支持的 TCP端口上限是65535，去除常用端口和程序源端口后，留给 Service nodePort 的端口数量是有限的，这限制了集群支持的 Service 的数量。\n\n#### 故障域控制\n\n集群规模越大，控制平面组件出现故障时的影响范围就越大。为了更好地控制故障域（FaultDomain），需要将大规模的数据中心切分成多个规模相对较小的集群，每个集群控制在一定规模。\n\n#### 应用高可用部署\n\n生产应用通常需要多数据中心部署来保障跨地域高可用，以确保当其中一个数据中心出现故障，或者集群做技术迭代更新时，其他数据中心可以继续提供服务。\n\n#### 混合云\n\n私有云加公有云的混合云模式逐渐成为企业的主流架构。\n\n## 集群联邦的职责\n\n- 跨集群同步资源\n\t- 联邦可以将资源同步到多个集群并协调资源的分配。例如，联邦可以保证一个应用的 Deployment 被部署到多个集群中，同时能够满足全局的调度策略。\n- 跨集群服务发现\n\t- 联邦汇总各个集群的服务和 Ingress，并暴露到全局 DNS 服务中。\n- 高可用\n\t- 联邦可以动态地调整每个集群的应用实例，且隐藏了具体的集群信息。\n- 避免厂商锁定\n\t- 每个集群都是部署在真实的硬件或云供应商提供的硬件（或虚拟硬件）之上的，若要更换供应商，只需在新供应商提供的硬件上部署新的集群，并加入联邦。联邦可以几乎透明地将应用从原集群迁移到新集群而无须对应用做更改。\n\n## 混合云\n\n- **混合云是指将公有云和私有云整合在一起的统一云平台。**\n- 用户业务可灵活部署或扩容在不同云中。\n- 混合云避免厂商锁定。\n- 混合云可支持更多复杂业务场景∶\n\t- Cloud Burst。\n\t- 如12306，低流量时运行在本地数据中心，当请求量突然爆发的时候，可以直接在公有云扩容，节省数据中心成本。\n\t- 可将业务分为包含敏感数据和不包含敏感数据的业务，将敏感数据业务运行在私有云，将非敏感数据业务运行在公有云。\n\t- 可重复利用公有云提供商数据中心靠近边缘的能力。\n\n## 集群联邦\n\n**集群联邦（Federation）是将多个Kubernetes 集群注册到统一控制平面，为用户提供统一API入口的多集群解决方案。**\n\n集群联邦设计的核心是提供在全局层面对应用的描述能力，并将联邦对象实例化为 Kubernetes 对象，分发到联邦下辖的各个成员集群中。\n\n![image.png](Assets/image_1667319263814_0.png)\n\n## 基于集群联邦的高可用应用部署\n\n![image.png](Assets/image_1667319487098_0.png)\n\n## 集群联邦核心架构\n\netcd作为分布式存储后端存储所有对象；\n\nAPIServer作为API网关，接收所有来自用户及控制平面组件的请求；\n\n不同的控制器对联邦层面的对象进行管理、协调等；\n\n调度控制器在联邦层面对应用进行调度、分配。\n\n**集群联邦支持灵活的对象扩展，允许将基本Kubernetes对象扩展为集群联邦对象，并通过统一的联邦控制器推送和收集状态。**\n\n![image.png](Assets/image_1667322401735_0.png)\n\n## 集群联邦管理的对象\n\n**成员集群是联邦的基本管理单元，所有待管理集群均须注册到集群联邦。**\n\n集群联邦V2提供了统一的工具集（Kubefedct），允许用户对单个对象动态地创建联邦对象.。\n动态对象的生成基于CRD。\n\n![image.png](Assets/image_1667322442338_0.png)\n\n## 集群注册中心\n\n**集群注册中心（ClusterRegistry）提供了所有联邦下辖的集群清单，以及每个集群的认证信息，状态信息等。**\n\n集群联邦本身不提供算力，它只承担多集群的协调工作，所有被管理的集群都应注册到集群联邦中。\n\n集群联邦使用了单独的KubeFedCluster 对象（同样适用CRD来定义）来管理集群注册信息：\n\n- 在该对象的定义中，不仅包含集群的注册信息，还包含集群的认证信息的引用，以明确每个集群使用的认证信息；\n- 该对象还包含各个集群的健康状态、域名等；\n- 当控制器行为出现异常时，直接通过集群状态信息即可获知控制器异常的原因。\n\n## 集群类型\n\n**Host：**\n\n用于提供KubeFed API与控制平面的集群，本质上就是 Federation控制面。\n\n**Member：**\n\n通过 KubeFed API注册的集群，并提供相关身份凭证来让KubeFed Controller 能够存取集群。Host集群也可以作为Member 被加入。\n\n## 注册集群KubeFedCluster\n\n用来定义哪些Kubernetes集群要被联邦。\n\n可透过**kubefedctl join/unjoin** 来加入/删除集群，当成功加入时，会建立一个KubeFedCluster组件来储存集群相关信息，如**APIEndpoint、CA Bundle**等。\n\n这些信息会被用在**KubeFed Controller**存取不同Kubernetes 集群上，以确保能够建立KubernetesAPI资源。\n\n![image.png](Assets/image_1667323372563_0.png)\n\n## KubeFedCluster\n\n```yaml\napiVersion: core.kubefed.io/v1beta1\nkind: KubeFedCluster\nmetadata:\n  name: \"cluster1\"\nspec:\n  apiEndpoint: https://API Server.cluster1.example.com\n  caBundle: LSOtLS***LSOK\n  disabledTLSValidations:\n  - ‘*’\n  secretRef:\n    name: cluster1-vhvfw\nstatus:\n  conditions:\n  - reason: ClusterReady\n    type: Ready\n  region: China\n  zones:\n  - Shanghai\n```\n\n![image.png](Assets/image_1667323641894_0.png)\n\n## Federation 支持的核心对象\n\n![image.png](Assets/image_1667323667635_0.png)\n\n## Type Configuration\n\n定义了哪些Kubernetes API资源要被用于联邦管理。\n\n若想新增 Federated API的话，可通过`kubefedctlenable \u003cres\u003e`指令来建立。\n\n比如说想将ConfigMap 资源通过联邦机制建立在不同集群上时，就必须先在 **Federation Host**集群中，通过CRD建立新资源 **FederatedConfigMap**，接着再建立名称为configmaps 的 Type configuration（**FederatedTypeConfig**）资源，然后描述ConfigMap 要被 FederatedConfigMap 所管理\n\n这样KubeFed Controllers才能知道如何建立 Federated 资源。\n\n## FederatedConfigMap\n\n```bash\nk get crd federatedconfigmaps.types.kubefed.io\nNAME                                  CREATEDAT\nfederatedconfigmaps.types.kubefed.io 2021-09-13T10:10:03Z\n```\n\n```yaml\napiVersion: core.kubefed.io/v1beta1\nkind: FederatedTypeConfig\nmetadata:\n  name: configmaps\nspec:\n  federatedType:\n    group: types.kubefed.io\n    kind: FederatedConfigMap\n    pluralName: federatedconfigmaps\n    scope: Namespaced\n    version: v1betal\n  propagation: Enabled\n  targetType:\n    kind: ConfigMap\n    pluralName: configmaps\n    scope: Namespaced\n    version: v1\n```\n\n## 联邦对象组成\n\nTemplate：定义 Kubernetes 对象的模板。\n\nPlacement：定义联邦对象需要被同步的目标集群。\n\nOverrides：不同目标集群中，对Kubernetes 对象模板的本地化属性。\n\n![image.png](Assets/image_1667324020820_0.png)\n\n## Template\n\n**Template是联邦对象中定义Kubernetes 集群对象的模板部分，它的内容为完整的Kubernetes对象。**\n\n## Placement\n\nPlacement用来配置联邦对象的目标集群，其值可以是具体的集群名单，也可以是clusterSeletor选择对应标签（label）的集群。\n\n当两者同时存在时，明确定义的集群名单具有较高优先级。\n\n联邦根据优先级来定义要同步对象的目标集群，如果提供了集群名单（哪怕是一个空List），则无论clusterSelector提供什么内容，都会被忽略。\n\n## Overrides\n\nOverrides 用于针对每个集群进行本地化定制。\n\n在实际部署应用时，通常会通过调整不同集群中的配置模板，部署符合特定集群需求的应用，以更好地发挥网络、计算资源、存储等的优势。\n\n目前 Overrides 不支持 List（Array）。比如说无法修改\n\nspec.template.spec.containers\\[0\\].image。\n\n思考一下为什么\n\n## 使用 Federated 对象\n\n**将Namespace 设置为联邦对象**\n\nkubefedctl federate ns default\n\n## 联邦调度\n\n**KubeFed**提供了一种自动化机制来将工作负载实例分散到不同的集群中，这能够基于总副本数与集群的定义策略来将Deployment或ReplicaSet资源进行编排。\n\n编排策略是通过建立**ReplicaSchedulingPreference(RSP)** 文件，再由KubeFed RSP Controller 监听与撷取RSP内容来将工作负载实例建立到指定的集群上。\n\n![image.png](Assets/image_1667324454149_0.png)\n\n## 多集群DNS\n\n**KubeFed提供了一组 API资源，以及Controllers 来实现跨集群 Service/Ingress 的 DNS records自动产生机制。**\n\n需要结合 ExternalDNS来同步更新至 DNS服务供应商。\n\n在 2020年被移出，这里提供一个DNS接入的思路。\n\n## ServiceDNSRecord 原理\n\n![image.png](Assets/image_1667324649874_0.png)\n\n# Clusternet\n\n## Clusternet 简介\n\n**Clusternet（ClusterInternet）是一个兼具多集群管理和跨集群应用编排的开源云原生管控平台打通了跨VPC、跨地域、跨云的集群管理。**\n\nClusternet面向未来混合云、分布式云和边缘计算场景设计，支持海量集群的接入和管理。\n\nClusternet 在保证无侵入目轻量化的基础上，创建了一张集群网络，对子集群进行纳管，并支持多集群的应用编排与治理。\n\n开源项目地址：https∶//github.com/clusternet/clusternet\n\n## Clusternet 能力一览\n\n- 统一管控各类**Kubernetes**集群；\n- 集群管理**Pull/Push**模式；\n- 轻量化，开箱即用，易于部署和维护；\n- 跨集群的服务发现及服务互访；\n- Kubernetes 原生，没有额外的学习成本；\n- 完善的**RBAC**能力，访问任一子集群；\n- 完善的接入能力：**kubectl plugin/client-go**；\n- 支持分发各类原生应用/**CRD/HelmChart**。\n\n![image.png](Assets/image_1667324958372_0.png)\n\n## 多集群管理的挑战\n\n- 集群无处不在\n- 公有云、私有云、混合云、边缘、裸金属\n- 提供一致的纳管能力\n\t- 集群一键注册能力\n\t- 一致性的集群访问体验，比如 exec，logs\n\t- 权限管控 RBAC\n- 架构要足够轻量化，方便一键接入\n\n## Clusternet 架构\n\n![image.png](Assets/image_1667325210364_0.png)\n\n## Clusternet 架构\n\n- 最轻量化的架构\n- 一站式连接各类集群\n- 支持 **Pull**和 **Push** 模式\n- 跨集群路由访问\n- 支持子集群 **RBAC**访问\n- 提供一致的集群访问体验，比如 **exec，logs**\n- 原生 **Kubernetes API**\n- 接入成本低，**kubectl plugin/client-go**\n\n## 如何通过 Clusternet 访问任一子集群\n\nClusternet支持通过Bootstrap Token，Service Token，TLS证书等RBAC的方式访问子集群；\n\n**子集群的 credential信息不需要存在父集群中**\n\n也支持通过kubectl命令行的方式对子集群进行 create/get/List/watch/patch/exec等操作。\n\n---\n\n更详细步骤及功能可以参照\n\nhttps://aithub.com/clusternet/clusternet/blob/main/docs/tutorials/visiting-child-clusters-with-rbac.md\n\n- 使用curl进行访问\n- 通过kubectl进行访问\n\n## Clusternet 应用分发设计\n\n- 完全兼容**Kubernetes 的内置API**\n- 支持 **Helm Charts**\n- 支持**CRD**\n- 丰富灵活的分发策略配置、差异化策略\n\n## 应用分发的差异化痛点\n\n1. 在分发的资源上全部打上统一的标签，比如 apps.my.company/deployed-by∶my-platform;\n2. 在分发到子集群的资源上标记集群的信息，比如 apps.my.company/running-in∶cluster-01;\n3. 调整应用在每个集群中的副本数目、镜像名称等;\n4. 在分发到某集群前，调整应用在该集群中的一些配置，比如注入一个 Sidecar 容器等\n5. 灰度升级，变更可控，方便回滚;\n6. 重复定义怎么办?冲突吗?\n7. ...\n\n## Clusternet 应用分发模型\n\n![image.png](Assets/image_1667326114977_0.png)\n\n# Istio多集群\n\n## 跨地域流量管理的挑战\n\n采用多活数据中心的网络拓扑，任何生产应用都需要完成跨三个数据中心的部署。\n\n为满足单集群的高可用，针对每个数据中心，任何应用都需进行多副本部署，并配置负载均衡。\n\n以实现全站微服务化，但为保证高可用，服务之间的调用仍以南北流量为主。\n\n针对核心应用，除集群本地负载均衡配置以外，还需配置跨数据中心负载均衡，并通过权重控制将**99%** 的请求转入本地数据中心，将**1%** 的流量转向跨地域的数据中心。\n\n![image.png](Assets/image_1667373822431_0.png)\n\n## 规模化带来的挑战\n\n- 3主数据中心，20边缘数据中心，100+Kubernetes集群\n- 规模化运营Kubernetes集群\n\t- 总计100000物理节点\n\t- 单集群物理机节点规模高达5000\n- 业务服务全面容器化，单集群\n\t- Pod实例可达100000\n\t- 发布服务5000-10000\n- 单集群多环境支持\n\t- 功能测试、集成测试、压力测试共用单集群\n\t- 不同环境需要彼此隔离\n- 异构应用\n\t- 云业务，大数据，搜索服务\n\t- 多种应用协议\n\t- 灰度发布\n- 日益增长的安全需求\n\t- 全链路TLS\n- 可见性需求\n\t- 访问日志\n\t- Tracing\n\n## 多集群部署\n\n- Kubernetes集群联邦\n\t- 集群联邦APIServer作为用户访问Kubernetes集群入口\n\t- 所有Kubernetes 集群注册至集群联邦\n- 可用区\n\t- 数据中心中具有独立供电制冷设备的故障域\n\t- 同一可用区有较小网络延迟\n\t- 同一可用区部署了多个Kubernetes 集群\n- 多集群部署\n\t- 同一可用区设定一个网关集群\n\t- 网关集群中部署Istio Primary\n\t- 同一可用区的其他集群中部署Istio Remote\n\t- 所有集群采用相同RootCA\n\t- 相同环境TrustDomain相同\n- 东西南北流量统一管控\n\t- 同一可用区的服务调用基于Sidecar ·  跨可用区的服务调用基于 Istio Gateway\n\n![image.png](Assets/image_1667375341308_0.png)\n\n## 入站流量架构 L4+L7\n\n**为不同应用配置独立的网关服务以方便网络隔离。**\n\n基于IPVS/xDP的ServiceController：\n\n- 四层网关调度；\n- 虚拟IP地址分配；\n- 基于IPIP协议的转发规则配置；\n- 基于BGP的IP路由宣告；\n- 在Ingress Pod中配置Tunnel设备，\n- 并绑定虚拟IP地址以卸载IPIP包。\n\n![image.png](Assets/image_1667375443775_0.png)\n\n## 单网关集群多环境支持\n\n![image.png](Assets/image_1667375755323_0.png)\n\n## 应用高可用接入方案\n\n![image.png](Assets/image_1667376301327_0.png)\n\n## 为应用发布服务\n\n定义**LoadBalancer Type Service**，提供集群外可访问的 **LoadBalancerIlP**。\n\n其他集群可通过定义**WorkloadEntry**指向该**LoadBalancerIP**，以实现故障转移目的。\n\n## 创建 WorkloadEntry\n\n**创建WorkloadEntry指向其他数据中心 LoadBalancerIP**\n\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: WorkloadEntry\nmetadata:\n  name: foo\nspec:\n  address: foo.bar.svc.cluster2\n  labels:\n    run: foo\n  locality: region1/zone1\n```\n\n## 定义 ServiceEntry\n\n同时选择WorkloadEntry和本地Pod。\n\n**ServiceEntry对象可以将本地Pod和具有相同Label的WorkloadEntry定义成相同的Envoy Cluster。**\n\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: ServiceEntry\nmetadata:\n  name: foo\nspec:\n  hosts:\n  - foo.com\n  ports:\n  - name: http-default\n    number: 80\n    protocol: HTTP\n    targetPort: 80\n  resolution: STATIC\n  workloadSelector:\n    labels:\n      run: foo\n```\n\n## 在 VirtualService 中引用 ServiceEntry\n\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: foo\nspec:\n  gateways:\n  - foo\n  hosts:\n  - foo.com\n  http:\n  - match:\n    - port: 80\n    route:\n    - destination:\n      host: foo.com\n      port:\n        number: 80\n```\n\n```yaml\napiVersion: networking.istio.io/v1betal\nkind: Gateway\nmetadata:\n  name:foo\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - hosts:\n    - foo.com\n    port:\n      name: http-default\n      number: 80\n      protocol: HTTP\n```\n\n## 为workload 添加 Locality 信息\n\nIstio 从如下配置中读取，基于这些配置，我们可以为 Istio 中运行的所有 workload 添加地域属性。\n\n**Kubernetes Node 对象中的地域信息，所有 Pod 自动继承该 Locality 信息**\n\n- region: topology.kubernetes.io/region\n- zone: topology.kubernetes.io/zone\n- ubzone: topology.istio.io/subzone\n\n**Kubernetes Pod 的 istio-locality标签，可覆盖节点 Locality 信息**\n\n- istio-locality: \"region/zone/subzone\"\n\n**WorkloadEntry 的 Locality 属性**\n\n- locality: region/zone/subzone\n\n## 定义基于 Locality 的流量转发规则\n\n**Distribute**\n\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: foo\nspec:\n  host: foo.com\n  trafficPolicy:\n    loadBalancer:\n      localityLbSetting:\n        distribute:\n        - from: \"*/*\"\n          to:\n            region1/zone1/*: 99\n            region2/zone2/*: 1\n        enabled: true\n    outlierDetection:\n      baseEjectionTime: 10s\n      consecutive5xxErrors: 100\n      interval: 10s\n  tls:\n    mode: ISTIO_MUTUAL\n```\n\n**Failover**\n\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: foo\nspec:\n  host: foo.com\n  trafficPolicy:\n    loadBalancer:\n      localityLbSetting:\n        enabled: true\n        failover:\n        - from: region1/zone1\n          to: region2/zone2\n    outlierDetection:\n      baseEjectionTime: 10m\n      consecutive5xxerrors: 1\n      interval: 2s\n  tls:\n    mode: ISTIO_MUTUAL\n```\n\n## 应对规模化集群挑战\n\n**Istio xDS 默认发现集群中所有的配置和服务状态，在超大规模集群中，Istiod或者 Envoy都承受比较大的压力。**\n\n- 集群中的有10000个Service，每个Service开放80和443两个端口，Istio的CDS会discover出20000个EnvoyCluster。\n- 如果开启多集群，Istio 还会为每个cluster创建符合域名规范的集群。\n- Istio 还需要发现remote cluster 中的 Service，Endpoint 和 Pod 信息，而这些信息的频繁变更，会导致网络带宽占用和控制面板的压力都很大。\n\n**meshConfig中控制可见性∶**\n\n```yaml\ndefaultServiceExportTo: \n- \".\"\ndefaultVirtualServiceExportTo:\n- \".\"\ndefaultDestinationRuleExportTo:\n- \".\"\n```\n\n**通过Istio对象中的exportTo属性覆盖默认配置。**\n\n## Istiod 自身的规模控制\n\n**社区新增加了 discoverySelector 的支持，允许Istiod只发现添加了特定 label 的 namespaces下的Istio 以及Kubernetes 对象。**\n\n但因为Kubernetes框架的限制，改功能依然要让 Istiod接收所有配置和状态变更新细，并且在Istiod 中进行对象过滤。在超大集群规模中，并未降低网络带宽占用和Istiod 的处理压力。\n\n需要继续寻求从Kubernetes Server端过滤的解决方案。\n\n## 基于联邦的统一流量模型\n\n- Spec\n\t- Scope\n\t- TrafficTemplate\n\t\t- Istio models\n\t\t- Kubernetes Services\n\t- Override\n\t\t- 可为不同目标集群修改模板属性值\n\t\t- 支持多种override方法\n\t\t\t- jsonPatch\n\t\t\t- mergePatch\n\t- Policy\n\t\t- PlacementPolicy\n\t\t- RolloutPolicy\n\t\t- RuntimePolicy\n- Status\n\t- Conditions\n\t\t- 四层网关状态\n\t\t- 七层网关配置完成度\n\t\t- 证书版本\n\t\t- 网关服务IP和FQDN\n\n![image.png](Assets/image_1667393693694_0.png)\n\n## 统一流量模型-NameService\n\n- Spec\n\t- Global Name FQDN\n\t- TTL\n\t- DNSPolicy\n\t\t- RoundRobin\n\t\t- Locality\n\t\t- Ratio\n\t- HeathCheck Port\n\t- Target\n\t\t- Target Service FQDN\n\t\t- Ratio\n- Status\n\t- Conditions\n\t\t- 域名配置结果\n\t- 配置错误信息\n\n## AccessPoint 控制器\n\n**PcementPolicy** 控制，用户可以选择目标集群来完成流量配置，甚至可以选择关联的**FederatedDeployment** 对象，使得 **AccessPoint** 自动发现目标集群并完成配置。\n\n完成了状态上报，包括网关虚拟**IP**地址，网关 **FQDN**，证书安装状态以及版本信息，路由策略是否配置完成等。这补齐了**Istio** 自身的短板，，使得任何部署在 **Istio** 的应用的网络配置状态一目了然。\n\n发布策略控制，针对多集群的配置，可实现单集群的灰度发布，并且能够自动暂停发布，管理员验证单个集群的变更正确以后，再继续发布。通过此机制，避免因为全局流量变更产生的故障。\n\n不同域名的 **AccessPoint** 可拥有不同的四层网关虚拟**IP**地址，以实现基于**IP**地址的四层网络隔离。\n\n控制器可以基于**AccessPoint** 自动创建 **WorkloadEntry**，并设置 **Locality** 信息。\n\n## 未来展望\n\n- 全面构建基于 Mesh 的流量管理\n- 在用户无感知的前提下将南北流量转成东西流量\n- 数据平面加速Cilium\n\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/APIServer":{"title":"APIServer","content":"\n# API Server\n\nkube-apiserver是Kubernetes最重要的核心组件之一，主要提供以下的功能\n\n提供集群管理的REST API接口，包括认证授权、数据校验以及集群状态变更等\n\n提供其他模块之间的数据交互和通信的枢纽（其他模块通过API Server查询或修改数据，只有API Server才直接操作etcd）\n\n## 访问控制概览\n\nKubernetes API的每个请求都会经过多阶段的访问控制之后才会被接受，这包括认证、授权以及准入控制（Admission Control）等。\n\n![image.png](Assets/image_1665936179908_0.png)\n\n## 访问控制细节\n\n![image.png](Assets/image_1665936203999_0.png)\n\n# 认证\n\n开启TLS时，所有的请求都需要首先认证。Kubernetes支持多种认证机制，并支持同时开启多个认证插件（只要有一个认证通过即可）。如果认证成功，则用户的username会传入授权模块做进一步授权验证；而对于认证失败的请求则返回HTTP 401。\n\n## 认证插件\n\n- X509证书\n\t- 使用X509客户端证书只需要API Server启动时配置--client-ca-file=SOMEFILE。在证书认证时，其CN域用作用户名，而组织机构域则用作group名。\n- 静态Token文件\n\t- 使用静态Token文件认证只需要API Server启动时配置--token-auth-file=SOMEFILE。\n\t- 该文件为csv格式，每行至少包括三列token,username,user id，\n\t\t- `token,user,uid,\"group1,group2,group3”`\n- 引导Token\n\t- 为了支持平滑地启动引导新的集群，Kubernetes 包含了一种动态管理的持有者令牌类型， 称作 启动引导令牌（BootstrapToken）。\n\t- 这些令牌以 Secret 的形式保存在 kube-system 名字空间中，可以被动态管理和创建。\n\t- 控制器管理器包含的 TokenCleaner 控制器能够在启动引导令牌过期时将其删除。\n\t- 在使用kubeadm部署Kubernetes时，可通过kubeadm token list命令查询。\n- 静态密码文件\n\t- 需要API Server启动时配置--basic-auth-file=SOMEFILE，文件格式为csv，每行至少三列password, user, uid，后面是可选的group名\n\t\t- `password,user,uid,\"group1,group2,group3”`\n- ServiceAccount\n\t- ServiceAccount是Kubernetes自动生成的，并会自动挂载到容器的/run/secrets/kubernetes.io/serviceaccount目录中。\n- OpenID\n\t- OAuth 2.0的认证机制\n- Webhook 令牌身份认证\n\t- --authentication-token-webhook-config-file 指向一个配置文件，其中描述 如何访问远程的 Webhook 服务。\n\t- --authentication-token-webhook-cache-ttl 用来设定身份认证决定的缓存时间。 默认时长为 2 分钟。\n- 匿名请求\n\t- 如果使用AlwaysAllow以外的认证模式，则匿名请求默认开启，但可用--anonymous-auth=false禁止匿名请求。\n\n## 基于webhook的认证服务集成\n\n### 构建符合Kubernetes规范的认证服务\n\n需要依照Kubernetes规范，构建认证服务，用来认证tokenreview request构建认证服务\n\n* 认证服务需要满足如下Kubernetes的规范\n* URL： https://authn.example.com/authenticate\n* Method： POST\n* Input:\n* Output:\n\n```json\n{ \"apiVersion\": \"authentication.k8s.io/v1beta1\", \"kind\": \"TokenReview\",\n\"spec\": { \"token\": \"(BEARERTOKEN)\" } }\n\n{\n  \"apiVersion\": \"authentication.k8s.io/v1beta1\",\n  \"kind\": \"TokenReview\",\n   \"status\": {\n    \"authenticated\": true,\n    \"user\": {\n      \"username\": \"janedoe@example.com\",\n      \"uid\": \"42\",\n      \"groups\": [\n        \"developers\",\n        \"qa\"\n      ]}}\n}\n```\n\n### 开发认证服务\n\n解码认证请求\n\n```go\ndecoder := json.NewDecoder(r.Body)\nvar tr authentication.TokenReview\nerr := decoder.Decode(\u0026tr)\nif err != nil {\n\tlog.Println(\"[Error]\", err.Error())\n\tw.WriteHeader(http.StatusBadRequest)\n\tjson.NewEncoder(w).Encode(map[string]interface{}{\n\t\t\"apiVersion\": \"authentication.k8s.io/v1beta1\",\n\t\t\"kind\": \"TokenReview\",\n\t\t\"status\": authentication.TokenReviewStatus{\n\t\t\tAuthenticated: false,\n\t\t},\n\t})\n\treturn\n}\n```\n\n转发认证请求至认证服务器\n\n```go\n// Check User\nts := oauth2.StaticTokenSource(\n\t\u0026oauth2.Token{AccessToken: tr.Spec.Token},\n)\ntc := oauth2.NewClient(oauth2.NoContext, ts)\nclient := github.NewClient(tc)\nuser, _, err := client.Users.Get(context.Background(), \"\")\nif err != nil {\n\tlog.Println(\"[Error]\", err.Error())\n\tw.WriteHeader(http.StatusUnauthorized)\n\tjson.NewEncoder(w).Encode(map[string]interface{}{\n\t\t\"apiVersion\": \"authentication.k8s.io/v1beta1\",\n\t\t\"kind\": \"TokenReview\",\n\t\t\"status\": authentication.TokenReviewStatus{\n\t\tAuthenticated: false,\n\t\t},\n\t})\n\treturn\n}\n```\n\n认证结果返回给APIServer\n\n```go\nw.WriteHeader(http.StatusOK)\ntrs := authentication.TokenReviewStatus{\n\tAuthenticated: true,\n\tUser: authentication.UserInfo{\n\t\tUsername: *user.Login,\n\t\tUID: *user.Login,\n\t},\n}\njson.NewEncoder(w).Encode(map[string]interface{}{\n\t\"apiVersion\": \"authentication.k8s.io/v1beta1\",\n\t\"kind\": \"TokenReview\",\n\t\"status\": trs,\n})\n```\n\n### 配置apiserver\n\n- 可以是任何认证系统\n\t- 但在用户认证完成后，生成代表用户身份的token\n\t- 该token通常是有失效时间的\n\t- 用户获取该token以后以后，将token配置进kubeconfig\n- 修改apiserver设置，开启认证服务，apiserver保证将所有收到的\n- 请求中的token信息，发给认证服务进行验证\n\t- --authentication-token-webhook-config-file，该文件描述如何访问认证服务\n\t- --authentication-token-webhook-cache-ttl，默认2分钟\n- 配置文件需要mount进Pod\n- 配置文件中的服务器地址需要指向authService\n\n```json\n{\n    \"kind\": \"Config\",\n    \"apiVersion\": \"v1\",\n    \"preferences\": {},\n    \"clusters\": [\n        {\n            \"name\": \"github-authn\",\n            \"cluster\": {\n                \"server\": \"http://localhost:3000/authenticate\"\n            }\n        }\n    ],\n    \"users\": [\n        {\n            \"name\": \"authn-apiserver\",\n            \"user\": {\n                \"token\": \"secret\"\n            }\n        }\n    ],\n    \"contexts\": [\n        {\n            \"name\": \"webhook\",\n            \"context\": {\n                \"cluster\": \"github-authn\",\n                \"user\": \"authn-apiserver\"\n            }\n        }\n    ],\n    \"current-context\": \"webhook\"\n}\n```\n\n# 鉴权\n\n## 授权\n\n授权主要是用于对集群资源的访问控制，通过检查请求包含的相关属性值，与相对应的访问策略相比较，API请求必须满足某些策略才能被处理。跟认证类似，Kubernetes也支持多种授权机制，并支持同时开启多个授权插件（只要有一个验证通过即可）。如果授权成功，则用户的请求会发送到准入控制模块做进一步的请求验证；对于授权失败的请求则返回HTTP 403。\n\nKubernetes授权仅处理以下的请求属性：\n\n- user, group, extra\n- API、请求方法（如get、post、update、patch和delete）和请求路径（如/api）\n- 请求资源和子资源\n- Namespace\n- API Group\n\n目前，Kubernetes支持以下授权插件：\n\n- ABAC\n- RBAC\n- Webhook\n- Node\n\n## RBAC vs ABAC\n\nABAC（Attribute Based Access Control）本来是不错的概念，但是在 Kubernetes 中的实现比较难于管理和理解，而且需要对 Master 所在节点的 SSH 和文件系统权限，要使得对授权的变更成功生效，还需要重新启动 API Server。\n\n而 RBAC 的授权策略可以利用 kubectl 或者 Kubernetes API 直接进行配置。RBAC 可以授权给用户，让用户有权进行授权管理，这样就可以无需接触节点，直接进行授权管理。RBAC 在 Kubernetes中被映射为 API 资源和操作。\n\n## RBAC老图\n\n![image.png](Assets/image_1665943450461_0.png)\n\n## RBAC新解\n\n![image.png](Assets/image_1665943481332_0.png)\n\n## Role与ClusterRole\n\nRole（角色）是一系列权限的集合，例如一个角色可以包含读取 Pod 的权限和列出 Pod 的权限。Role只能用来给某个特定namespace中的资源作鉴权，对多namespace和集群级的资源或者是非资源类的API（如/healthz）使用ClusterRole。\n\n![image.png](Assets/image_1665943682167_0.png)\n\n## binding\n\n![image.png](Assets/image_1665943725607_0.png)\n\n## 账户／组的管理\n\n角色绑定（Role Binding）是将角色中定义的权限赋予一个或者一组用户。它包含若干 **主体**（用户、组或服务账户）的列表和对这些主体所获得的角色的引用。\n\n组的概念：\n\n- 当与外部认证系统对接时，用户信息（UserInfo）可包含Group信息，授权可针对用户群组\n- 当对ServiceAccount授权时，Group代表某个Namespace下的所有ServiceAccount\n\n![image.png](Assets/image_1665943926495_0.png)\n\n## 规划系统角色\n\n- User\n\t- 管理员\n\t\t- 所有资源的所有权限？？（不给普通用的secret的读取权限）\n\t- 普通用户\n\t\t- 是否有该用户创建的namespace下的所有object的操作权限？\n\t\t- 对其他用户的namespace资源是否可读，是否可写？\n- ServiceAccount\n\t- ServiceAccount是开发者（kubernetes developer或者domain developer）创建应用后，应用于apiserver通讯需要的身份\n\t- 用户可以创建自定的ServiceAccount，kubernetes也为每个namespace创建default ServiceAccount\n\t- Default ServiceAccount通常需要给定权限以后才能对apiserver做写操作\n\n## 实现方案\n\n- 在cluster创建时，创建自定义的clusterRole，比如namespace-creator\n- Namespace-creator role定义用户可操作的对象和对应的读写操作。\n- 创建自定义的namespace admission control webhook\n\t- 当namespace创建请求被处理时，获取当前用户信息并annotate到namespace\n- 创建RBAC controller\n\t- Watch namespace的创建事件\n\t- 获取当前namespace的创建者信息\n\t- 在当前namespace创建rolebinding对象，并将namespace-creator 角色和用户绑定\n\n## 与权限相关的其他最佳实践\n\n- ClusterRole是非namespace绑定的，针对整个集群生效\n- 通常需要创建一个管理员角色，并且绑定给开发运营团队成员\n- CustomResourceDefinition是全局资源，普通用户创建CustomResourceDefinition以后，需要管理员授予相应权限后才能真正操作该对象\n- 针对所有的角色管理，建议创建spec，用源代码驱动\n\t- 虽然可以通过edit操作来修改权限，但后期会导致权限管理混乱，可能会有很多临时创建出来的角色和角色绑定对象，重复绑定某一个资源权限\n- 权限是可以传递的，用户A可以将其对某对象的某操作，抽取成一个权限，并赋给用户B\n- 防止海量的角色和角色绑定对象，因为大量的对象会导致鉴权效率低，同时给apiserver增加负担\n- ServiceAccount也需要授权的，否则你的component可能无法操作某对象\n- Tips：SSH到master节点通过insecure port访问apiserver可绕过鉴权，当需要做管理操作又没有权限时可以使用（不推荐）\n\n# 准入\n\n## 准入控制\n\n- 为资源增加自定义属性\n- 作为多租户集群方案中的一环，我们需要在namespace的准入控制中，获取用户信息，并将用户信息更新的namespace的annotation\n- 有当namespace中有有效用户信息时，我们才可以在namespace创建时，自动绑定用户权限，namespace才可用。\n\n---\n\n- 配额管理\n\t- 原因：资源有限，如何限定某个用户有多少资源？\n- 方案：\n\t- 预定义每个Namespace的ResourceQuota，并把spec保存为configmap\n\t\t- 用户可以创建多少个Pod\n\t\t\t- BestEffortPod\n\t\t\t- QoSPod\n\t\t- 用户可以创建多少个service\n\t\t- 用户可以创建多少个ingress\n\t\t- 用户可以创建多少个service VIP\n\t- 创建ResourceQuota Controller\n\t\t- 监控namespace创建事件，当namespace创建时，在该namespace创建对应的ResourceQuota 对象\n\t- apiserver中开启ResourceQuota的admission plugin\n\n---\n\n准入控制（Admission Control）在授权后对请求做进一步的验证或添加默认参数。不同于授权和认证只关心请求的用户和操作，准入控制还处理请求的内容，并且仅对创建、更新、删除或连接（如代理）等有效，而对读操作无效。\n\n准入控制支持同时开启多个插件，它们依次调用，只有全部插件都通过的请求才可以放过进入系统。\n\n## 准入控制插件\n\n- AlwaysAdmit: 接受所有请求。\n- AlwaysPullImages: 总是拉取最新镜像。在多租户场景下非常有用。\n- DenyEscalatingExec: 禁止特权容器的exec和attach操作。\n- ImagePolicyWebhook: 通过webhook决定image策略，需要同时配置--admission-control-config-file\n- ServiceAccount：自动创建默认ServiceAccount，并确保Pod引用的ServiceAccount已经存在\n- SecurityContextDeny：拒绝包含非法SecurityContext配置的容器\n- ResourceQuota：限制Pod的请求不会超过配额，需要在namespace中创建一个ResourceQuota对象\n- LimitRanger：为Pod设置默认资源请求和限制，需要在namespace中创建一个LimitRange对象\n- InitialResources：根据镜像的历史使用记录，为容器设置默认资源请求和限制\n- NamespaceLifecycle：确保处于termination状态的namespace不再接收新的对象创建请求，并拒绝请求不存在的namespace\n- DefaultStorageClass：为PVC设置默认StorageClass\n- DefaultTolerationSeconds：设置Pod的默认forgiveness toleration为5分钟\n- PodSecurityPolicy：使用Pod Security Policies时必须开启\n- NodeRestriction：限制kubelet仅可访问node、endpoint、pod、service以及secret、configmap、PV和PVC等相关的资源\n\n## 准入控制插件的开发\n\n除默认的准入控制插件以外，Kubernetes预留了准入控制插件的扩展点，用户可自定义准入控制插件实现自定义准入功能\n\nMutatingWebhookConfiguration：变形插件，支持对准入对象的修改\n\nValidatingWebhookConfiguration：校验插件，只能对准入对象合法性进行校验，不能修改\n\n![image.png](Assets/image_1665998764708_0.png)\n\n```yaml\n# {{if eq .k8snode_validating \"enabled\"}}\napiVersion: admissionregistration.k8s.io/v1beta1\nkind: MutatingWebhookConfiguration\nmetadata:\n  name: ns-mutating.webhook.k8s.io\nwebhooks:\n- clientConfig:\n  caBundle: {{.serverca_base64}}\n  url: https://admission.local.tess.io/apis/admission.k8s.io/v1alpha1/ ns-mutating\nfailurePolicy: Fail\nname: ns-mutating.webhook.k8s.io\nnamespaceSelector: {}\nrules:\n  - apiGroups:\n    - \"\"\n    apiVersions:\n      - '*'\n    operations:\n      - CREATE\n    resources:\n      - nodes\nsideEffects: Unknown\n# {{end}}\n```\n\n# 限流\n\n## 计数器固定窗口算法\n\n原理就是对一段固定时间窗口内的请求进行计数，如果请求数超过了阈值，则舍弃该请求；\n如果没有达到设定的阈值，则接受该请求，且计数加1。\n\n当时间窗口结束时，重置计数器为0。\n\n![image.png](Assets/image_1666013649334_0.png)\n\n## 计数器滑动窗口算法\n\n在固定窗口的基础上，将一个计时窗口分成了若干个小窗口，然后每个小窗口维护一个独立的计数器。\n\n当请求的时间大于当前窗口的最大时间时，则将计时窗口向前平移一个小窗口。\n\n平移时，将第一个小窗口的数据丢弃，然后将第二个小窗口设置为第一个小窗口，同时在最后面新增一个小窗口，将新的请求放在新增的小窗口中。\n\n同时要保证整个窗口中所有小窗口的请求数目之后不能超过设定的阈值。\n\n![image.png](Assets/image_1666013698808_0.png)\n\n## 漏斗算法\n\n漏斗算法的原理也很容易理解。请求来了之后会首先进到漏斗里，然后漏斗以恒定的速率将请求流出进行处理，从而起到平滑流量的作用。\n\n当请求的流量过大时，漏斗达到最大容量时会溢出，此时请求被丢弃。\n\n在系统看来，请求永远是以平滑的传输速率过来，从而起到了保护系统的作用。\n\n![image.png](Assets/image_1666013753347_0.png)\n\n## 令牌桶算法\n\n令牌桶算法是对漏斗算法的一种改进，除了能够起到限流的作用外，还允许一定程度的流量突发。\n\n在令牌桶算法中，存在一个令牌桶，算法中存在一种机制以恒定的速率向令牌桶中放入令牌。令牌桶也有一定的容量，如果满了令牌就无法放进去了。\n\n当请求来时，会首先到令牌桶中去拿令牌，如果拿到了令牌，则该请求会被处理，并消耗掉拿到的令牌；\n\n如果令牌桶为空，则该请求会被丢弃。\n\n![image.png](Assets/image_1666013871935_0.png)\n\n## 传统限流方法的局限性\n\n- 粒度粗\n\t- 无法为不同用户，不同场景设置不通的限流\n- 单队列\n\t- 共享限流窗口/桶，一个坏用户可能会将整个系统堵塞，其他正常用户的请求无法被及时处理\n- 不公平\n\t- 正常用户的请求会被排到队尾，无法及时处理而饿死\n- 无优先级\n\t- 重要的系统指令一并被限流，系统故障难以恢复\n\n## APIServer中的限流\n\n- max-requests-inflight： 在给定时间内的最大 non-mutating 请求数\n- max-mutating-requests-inflight： 在给定时间内的最大 mutating 请求数，调整 apiserver 的流控 qos\n- 代码\n\t- staging/src/k8s.io/apiserver/pkg/server/filters/maxinflight.go:WithMaxInFlightLimit()\n\n![image.png](Assets/image_1666014967707_0.png)\n\n## API Priority and Fairness\n\n- APF 以更细粒度的方式对请求进行分类和隔离。\n- 它还引入了空间有限的排队机制，因此在非常短暂的突发情况下，API 服务器不会拒绝任何请求。\n- 通过使用公平排队技术从队列中分发请求，这样， 一个行为不佳的控制器就不会饿死其他控制器（即使优先级相同）。\n- APF的核心\n\t- 多等级\n\t- 多队列\n\n![image.png](Assets/image_1666015880281_0.png)\n\n---\n\nAPF 的实现依赖两个非常重要的资源 FlowSchema, PriorityLevelConfiguration\n\nAPF 对请求进行更细粒度的分类，每一个请求分类对应一个 FlowSchema (FS)\n\nFS 内的请求又会根据 distinguisher 进一步划分为不同的 Flow.\n\nFS 会设置一个优先级 (Priority Level, PL)，不同优先级的并发资源是隔离的。所以不同优先级的资源不会相互排挤。特定\n\n优先级的请求可以被高优处理。\n\n一个 PL 可以对应多个 FS，PL 中维护了一个 QueueSet，用于缓存不能及时处理的请求，请求不会因为超出 PL 的并发限制而被丢弃。\n\nFS 中的每个 Flow 通过 shuffle sharding 算法从 QueueSet 选取特定的 queues 缓存请求。\n\n每次从 QueueSet 中取请求执行时，会先应用 fair queuing 算法从 QueueSet 中选中一个 queue，然后从这个 queue中取出 oldest 请求执行。所以即使是同一个 PL 内的请求，也不会出现一个 Flow 内的请求一直占用资源的不公平现象。\n\n### 概念\n\n传入的请求通过 *FlowSchema* 按照其属性分类，并分配优先级。\n\n每个优先级维护自定义的并发限制，加强了隔离度，这样不同优先级的请求，就不会相互饿死。\n\n在同一个优先级内，公平排队算法可以防止来自不同 *flow* 的请求相互饿死。\n\n该算法将请求排队，通过排队机制，防止在平均负载较低时，通信量突增而导致请求失败。\n\n### 优先级\n\n如果未启用 APF，API 服务器中的整体并发量将受到 kube-apiserver 的参数 --max\u0002requests-inflight 和 --max-mutating-requests-inflight 的限制。\n\n启用 APF 后，将对这些参数定义的并发限制进行求和，然后将总和分配到一组可配置的 *优先级* 中。 每个传入的请求都会分配一个优先级；\n\n每个优先级都有各自的配置，设定允许分发的并发请求数。\n\n例如，默认配置包括针对领导者选举请求、内置控制器请求和 Pod 请求都单独设置优先级。这表示即使异常的 Pod 向 API 服务器发送大量请求，也无法阻止领导者选举或内置控制器的操作执行成功。\n\n### 排队\n\n即使在同一优先级内，也可能存在大量不同的流量源。\n\n在过载情况下，防止一个请求流饿死其他流是非常有价值的 （尤其是在一个较为常见的场景中，一个有故障的客户端会疯狂地向 kube-apiserver 发送请求， 理想情况下，这个有故障的客户端不应对其他客户端产生太大的影响）。\n\n公平排队算法在处理具有相同优先级的请求时，实现了上述场景。\n\n每个请求都被分配到某个 流 中，该流由对应的 FlowSchema 的名字加上一个 流区分项（FlowDistinguisher） 来标识。\n\n这里的流区分项可以是发出请求的用户、目标资源的名称空间或什么都不是。\n\n系统尝试为不同流中具有相同优先级的请求赋予近似相等的权重。\n\n将请求划分到流中之后，APF 功能将请求分配到队列中。\n\n分配时使用一种称为 混洗分片（Shuffle-Sharding） 的技术。 该技术可以相对有效地利用队列隔离低强度流与高强度流。\n\n排队算法的细节可针对每个优先等级进行调整，并允许管理员在内存占用、 公平性（当总流量超标时，各个独立的流将都会取得进展）、 突发流量的容忍度以及排队引发的额外延迟之间进行权衡。\n\n### 豁免请求\n\n某些特别重要的请求不受制于此特性施加的任何限制。这些豁免可防止不当的流控配置完全禁用\nAPI 服务器。\n\n### 默认配置\n\nsystem\n\n用于 system:nodes 组（即 kubelets）的请求； kubelets 必须能连上 API 服务器，以便工作负载能够调度到其上。\n\nleader-election\n\n用于内置控制器的领导选举的请求 （特别是来自 kube-system 名称空间中 system:kube-controller-manager 和 system:kube-scheduler 用户和服务账号，针对 endpoints、configmaps 或 leases 的请求）。\n\n将这些请求与其他流量相隔离非常重要，因为领导者选举失败会导致控制器发生故障并重新启动，这反过来会导致新启动的控制器在同步信息时，流量开销更大。\n\nworkload-high\n\n优先级用于内置控制器的请求。\n\nworkload-low\n\n优先级适用于来自任何服务帐户的请求，通常包括来自 Pods 中运行的控制器的所有请求。\n\nglobal-default\n\n优先级可处理所有其他流量，例如：非特权用户运行的交互式 kubectl 命令。\n\nexempt\n\n优先级的请求完全不受流控限制：它们总是立刻被分发。 特殊的 exempt FlowSchema把 system:masters 组的所有请求都归入该优先级组。\n\ncatch-all\n\n优先级与特殊的 catch-all FlowSchema 结合使用，以确保每个请求都分类。\n\n一般不应该依赖于 catch-all 的配置，而应适当地创建自己的 catch-all FlowSchema 和PriorityLevelConfigurations（或使用默认安装的 global-default 配置）。\n\n为了帮助捕获部分请求未分类的配置错误，强制要求 catch-all 优先级仅允许5个并发份额，并且不对请求进行排队，使得仅与 catch-all FlowSchema 匹配的流量被拒绝的可能性更高，并显示 HTTP 429 错误。\n\n### PriorityLevelConfiguration\n\n一个 PriorityLevelConfiguration 表示单个隔离类型。\n\n每个 PriorityLevelConfigurations 对未完成的请求数有各自的限制，对排队中的请求数也有限制。\n\n![image.png](Assets/image_1666017137479_0.png)\n\n### FlowSchema\n\nFlowSchema 匹配一些入站请求，并将它们分配给优先级。\n\n每个入站请求都会对所有 FlowSchema 测试是否匹配， 首先从 matchingPrecedence 数值最低的匹配开始（我们认为这是逻辑上匹配度最高）， 然后依次进行，直到首个匹配出现\n\n![image.png](Assets/image_1666017081344_0.png)\n\n### 调试\n\n- /debug/api_priority_and_fairness/dump_priority_levels —— 所有优先级及其当前状态的列表\n\t- kubectl get --raw /debug/api_priority_and_fairness/dump_priority_levels\n- /debug/api_priority_and_fairness/dump_queues —— 所有队列及其当前状态的列表\n\t- kubectl get --raw /debug/api_priority_and_fairness/dump_queues\n- /debug/api_priority_and_fairness/dump_requests ——当前正在队列中等待的所有请求的列表\n\t- kubectl get --raw /debug/api_priority_and_fairness/dump_requests\n\n# APIServer对象的实现\n\n## 构建高可用的多副本apiserver\n\n- apiserver是无状态的Rest Server\n- 无状态所以方便Scale Up／down\n- 负载均衡\n\t- 在多个apiserver实例之上，配置负载均衡\n\t- 证书可能需要加上Loadbalancer VIP重新生成\n\n## 预留充足的CPU、内存资源\n\n随着集群中节点数量不断增多，APIServer对CPU和内存的开销也不断增大。过少的CPU资源会降低其处理效率，过少的内存资源会导致Pod被OOMKilled，直接导致服务不可用。在规划APIServer资源时，不能仅看当下需求，也要为未来预留充分。\n\n## 善用速率限制（RateLimit）\n\nAPIServer的参数“--max-requests-inflight”和“--max-mutating-requests-inflight”支持在给定时间内限制并行处理读请求（包括Get、List和Watch操作）和写请求（包括Create、Delete、Update和Patch操作）的最大数量。当APIServer接收到的请求超过这两个参数设定的值时，再接收到的请求将会被直接拒绝。通过速率限制机制，可以有效地控制APIServer内存的使用。如果该值配置过低，会经常出现请求超过限制的错误，如果配置过高，则APIServer可能会因为占用过多内存而被强制终止，因此需要根据实际的运行环境，结合实时用户请求数量和APIServer的资源配置进行调优。\n\n客户端在接收到拒绝请求的返回值后，应等待一段时间再发起重试，无间隔的重试会加重APIServer的压力，导致性能进一步降低。针对并行处理请求数的过滤颗粒度太大，在请求数量比较多的场景，重要的消息可能会被拒绝掉，自1.18版本开始，社区引入了优先级和公平保证（Priority and Fairness）功能，以提供更细粒度地客户端请求控制。该功能支持将不同用户或不同类型的请求进行优先级归类，保证高优先级的请求总是能够更快得到处理，从而不受低优先级请求的影响。\n\n## 设置合适的缓存大小\n\nAPIServer与etcd之间基于gRPC协议进行通信，gRPC协议保证了二者在大规模集群中的数据高速传输。gRPC基于连接复用的HTTP/2协议，即针对相同分组的对象，APIServer和etcd之间共享相同的TCP连接，不同请求由不同的stream传输。\n\n一个HTTP/2连接有其stream配额 ，配额的大小限制了能支持的并发请求。APIServer提供了集群对象的缓存机制，当客户端发起查询请求时，APIServer默认会将其缓存直接返回给客户端。缓存区大小可以通过参数“--watch-cache-sizes”设置。针对访问请求比较多的对象，适当设置缓存的大小，极大降低对etcd的访问频率，节省了网络调用，降低了对etcd集群的读写压力，从而提高对象访问的性能。\n\n但是APIServer也是允许客户端忽略缓存的，例如客户端请求中ListOption中没有设置resourceVersion，这时APIServer直接从etcd拉取最新数据返回给客户端。客户端应尽量避免此操作，应在ListOption中设置resourceVersion为0，APIServer则将从缓存里面读取数据，而不会直接访问etcd。\n\n## 客户端尽量使用长连接\n\n当查询请求的返回数据较大且此类请求并发量较大时，容易引发TCP链路的阻塞，导致其他查询操作超时。因此基于Kubernetes开发组件时，例如某些DaemonSet和Controller，如果要查询某类对象，应尽量通过长连接ListWatch监听对象变更，避免全量从APIServer获取资源。如果在同一应用程序中，如果有多个Informer监听APIServer资源变化，可以将这些Informer合并，减少和APIServer的长连接数，从而降低对APIServer的压力。\n\n## 搭建多租户的Kubernetes集群\n\n- 授信\n\t- 认证：\n\t\t- 禁止匿名访问，只允许可信用户做操作。\n\t- 授权：\n\t\t- 基于授信的操作，防止多用户之间互相影响，比如普通用户删除Kubernetes核心服务，或者A用户删除或修改B用户的应用。\n- 隔离\n\t- 可见行隔离：\n\t\t- 用户只关心自己的应用，无需看到其他用户的服务和部署。\n\t- 资源隔离：\n\t\t- 有些关键项目对资源需求较高，需要专有设备，不与其他人共享。\n\t- 应用访问隔离：\n\t\t- 用户创建的服务，按既定规则允许其他用户访问。\n- 资源管理\n\t- Quota管理\n\t\t- 谁能用多少资源？\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/CNI":{"title":"CNI","content":"\n# CNI\n\nKubernetes 网络模型设计的基础原则是：\n\n- 所有的 Pod 能够不通过 NAT就能相互访问。\n- 所有的节点能够不通过NAT就能相互访问。\n- 容器内看见的 IP地址和外部组件看到的容器 IP是一样的。\n \nKubernetes 的集群里，IP地址是以 Pod为单位进行分配的，每个 Pod都拥有一个独立的 IP地址。一个Pod内部的所有容器共享一个网络栈，即宿主机上的一个网络命名空间，包括它们的IP 地址、网络设备、配置等都是共享的。也就是说，Pod 里面的所有容器能通过localhost:port来连接对方。在Kubernetes中，提供了一个轻量的通用容器网络接口 CNI（Container Network Interface），专门用于设置和删除容器的网络连通性。容器运行时通过 CNI调用网络插件来完成容器的网络设置。\n\n# CNI插件分类和常见插件\n\n- IPAM：IP地址分配\n- 主插件：网卡设置\n- bridge：创建一个网桥，并把主机端口和容器端口插入网桥\n- ipvlan：为容器添加ipvlan网口\n- loopback：设置loopback网口\n- Meta：附加功能\n- portmap：设置主机端口和容器端口映射\n- bandwidth：利用Linux Traffic Control限流\n- firewall：通过iptables或firewalld为容器设置防火墙规则\n\nhttps://ithub.com/containernetworking/plugins\n\n# CNI 插件运行机\n\n容器运行时在启动时会从CNI 的配置目录中读取JSON 格式的配置文件，文件后缀为\".conf\"\".conflist\"\".json\"。如果配置目录中包含多个文件，一般情况下，会以名字排序选用第一个配置文件作为默认的网络配置，并加载获取其中指定的CNI插件名称和配置参数。\n\n![image.png](Assets/image_1666107209591_0.png)\n\n# CNI 的运行机制\n\n关于容器网络管理，容器运行时一般需要配置两个参数--cni-bin-dir 和--cni-conf-dir。有一种特殊情况，kubelet 内置的 Docker 作为容器运行时，是由kubelet来查找CNI插件的，运行插件来为容器设置网络，这两个参数应该配置在 kubelet 处：\n\n- cni-bin-dir：网络插件的可执行文件所在目录。默认是/opt/cni/bin。\n- cni-conf-dir：网络插件的配置文件所在目录。默认是/etc/cni/net.d。\n\n# CNI插件设计考量\n\n容器运行时必须在调用任何插件之前为容器创建一个新的网络命名空间。\n\n容器运行时必须决定这个容器属于哪些网络，针对每个网络，哪些插件必须要执行。\n\n容器运行时必须加载配置文件，并确定设置网络时哪些插件必须被执行。\n\n网络配置采用 JSON 格式，可以很容易地存储在文件中。\n\n容器运行时必须按顺序执行配置文件里相应的插件。\n\n在完成容器生命周期后，容器运行时必须按照与执行添加容器相反的顺序执行插件，以便将容器与网络断开连接。\n\n容器运行时被同一容器调用时不能并行操作，但被不同的容器调用时，允许并行操作。\n\n容器运行时针对一个容器必须按顺序执行ADD 和 DEL操作，ADD后面总是跟着相应的 DEL。 DEL 可能跟着额外的 DEL，插件应该允许处理多个DEL。\n\n容器必须由 ContainerIlD 来唯一标识，需要存储状态的插件需要使用网络名称容器ID和网络接口组成的主 ke用于索引。\n\n容器运行时针对同一个网络、同一个容器、同一个网络接口，不能连续调用两次ADD命令。\n\n# 打通主机层网络\n\nCNI插件外，Kubernetes 还需要标准的 CNI插件 lo，最低版本为 0.2.0版本。网络插件除支持设置和清理 Pod 网络接口外，该插件还需要支持Iptables。如果Kube-proxy 工作在 Iptables 模式，网络插件需要确保容器流量能使用Iptables 转发。例如，如果网络插件将容器连接到Linux 网桥，必须将net/bridge/bridge-nf-call-iptables参数 sysctl设置为1，网桥上数据包将遍历 Iptables 规则。如果插件不使用Linux桥接器（而是类似 Open vSwitch 或其他某种机制的插件），则应确保容器流量被正确设置了路由。\n\n# CNI Plugin\n\nContainerNetworking组维护了一些CNI插件，包括网络接口创建的 bridge、ipvlan、loopback、macvlan、ptp、host-device 等，IP地址分配的 DHCP、host-local和 static，其他的 Flannel，tunning、portmap、firewall 等。\n\n社区还有些第三方网络策略方面的插件，例如Calico、Cilium 和 Weave 等。可用选项的多样性意味着大多数用户将能够找到适合其当前需求和部署环境的 CNI插件，并在情况变化时迅捷转换解决方案。\n\n# Flannel\n\nFlannel是由CoreOS开发的项目，是CNI插件早期的入门产品，简单易用。\n\nFlannel使用Kubernetes集群的现有etcd集群来存储其状态信息，从而不必提供专用的数据存储，只需要在每个节点上运行flanneld来守护进程。\n\n每个节点都被分配一个子网，为该节点上的Pod分配IP地址。\n\n同一主机内的Pod可以使用网桥进行通信，而不同主机上的Pod将通过flanneld将其流量封装在UDP数据包中，以路由到适当的目的地。\n\n封装方式默认和推荐的方法是使用VxLAN，因为它具有良好的性能，并且比其他选项要少些人为干预。虽然使用VXLAN之类的技术封装的解决方案效果很好，但缺点就是该过程使流量跟踪变得困难。\n\n![image.png](Assets/image_1666108009446_0.png)\n\n# Calico\n\nCalico 以其性能、灵活性和网络策略而闻名，不仅涉及在主机和 Pod 之间提供网络连接，而且还涉及网络安全性和策略管理。\n\n对干同网段通信，基于第 3层，Calico 使用 BGP路由协议在主机之间路由数据包，使用BGP路由协议也意味着数据包在主机之间移动时不需要包装在额外的封装层中。\n\n对于跨网段通信，基于IPinIP 使用虚拟网卡设备 tunlO，用一个IP数据包封装另一个IP数据包，外层IP数据包头的源地址为隧道入口设备的 IP地址，目标地址为隧道出口设备的IP地址。\n\n网络策略是 Calico 最受欢迎的功能之一，使用 ACLs协议和 kube-proxy来创建 iptables 过滤规则，从而实现隔离容器网络的目的。\n\n此外，Calico 还可以与服务网格 Istio集成，在服务网格层和网络基础结构层上解释和实施集群中工作负载的策略。\n\n这意味着您可以配置功能强大的规则，以描述 Pod 应该如何发送和接收流量，提高安全性及加强对网络环境的控制。\n\nCalico 属于完全分布式的横向扩展结构，允许开发人员和管理员快速和平稳地扩展部署规模。对于性能和功能（如网络策略）要求高的环境，CalicO 是一个不错选择。\n\n# Calico组件（IPinIP）\n\n![image.png](Assets/image_1666110657777_0.png)\n\n# Calico初始化\n\n配置和CNI二进制文件由initContainer推送\n\n```yaml\n- command:\n  - /opt/cni/bin/install\n  env:\n  - name: CNI_CONF_NAME\n    value: 10-calico.conflist\n  - name: SLEEP\n    value: \"false\"\n  - name: CNI_NET_DIR\n    value: /etc/cni/net.d\n  - name: CNI_NETWORK_CONFIG\n    valueFrom:\n      configMapKeyRef:\n        key: config\n        name: cni-config\n  - name: KUBERNETES_SERVICE_HOST\n    value: 10.96.0.1\n  - name: KUBERNETES_SERVICE_PORT\n    value: \"443\"\n  image: docker.io/calico/cni:v3.20.1\n  imagePullPolicy: IfNotPresent\n  name: install-cni\n```\n\n# Calico配置一览\n\n```yaml\n{\n    \"name\": \"k8s-pod-network\",\n    \"cniVersion\": \"0.3.1\",\n    \"plugins\": [\n        {\n            \"type\": \"calico\",\n            \"datastore_type\": \"kubernetes\",\n            \"mtu\": 0,\n            \"nodename_file_optional\": false,\n            \"log_level\": \"Info\",\n            \"log_file_path\": \"/var/log/calico/cni/cni.log\",\n            \"ipam\": {\n                \"type\": \"calico-ipam\",\n                \"assign_ipv4\": \"true\",\n                \"assign_ipv6\": \"false\"\n            },\n            \"container_settings\": {\n                \"allow_ip_forwarding\": false\n            },\n            \"policy\": {\n                \"type\": \"k8s\"\n            },\n            \"kubernetes\": {\n                \"k8s_api_root\": \"https://10.96.0.1:443\",\n                \"kubeconfig\": \"/etc/cni/net.d/calico-kubeconfig\"\n            }\n        },\n        {\n            \"type\": \"bandwidth\",\n            \"capabilities\": {\n                \"bandwidth\": true\n            }\n        },\n        {\n            \"type\": \"portmap\",\n            \"snat\": true,\n            \"capabilities\": {\n                \"portMappings\": true\n            }\n        }\n    ]\n}\n```\n\n# Calico VXLan\n\n![image.png](Assets/image_1666111513662_0.png)\n\n# IPPool\n\nIPPool用来定义一个集群的预定义IP段\n\n```yaml\napiVersion: crd.projectcalico.org/v1\nkind: IPPool\nmetadata:\n  name: default-ipv4-ippool\nSpec:\n  blockSize: 26\n  cidr: 192.168.0.0/16\n  lipipMode: Never\n  natOutgoing: true\n  nodeSelector: all()\n  vxlanMode: CrossSubnet\n```\n\n# IPAMBlock\n\nIPAMBlock用来定义每个主机预分配的IP段\n\n```yaml\napiVersion: crd.projectcalico.org/v1\nkind: IPAMBlock\nmetadata:\n  annotations:\n  name: 192-168-119-64-26\nSpec:\n  affinity: host:cadmin\n  allocations:\n  - null\n  - 0\n  - null\n  - 1\n  - 2\n  - 3\nattributes:\n  - handle_id: vxlan-tunnel-addr-cadmin\n    secondary:\n      node: cadmin\n      type: vxlanTunnelAddress\n  - handle_id: k8s-pod-network.6680d3883d6150e75ffbd031f86c689a97a5be0f260c6442b2b567c2ca40\n    secondary:\n      namespace: calico-apiserver node:cadmin\n      pod: calico-apiserver-77dffffcdf-g2tcx\n      timestamp: 2021-09-3009:46:57.45651816+0000UTC\n  - handle_id: k8s-pod-network.b10d7702bf334fc55a5e399a731ab3201ea990ale3bc79894abdd712646699\n    secondary:\n      namespace: calico-system\n      node: cadmin\n      pod: calico-kube-controllers-bdd5f97c5-554z5\n      timestamp: 2021-09-3009:46:57.502351346+0000 UTC\n```\n\n# IPAMHandle\n\nIPAMHandle用来记录IP分配的具体细节\n\n```yaml\napiVersion: crd.projectcalico.org/v1\nkind: IPAMHandle\nmetadata:\n  name: k8s-pod-network.8d75b941d85c4998016b72c83f9c5a75512c82c052357daf0ec8e67365635d93\nSpec:\n  block:\n    192.168.119.64/26:1\n  deleted: false\n  handlelD: k8s-pod-network.8d75b941d85c4998016b72c83f9c5a75512c82c052357daf0ec8e67365635d93\n```\n\n# 创建Pod并查看IP配置情况\n\n![image.png](Assets/image_1666113174345_0.png)\n\n# CNI plugin的对比\n\n![image.png](Assets/image_1666113196669_0.png)\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/CRI":{"title":"CRI","content":"\n# CRI\n\n容器运行时（Container Runtime），运行于Kubernetes（k8s）集群的每个节点中，负责容器的整个生命周期。其中 Docker 是目前应用最广的。随着容器云的发展，越来越多的容器运行时涌现。为了解决这些容器运行时和 Kubernetes 的集成问题，在 Kubernetes1.5版本中，社区推出了CRI（Container Runtime Interface，容器运行时接口）以支持更多的容器运行时。\n\n![image.png](Assets/image_1666097629440_0.png)\n\n---\n\nCRI是 Kubernetes定义的一组 gRPC服务。kubelet作为客户端，基于gRPC框架，通过 Socket 和容器运行时通信。 它包括两类服务∶镜像服务（Image Service）和运行时服务（Runtime Service）。 镜像服务提供下载、检查和删除镜像的远程程序调用。 运行时服务 包含用于管理容器生命周期，以及与容器交互的调用（exeC/ attach/ port-forward）的远程程序调用。\n\n![image.png](Assets/image_1666097676671_0.png)\n\n# 运行时的层级\n\nDockershim，containerd和 CRI-O 都是遵循CRI 的容器运行时，我们称他们为 **高层级运行时(High-level Runtime)**。\n\nOCI（Open Container Initiative，开放容器计划）定义了创建容器的格式和运行时的开源行业标准，包括镜像规范（Image Specification）和运行时规范（Runtime Specification）。\n\n镜像规范定义了 OCI 镜像的标准。高层级运行时将会下载一个OCl镜像，并把它解压成OCl 运行时文件系统包（filesystem bundle）。\n\n运行时规范则描述了如何从 OCI 运行时文件系统包运行容器程序，并且定义它的配置、运行环境和生命周期。如何为新容器设置命名空间（namepsaces）和控制组（cgroups），以及挂载根文件系统等等操作，都是在这里定义的。它的一个参考实现是 runC。我们称其为 **低层级运行时（Low-levelRuntime）**。除runC以外，也有很多其他的运行时遵循OCI标准，例如 kata-runtime.。\n\n# CRI\n\n容器运行时是真正起删和管理容器的组件。容器运行时可以分为高层和低层的运行时。高层运行时主要包括 Docker，containerd和 CRI-O，低层的运行时，包含了runc，kata，以及 gVisor。低层运行时kata 和 gVisor都还处于小规模落地或者实验阶段，其生态成熟度和使用案例都比较欠缺，所以除非有特殊的需求，否则 runc 几平是必然的选择。因此在对容器运行时的选择上，主要是聚焦于上层运行时的选择。\n\nDocker 内部关于容器运行时功能的核心组件是 containerd，后来 containerd 也可直接和 kubelet 通过 CRI 对接，独立在 Kubernetes中使用。相对于Docker 而言，containerd减少了Docker 所需的处理模块 Dockerd 和Docker-shim，并且对 Docker 支持的存储驱动进行了优化，因此在容器的创建启动停止和删除，以及对镜像的拉取上，都具有性能上的优势。架构的简化同时也带来了维护的便利。当然 Docker 也具有很多 containerd 不具有的功能，例如支持 zfs 存储驱动，支持对日志的大小和文件限制，在以overlayfs2做存储驱动的情况下，可以通过xfs quota来对容器的可写层进行大小限制等。尽管如此，containerd 目前也基本上能够满足容器的众多管理需求，所以将它作为运行时的也越来越多。\n\nKubelet和运行时的关系：\n\n![image.png](Assets/image_1666097993198_0.png)\n\n---\n\n![image.png](Assets/image_1666098015943_0.png)\n\n# 开源运行时的比较\n\nDocker 的多层封装和调用，导致其在可维护性上略逊一筹，增加了线上问题的定位难度;几乎除了重启Docker，我们就毫无他法了。\n\ncontainerd和 CRI-O 的方案比起 Docker 简洁很多。\n\n![image.png](Assets/image_1666098269820_0.png)\n\n# Docker和containerd的差异细节\n\n![image.png](Assets/image_1666098300776_0.png)\n\n# 多种运行时性能比较\n\ncontainerd在各个方面都表现良好，除了启动容器这项。从总用时来看，containerd 的用时还是要比CRI-O 要短的\n\n![image.png](Assets/image_1666098442449_0.png)\n\n# 运行时优劣对比\n\n功能性来讲，containerd和CRI-O都符合CRI和 OCI的标准；\n\n在稳定性上，containerd 略胜一筹；\n\n从性能上讲，containerd胜出。\n\n![image.png](Assets/image_1666098492945_0.png)\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/CSI":{"title":"CSI","content":"\n# 容器运行时存储\n\n除外挂存储卷外，容器启动后，运行时所需文件系统性能直接影响容器性能；\n\n早期的 Docker 采用 Device Mapper 作为容器运行时存储驱动，因为 OverlayFS尚未合并进Kernel；\n\n目前 Docker 和 containerd都默认以OverlayFS 作为运行时存储驱动；\n\nOverlayFS目前已经有非常好的性能，与DeviceMapper 相比优 20%，与操作主机文件性能几乎一致\n\n# 存储卷插件管理\n\nKubernetes支持以插件的形式来实现对不同存储的支持和扩展，这些扩展基于如下三种方式：\n\n![image.png](Assets/image_1666114625255_0.png)\n\n# out-of-tree CSI插件\n\nCSI 通过 RPC与存储驱动进行交互。\n\n在设计 CSI的时候，Kubernetes 对CSI存储驱动的打包和部署要求很少，主要定义了Kubernetes 的两个相关模块：\n\n- **kube-controller-manager：**\n\t- kube-controller-manager 模块用于感知CSI驱动存在。\n\t- Kubernetes 的主控模块通过Unix domain socket（而不是CSI驱动）或者其他方式进行直接地交互。\n\t- Kubernetes 的主控模块只与 Kubernetes相关的 API进行交互。\n\t- 因此CSI驱动若有依赖于Kubernetes API 的操作，例如卷的创建、卷的 attach、卷的快照等，需要在 CSI驱动里面通过Kubernetes 的 API，来触发相关的CSI操作。\n- **kubelet：**\n\t- kubelet模块用于与 CSI驱动进行交互。\n\t- kubelet通过 Unix domain socket 向 CSI驱动发起 CSI 调用（如 NodeStageVolume、NodePublishVolume 等），再发起 mount 卷和 umount卷。\n\t- kubelet通过插件注册机制发现 CSI驱动及用于和 CSI驱动交互的 Unix Domain Socket。\n\t- 所有部署在 Kubernetes集群中的 CSI驱动都要通过kubelet 的插件注册机制来注册自己。\n\n# CSI驱动\n\nCSI 的驱动一般包含external-attacher、external-provisioner、external-resizer、external\u0002snapshotter、node-driver-register、CSI driver 等模块，可以根据实际的存储类型和需求进行不同方式的部署。\n\n![image.png](Assets/image_1666115068189_0.png)\n\n# 临时存储\n\n常见的临时存储主要就是emptyDir 卷。\n\nemptyDir 是一种经常被用户使用的卷类型，顾名思义，\"卷\"最初是空的。当 Pod 从节点上删除时，emptyDir 卷中的数据也会被永久删除。但当 Pod 的容器因为某些原因退出再重启时，emptvDir 卷内的数据并不会丢失。\n\n默认情况下，emptvDir 卷存储在支持该节点所使用的存储介质上，可以是本地磁盘或网络存储。emptyDir 也可以通过将 emptyDir.medium 字段设置为\"Memory\"来通知Kubernetes为容器安装tmpfs，此时数据被存储在内存中，速度相对于本地存储和网络存储快很多。但是在节点重启的时候，内存数据会被清除;而如果存在磁盘上，则重启后数据依然存在。另外，使用 tmpfs 的内存也会计入容器的使用内存总量中，受系统的Cgroup 限制。\n\nemptyDir 设计的初衷主要是给应用充当缓存空间，或者存储中间数据，用于快速恢复。然而，这并不是说满足以上需求的用户都被推荐使用 emptyDir，我们要根据用户业务的实际特点来判断是否使用emptyDir。因为 emptyDir 的空间位于系统根盘，被所有容器共享，所以在磁盘的使用率较高时会触发Pod 的 eviction 操作，从而影响业务的稳定。\n\n# 半持久化存储\n\n常见的半持久化存储主要是hostPath卷。hostPath卷能将主机节点文件系统上的文件或目录挂载到指定 Pod 中。对普通用户而言一般不需要这样的卷，但是对很多需要获取节点系统信息的Pod而言，却是非常必要的。\n\n例如，hostPath 的用法举例如下：\n\n- 某个Pod 需要获取节点上所有Pod 的 log，可以通过hostPath 访问所有 Pod 的 stdout输出存储目录，例如/Var/log/pods 路径。\n- 某个Pod 需要统计系统相关的信息，可以通过 hostPath 访问系统的/proc目录。\n\n使用 hostPath 的时候，除设置必需的path 属性外，用户还可以有选择性地为 hostPath 卷指定类型，支持类型包含目录、字符设备、块设备等。\n\n# hostPath 卷需要注意\n\n使用同一个目录的 Pod 可能会由于调度到不同的节点，导致目录中的内容有所不同。\n\nKubernetes在调度时无法顾及由 hostPath 使用的资源。\n\nPod被删除后，如果没有特别处理，那么hostPath上写的数据会遗留到节点上，占用磁盘空间。\n\n# 持久化存储\n\n支持持久化的存储是所有分布式系统所必备的特性。针对持久化存储，Kubernetes 引入了StorageClass、Volume、PVC（Persistent Volume Claim）、PV（Persitent Volume）的概念，将存储独立于 Pod 的生命周期来进行管理。\n\nKuberntes 目前支持的持久化存储包含各种主流的块存储和文件存储，譬如 awsElasticBlockStore、azureDisk、cinder、NFS、cephfs、iscsi 等，在大类上可以将其分为网络存储和本地存储两种类型\n\n## StorageClass\n\nStorageClass用于指示存储的类型，不同的存储类型可以通过不同的 StorageClass 来为用户提供服务。StorageClass 主要包含存储插件 provisioner、卷的创建和 mount 参数等字段。\n\n```yaml\nallowVolumeExpansion: true\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"false\"\n    name: rook-ceph-block\n    parameters:\n      clusterID: rook-ceph\n      csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner\n      csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph\n      Csi.storage.k8s.io/fstype: ext4\n      csi.storage.k8s.io/node-stage-secret-name: rook-Csi-rbd-node\n      Csi.storage.k8s.io/node-stage-secret-namespace: rook-Ceph\n      csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner\n      csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph\n      imageFeatures: layering\n      imageFormat: \"2\"\n      pool: replicapool\nprovisioner: rook-ceph.rbd.csi.ceph.com\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\n```\n\n## PVC\n\n由用户创建，代表用户对存储需求的声明，主要包含需要的存储大小、存储卷的访问模式、StroageClass 等类型，其中存储卷的访问模式必须与存储的类型一致\n\n![image.png](Assets/image_1666116424400_0.png)\n\n## PV\n\n由集群管理员提前创建，或者根据 PVC的申请需求动态地创建，它代表系统后端的真实的存储空间，可以称之为卷空间。\n\n## 存储对象关系\n\n用户通过创建 PVC来申请存储。控制器通过 PVC的 StorageClass 和请求的大小声明来存储后端创建卷，进而创建 PV，Pod 通过指定 PVC来引用存储。\n\n![image.png](Assets/image_1666116488809_0.png)\n\n# 生产实践经验分享\n\n不同介质类型的磁盘，需要设置不同的 StorageClass，以便让用户做区分。StorageClass需要设置磁盘介质的类型，以便用户了解该类存储的属性。\n\n在本地存储的 PV静态部署模式下，每个物理磁盘都尽量只创建一个 PV，而不是划分为多个分区来提供多个本地存储 PV，避免在使用时分区之间的I/O干扰。\n\n本地存储需要配合磁盘检测来使用。当集群部署规模化后，每个集群的本地存储 PV 可能会超过几万个，如磁盘损坏将是频发事件。此时，需要在检测到磁盘损坏、丢盘等问题后，对节点的磁盘和相应的本地存储 PV进行特定的处理，例如触发告警、自动 cordon 节点、自动通知用户等。\n\n对干提供本地存储节点的磁盘管理，需要做到灵活管理和自动化。节点磁盘的信息可以归一、集中化管理。在local-Volume-provisioner 中增加部署逻辑，当容器运行起来时，拉取该节点需要提供本地存储的磁盘信息，例如磁盘的设备路径，以Filesystem 或 Block 的模式提供本地存储，或者是否需要加入某个LVM的虚拟组（VG）等。local-volume-provisioner 根据获取的磁盘信息对磁盘进行格式化，或者加入到某个VG，从而形成对本地存储支持的自动化闭环。\n\n# 独占的 Local Volume\n\n创建 PV：通过local-volume-provisioner DaemonSet创建本地存储的PV。\n创建PVC：用户创建PVC，由于它处于pending状态，所以kube-controller-manager并不会对该PVC做任何操作。\n\n创建Pod：用户创建Pod。\n\nPod挑选节点：kube-scheduler开始调度Pod，通过PVC的resources.request.storage和volumeMode选择满足条件的PV，并且为Pod选择一个合适的节点。\n\n更新PV：kube-scheduler将PV的pv.Spec.claimRef设置为对应的PVC，并且设置 `annotation pv.kubernetes.io/bound-by-controller` 的值为“yes”。\n\nPVC和PV绑定：pv_controller同步PVC和PV的状态，并将PVC和PV进行绑定。\n\n监听PVC对象：kube-scheduler等待PVC的状态变成 Bound状态。\n\nPod 调度到节点：如果PVC的状态变为Bound则说明调度成功，而如果PVC一直处于pending状态，超时后会再次进行调度。\n\nMount 卷启动容器：kubelet 监听到有Pod 已经调度到节点上，对本地存储进行mount操作，并启动容器。\n\n![image.png](Assets/image_1666116919277_0.png)\n\n# Dynamic Local Volume\n\nCSI驱动需要汇报节点上相关存储的资源信息，以便用于调度但是机器的厂家不同，汇报方式也不同。\n\n例如，有的厂家的机器节点上具有 NVMe、SSD、HDD等多种存储介质，希望将这些存储介质分别进行汇报。\n\n这种需求有别于其他存储类型的 CSI驱动对接口的需求，因此如何汇报节点的存储信息，以及如何让节点的存储信息应用于调度，目前并没有形成统一的意见。\n\n集群管理员可以基于节点存储的实际情况对开源CSI 驱动和调度进行一些代码修改，再进行部署和使用\n\n# Local Dynamic 的挂载流程\n\n创建PVC：用户创建PVC，PVC处于pending状态。\n\n创建Pod：用户创建Pod。\n\nPod 选择节点：kube-scheduler开始调度Pod，通过PVC的pvc.spec.resources.request.storage等选择满足条件的节点。\n\n更新PVC：选择节点后，kube-scheduler会给PVC添加包含节点信息的 `annotation∶ volume.kubernetes.io/selected-node∶\u003c节点名字\u003e`。\n\n创建卷：运行在节点上的容器 external-provisioner 监听到PVC带有该节点相关的annotation，向相应的CSI驱动申请分配卷。\n\n创建PV：PVC申请到所需的存储空间后，external-provisioner创建PV，该PV的pv.Spec.claimRef设置为对应的PVC。\n\nPVC和PV绑定：kube-controller-manager将PVC和PV进行绑定，状态修改为 Bound。\n\n监听PVC状态：kube-scheduler等待PVC变成Bound状态。\n\nPod调度到节点：当PVC的状态为Bound时，Pod才算真正调度成功了。如果PVC一直处于Pending状态，超时后会再次进行调度。\n\nMount卷：kubelet监听到有Pod已经调度到节点上，对本地存储进行mount操作。\n\n启动容器：启动容器。\n\n![image.png](Assets/image_1666117282847_0.png)\n\n# Local Dynamic 的挑战\n\n如果将磁盘空间作为一个存储池（例如LVM）来动态分配，那么在分配出来的逻辑卷空间的使用上，可能会受到其他逻辑卷的I/O干扰，因为底层的物理卷可能是同一个。\n\n如果 PV后端的磁盘空间是一块独立的物理磁盘，则I/O 就不会受到干扰。\n\n# 生产实践经验分享\n\n不同介质类型的磁盘，需要设置不同的 StorageClass，以便让用户做区分。StorageClass需要设置磁盘介质的类型，以便用户了解该类存储的属性。\n\n在本地存储的 PV 静态部署模式下，每个物理磁盘都尽量只创建一个PV，而不是划分为多个分区来提供多个本地存储 PV，避免在使用时分区之间的I/O 干扰。\n\n本地存储需要配合磁盘检测来使用。当集群部署规模化后，每个集群的本地存储 PV 可能会超过几万个，如磁盘损坏将是频发事件。此时，需要在检测到磁盘损坏、丢盘等问题后，对节点的磁盘和相应的本地存储 PV进行特定的处理，例如触发告警、自动 cordon 节点、自动通知用户等。\n\n对于提供本地存储节点的磁盘管理，需要做到灵活管理和自动化。节点磁盘的信息可以归一、集中化管理。在 loCal-Volume-proVisioner 中增加部署逻辑，当容器运行起来时，拉取该节点需要提供本地存储的磁盘信息，例如磁盘的设备路径，以Filesystem 或 Block 的模式提供本地存储，或者是否需要加入某个LVM的虚拟组（VG）等。local-volume-provisioner根据获取的磁盘信息对磁盘进行格式化，或者加入到某个VG，从而形成对本地存储支持的自动化闭环。\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/Controller-Manager":{"title":"Controller Manager","content":"\n# 控制器的工作流程\n\n![image.png](Assets/image_1666083933125_0.png)\n\n# Informer 的内部机制\n\n![image.png](Assets/image_1666084265391_0.png)\n\n# 控制器的协同工作原理\n\n![image.png](Assets/image_1666084297645_0.png)\n\n# 通用 Controller\n\nJob Controller：处理job。\n\nPod AutoScaler：处理 pod 的自动缩容/扩容。\n\nRelicaSet：依据 Replicaset Spec 创建 Pod。\n\nService Controller：为 LoadBalancer type 的 service 创建LB VIP。\n\nServiceAccount Controller：确保serviceaccount在当前namespace存在。\n\nStatefulSet Controller：处理 statefulset 中的 pod。\n\nVolume Controller：依据 PV spec创建 volume。\n\nResource quota Controller：在用户使用资源之后，更新状态。\n\nNamespace Controller：保证namespace删除时，该namespace下的所有资源都先被删除\n\nReplication Controller：创建 RC后，负责创建 POD。\n\nNode Controller：维护 node 状态，处理 evict 请求等。\n\nDaemon Controller：依据 damonset创建 pod。\n\nDeployment Controller：依据 deployment spec创建 replicaset。\n\nEndpoint Controller：依据 service spec创建endpoint，依据 podip 更新 endpoint。\n\nGarbage Collector：处理级联删除，比如删除 deployment 的同时删除replicaset 以及pod。\n\nCronJob Controller：处理 cronjob。\n\n# Cloud Controller Manager\n\n**什么时候需要 cloud controller manager?**\n\nCloud Controller Manager 自Kubernetes1.6开始，从kube-controller-manager中分离出来，主要因为 Cloud Controller Manager往往需要跟企业 cloud 做深度集成，release cycle 跟Kubernetes相对独立。\n\n与Kubernetes核心管理组件一起升级是一件费时费力的事。\n\n**通常 cloud controller manager 需要：**\n\n认证授权：企业 cloud 往往需要认证信息，Kubernetes要与Cloud API通信，需要获取cloud系统里的 ServiceAccount；\n\nCloud controller manager 本身作为一个用户态的 component，需要在Kubernetes中有正确的RBAC设置，获得资源操作权限；\n\n高可用：需要通过 leader election来确保 cloud controller manger 高可用。\n\n# Cloud controller manager 的配置\n\n- cloud controller manager 是从老版本的 APIServer 分离出来的。\n\t- Kube-APIServer和kube-controller-manager中一定不能指定cloud-provider，否则会加载内置的 cloud controller manager。\n\t- Kubelet 要配置--cloud-provider=external。\n- Cloud Controller Manager 主要支持：\n\t- Node controller：访问 cloud API，来更新node 状态；在 cloud删除该节点以后，从kubernetes删除 node；\n\t- Service controller：负责配置为 loadbalancer类型的服务配置LB VIP；\n\t- Route Controller：在 cloud 环境配置路由；\n\t- 可以自定义任何需要的 Cloud Controller。\n\n# 需要定制的 Cloud controller\n\n- Ingress controller；\n- Service Controller；\n- 自主研发的 controller，比如之前提到的：\n\t- RBAC controller；\n\t- Account controller。\n\n# 来自生产的经验\n\n- 保护好 controller manager 的 kubeconfig：\n\t- 此 kubeconfig拥有所有资源的所有操作权限，防止普通用户通过 kubectl exec kube\u0002Controller-manager cat 获取该文件。\n\t- 用户可能做任何你想象不到的操作，然后来找你 support。\n- Pod evict 后 IP发生变化，但endpoint 中的 address 更新失败：\n\t- 分析 stacktrace发现 endpoint在更新LoadBalancer 时调用gophercloud 连接 hang住，导致 endpoint worker线程全部卡死。\n\n# 确保 scheduler 和 controller 的高可用\n\nLeader Election\n\nKubenetes提供基于configmap 和 endpoint 的 leader election类库\n\nKubernetes采用leader election模式启动component后，会创建对应endpoint，并把当前的leader 信息 annotate 到 endponit 上\n\n```yaml\napiversion: v1\nkind: Endpoints\nmetadata:\n  annotations:\n    control-plane.alpha.kubernetes.io/leader: '\"holderldentity\":\"minikube\",leaseDurationSeconds\":15,\"acquireTime\":\"2018-04-05T17:31:29Z\",\"renewTime\":\"2018-04-07T07:18:39Z\",\"leaderTransitions\":0}'\n  creationTimestamp: 2018-04-05T17:31:29Z\n  name: kube-scheduler\n  namespace: kube-system\n  resourceVersion: \"138930\"\n  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler\n  uid: 2d12578d-38f7-11e8-8df0-0800275259e5\nsubsets: null\n```\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/Headless":{"title":"Headless","content":"\n# Headless和ClusterIP的区别\n\nCoreDNS的作用：在K8S里，我们想要通过name来访问服务的方式就是在Deployment上面添加一层Service，这样我们就可以通过Service name来访问服务了，那其中的原理就是和CoreDNS有关，它将Service name解析成Cluster IP。\n\n这样我们访问Cluster IP的时候就通过Cluster IP作负载均衡，把流量分布到各个POD上面。\n\nK8s中资源的全局FQDN格式:  \n```\n　　Service_NAME.NameSpace_NAME.Domain.LTD.  \n　　Domain.LTD.=svc.cluster.local.　　　　 #这是默认k8s集群的域名。\n```\n\n## ClusterIP\n\n`ClusterIP`的原理：一个`Service`可能对应多个`EndPoint(Pod)`，`client`访问的是`Cluster IP`，通过`iptables`规则转到`Real Server`，从而达到负载均衡的效果。\n\n![[Assets/Pasted image 20230402230548.png]]\n\n从上面的结果中我们可以看到虽然`Service`有2个`endpoint`，但是`dns`查询时只会返回`Service`的地址。具体`client`访问的是哪个`Real Server`，是由`iptables`来决定的。\n\n## Headless\n\n![[Assets/Pasted image 20230402230702.png]]\n\n`dns`查询会如实的返回2个真实的`endpoint`。\n\n## Headless使用场景\n\n有状态应用，例如数据库。\n\n例如主节点可以对数据库进行读写操作，而其它的两个工作节点只能读，在这里客户端就没必要指定pod服务的集群地址，直接指定数据库Pod ip地址即可，这里需要绑定dns，客户端访问dns，dns会自动返回pod IP地址列表\n\n![[Assets/Pasted image 20230402230927.png]]\n\n---\n\n第一种：自主选择权，有时候`client`想自己来决定使用哪个`Real Server`，可以通过查询`DNS`来获取`Real Server`的信息。\n\n第二种：`Headless Service`的对应的每一个`Endpoints`，即每一个`Pod`，都会有对应的`DNS`域名；这样`Pod`之间就能互相访问，集群也能单独访问pod。\n\n![[Assets/Pasted image 20230402230752.png]]\n\n## 总结\n\n-   无头服务不需要指定集群地址\n-   无头服务适用有状态应用例如数据库\n-   无头服务dns查询会返回pod列表，开发人员可以自定义负载均衡策略\n-   普通Service可以通过负载均衡路由到不同的容器应用","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/K0S":{"title":"K0S","content":"\n## 安装K0S\n\n\u003e https://docs.k0sproject.io/v1.23.6+k0s.2/install/\n\n1. `curl -sSLf [https://get.k0s.sh](https://get.k0s.sh) | sudo sh`\n2. `sudo k0s install controller --single`\n3. `sudo k0s start`\n4. `sudo k0s status`\n5. `sudo k0s kubectl get nodes`\n\n## 卸载K0S\n\n1. sudo k0s stop\n2. sudo k0s reset\n3. reboot system\n\n\u003e💡 A few small k0s fragments persist even after the reset (for example, iptables). As such, you should initiate a reboot after the running of the `k0s reset` command.\n\n## 创建用户与kubeconfig\n\n`sudo k0s kubeconfig create --groups \"system:masters\" petrus \u003e k0s.config`\n\n`sudo k0s kubectl create clusterrolebinding --kubeconfig k0s.config petrus-admin-binding --clusterrole=admin --user=petrus`\n\n将生成的kubeconfig `k0s.config` 合并到客户端机器的 `~/.kube/config`\n\n## 查看当前CNI\n\nkubectl get pods -n kube-system\n\n## 配置文件\n\n\u003e https://docs.k0sproject.io/v1.23.6+k0s.2/configuration/\n\n```\nk0s config create \u003e /etc/k0s/k0s.yaml\n```","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/Kubelet":{"title":"Kubelet","content":"\n# kubelet 架构\n\n![image.png](Assets/image_1666095908370_0.png)\n\n# kubelet 管理 Pod 的核心流程\n\n![image.png](Assets/image_1666095931705_0.png)\n\n# kubelet\n\n每个节点上都运行一个kubelet 服务进程，默认监听 10250端口。\n\n- 接收并执行 master 发来的指令；\n- 管理 Pod及Pod 中的容器；\n- 每个kubelet 进程会在API Server 上注册节点自身信息，定期向 master 节点汇报节点的资源使用情况，并通过 cAdvisor监控节点和容器的资源。\n\n# 节点管理\n\n节点管理主要是节点自注册和节点状态更新：\n\n- Kubelet 可以通过设置启动参数--register-node 来确定是否向 API Server 注册自己；\n- 如果Kubelet 没有选择自注册模式，则需要用户自己配置 Node 资源信息，同时需要告知Kubelet集群上的AP| Server 的位置；\n- Kubelet在启动时通过 API Server 注册节点信息，并定时向API Server 发送节点新消息，APIServer 在接收到新消息后，将信息写入 etcd。\n\n# Pod管理\n\n获取Pod清单：\n\n- 文件：启动参数--config指定的配置目录下的文件（默认/etc/Kubernetes/manifests/）。该文件每20秒重新检查一次（可配置）。\n- HTTPendpoint（URL）：启动参数--manifest-url设置。每 20秒检香一次这个端点（可配置)。\n- API Server：通过 API Server监听 etcd 目录，同步 Pod 清单。\n- HTTP server：kubelet侦听 HTTP请求，并响应简单的 API 以提交新的 Pod 清单。\n\n# Pod启动流程\n\n![image.png](Assets/image_1666096575505_0.png)\n\n# Kubelet启动Pod的流程\n\n![image.png](Assets/image_1666097329536_0.png)\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/Service":{"title":"Service","content":"\n# Service对象\n\n- Service Selector\n\t- Kubernetes允许将Pod对象通过标签（Label）进行标记，并通过Service Selector定义基于Pod标签的过滤规则，以便选择服务的上游应用实例\n- Ports\n\t- Ports属性中定义了服务的端口、协议目标端口等信息\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\nSpec:\n  selector:\n    app: nginx\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n```\n\n# Endpoint对象\n\n- 当Service的selector不为空时，Kubernetes EndpointController会侦听服务创建事件，创建与Service同名的Endpoint对象\n- selector能够选取的所有PodIP都会被配置到addresses属性中\n\t- 如果此时selector所对应的filter查询不到对应的Pod，则addresses列表为空\n\t- 默认配置下，如果此时对应的Pod为not ready状态，则对应的PodIP只会出现在subsets的notReadyAddresses属性中，这意味着对应的Pod还没准备好提供服务，不能作为流量转发的目标。\n\t- 如果设置了PublishNotReadyAdddress为true，则无论Pod是否就绪都会被加入readyAddress list\n\n```yaml\napiVersion: v1\nkind: Endpoint\nmetadata:\n  name: nginx-service\nsubsets:\n  - addresses:\n    - ip: 10.1.1.21\n      nodeName: minikube\n      targetRef:\n        kind: Pod\n        name: nginx-deployment-5754944d6c-hnw27\n        namespace: default\n        resourceVersion: \"722191\"\n        uid: 8a3390ae-2f8e-47bf-b8dd-70fae7fb0d32\n```\n\n# EndpointSlice对象\n\n当某个Service对应的backend Pod较多时，Endpoint对象就会因保存的地址信息过多而变得异常庞大\n\nPod状态的变更会引起Endpoint的变更，Endpoint的变更会被推送至所有节点，从而导致持续占用大量网络带宽\n\nEndpointSlice对象，用于对Pod较多的Endpoint进行切片，切片大小可以自定义\n\n```yaml\napiVersion: discovery.k8s.io/v1betal\nkind: EndpointSlice\nmetadata:\n  name: example-abc\n  labels:\n    kubernetes.io/service-name: example\naddressType: IPv4\nports:\n\nname: http\n    protocol: TCP\n    port: 80\nendpoints:\n\naddresses:\n\n\"10.1.2.3\"\n    conditions:\n      ready: true\n      hostname: pod-1\n      topology:\n        kubernetes.io/hostname: node-1\n        topology.kubernetes.io/zone: us-west2-a\n```\n\n# 不定义Selector的Service\n\n- 用户创建了Service但不定义Selector\n\t- Endpoint Controller不会为该Service自动创建Endpoint\n\t- 用户可以手动创建Endpoint对象，并设置任意IP地址到Address属性\n\t- 访问该服务的请求会被转发至目标地址\n- 通过该类型服务，可以为集群外的一组Endpoint创建服务\n\n# Service、Endpoint和Pod的对应关系\n\n![image.png](Assets/image_1666193739512_0.png)\n\n# Service类型\n\n- clusterIP\n\t- Service的默认类型，服务被发布至仅集群内部可见的虚拟IP地址上。\n\t- 在API Server启动时，需要通过service-cluster-ip-range参数配置虚拟IP地址段，API Server中有用干分配IP地址和端口的组件，当该组件捕获Service对象并创建事件时，会从配置的虚拟IP地址段中取一个有效的IP地址，分配给该Service对象。\n- nodePort\n\t- 在APIServer启动时，需要通过node-port-range参数配置nodePort的范围，同样的，APIServer组件会捕获Service对象并创建事件，即从配置好的nodePort范围取一个有效端口，分配给该Service。\n\t- 每个节点的kube-proxy会尝试在服务分配的nodePort上建立侦听器接收请求，并转发给服务对应的后端Pod实例。\n- LoadBalancer\n\t- 企业数据中心一般会采购一些负载均衡器，作为外网请求进入数据中心内部的统一流量入口。\n\t- 针对不同的基础架构云平台，Kubernertes Cloud Manager提供支持不同供应商API的ServiceController。如果需要在Openstack云平台上搭建Kubernetes集群，那么只需提供一openstack.rc，Openstack Service Controller即可通过调用LBaaS API完成负载均衡配置。\n\n# 其他类型服务\n\n- Headless Service\n\t- Headless服务是用户将clusterIP显示定义为None的服务。\n\t- 无头的服务意味着Kubernetes不会为该服务分配统一入口，包括clusterIP，nodePort等\n- ExternalName Service\n\t- 为一个服务创建别名\n\n# Service Topology\n\n一个网络调用的延迟受客户端和服务器所处位置的影响，两者是否在同一节点、同一机架、同一可用区、同一数据中心，都会影响参与数据传输的设备数量\n\n在分布式系统中，为保证系统的高可用，往往需要控制应用的错误域（Failure Domain），比如通过反亲和性配置，将一个应用的多个副本部署在不同机架，甚至不同的数据中心\n\nKubernetes提供通用标签来标记节点所处的物理位置，如：\n\n- `topology.kubernetes.io/zone: us-west2-a`\n- `failure-domain.beta.kubernetes.io/region: us-west`\n- `failure-domain.tess.io/network-device:us-west05-ra053`\n- `failure-domain.tess.io/rack:us west02 02-314 19 12`\n- `ubernetes.io/hostname: node-1`\n\n---\n\nService引入了topologyKeyS属性，可以通过如下设置来控制流量\n\n当topologyKeys设置为[\"kubernetes.io/hostname\"]时，调用服务的客户端所在节点上如果有服务实例正在运行，则该实例处理请求，否则，调用失败。\n\n当topologyKeys设置为[\"kubernetes.io/hostname\", \"topology.kubernetes.io/zone\", \"topology.kubernetes.io/region\"]时，若同一节点有对应的服务实例，则请求会优先转发至该实例。否则，顺序查找当前zone及当前region是否有服务实例，并将请求按顺序转发。\n\n当topologyKeys设置为[\"topology.kubernetes.io/ zone\", \"\\*\"]时，请求会被优先转发至当前zone的服务实例。如果当前zone不存在服务实例，则请求会被转发至任意服务实例。\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/etcd":{"title":"etcd","content":"\n# etcd\n\nEtcd是CoreOS基于Raft开发的分布式key-value存储，可用于服务发现、共享配置以及一致性保障（如数据库选主、分布式锁等）。\n\n在分布式系统中，如何管理节点间的状态一直是一个难题，etcd像是专门为集群环境的服务发现和注册而设计，它提供了数据TTL失效、数据改变监视、多值、目录监听、分布式锁原子操作等功能，可以方便的跟踪并管理集群节点的状态。\n\n- 键值对存储：将数据存储在分层组织的目录中，如同在标准文件系统中\n- 监测变更：监测特定的键或目录以进行更改，并对值的更改做出反应\n- 简单: curl可访问的用户的API（HTTP+JSON）\n- 安全: 可选的SSL客户端证书认证\n- 快速: 单实例每秒1000次写操作，2000+次读操作\n- 可靠: 使用Raft算法保证一致性\n\n## 主要功能\n\n- 基本的key-value存储\n- 监听机制\n- key的过期及续约机制，用于监控和服务发现\n- 原子Compare And Swap和Compare And Delete，用于分布式锁和leader选举\n\n## 使用场景\n\n- 也可以用于键值对存储，应用程序可以读取和写入 etcd 中的数据\n- etcd 比较多的应用场景是用于服务注册与发现\n- 基于监听机制的分布式异步系统\n\n## 键值对存储\n\netcd 是一个**键值存储**的组件，其他的应用都是基于其键值存储的功能展开。\n\n- 采用kv型数据存储，一般情况下比关系型数据库快。\n- 支持动态存储(内存)以及静态存储(磁盘)。\n- 分布式存储，可集成为多节点集群。\n- 存储方式，采用类似目录结构。（B+tree）\n\t- 只有叶子节点才能真正存储数据，相当于文件。\n\t- 叶子节点的父节点一定是目录，目录不能存储数据。\n\n## 服务注册和发现\n\n- 强一致性、高可用的服务存储目录。\n\t- 基于 Raft 算法的 etcd 天生就是这样一个强一致性、高可用的服务存储目录。\n- 一种注册服务和服务健康状况的机制。\n\t- 用户可以在 etcd 中注册服务，并且对注册的服务配置 key TTL，定时保持服务的心跳以达到监控健康状态的效果。\n\n![image.png](Assets/image_1665845808268_0.png)\n\n## 消息发布与订阅\n\n在分布式系统中，最适用的一种组件间通信方式就是消息发布与订阅。\n\n即构建一个配置共享中心，数据提供者在这个配置中心发布消息，而消息使用者则订阅他们关心的主题，一旦主题有消息发布，就会实时通知订阅者。\n\n通过这种方式可以做到分布式系统配置的集中式管理与动态更新。\n\n应用中用到的一些配置信息放到etcd上进行集中管理。\n\n应用在启动的时候主动从etcd获取一次配置信息，同时，在etcd节点上注册一个Watcher并等待，以后每次配置有更新的时候，etcd都会实时通知订阅者，以此达到获取最新配置信息的目的。\n\n## 核心：TTL \u0026 CAS\n\nTTL（time to live）指的是给一个key设置一个有效期，到期后这个key就会被自动删掉，这在很多分布式锁的实现上都会用到，可以保证锁的实时有效性。\n\nAtomic Compare-and-Swap（CAS）指的是在对key进行赋值的时候，客户端需要提供一些条件，当这些条件满足后，才能赋值成功。这些条件包括：\n\n- prevExist：key当前赋值前是否存在\n- prevValue：key当前赋值前的值\n- prevIndex：key当前赋值前的Index\n\n这样的话，key的设置是有前提的，需要知道这个key当前的具体情况才可以对其设\n\n# Raft协议\n\n## Raft协议概览\n\nRaft协议基于quorum机制，即大多数同意原则，任何的变更都需超过半数的成员确认\n\n![image.png](Assets/image_1665850658533_0.png)\n\n## 理解Raft协议\n\nhttp://thesecretlivesofdata.com/raft/\n\n## learner\n\nRaft 4.2.1引入的新角色\n\n当出现一个etcd集群需要增加节点时，新节点与Leader的数据差异较大，需要较多数据同步才能跟上leader的最新的数据。\n\n此时Leader的网络带宽很可能被用尽，进而使得leader无法正常保持心跳。\n\n进而导致follower重新发起投票。\n\n进而可能引发etcd集群不可用。\n\n**Learner角色只接收数据而不参与投票，因此增加learner节点时，集群的quorum不变。**\n\n![image.png](Assets/image_1665853509499_0.png)\n\n## etcd基于Raft的一致性\n\n选举方法\n\n初始启动时，节点处于follower状态并被设定一个election timeout，如果在这一时间周期内没有收到来自 leader 的 heartbeat，节点将发起选举：将自己切换为 candidate 之后，向集群中其它 follower节点发送请求，询问其是否选举自己成为 leader。\n\n当收到来自集群中过半数节点的接受投票后，节点即成为 leader，开始接收保存 client 的数据并向其它的 follower 节点同步日志。如果没有达成一致，则candidate随机选择一个等待间隔（150ms ~ 300ms）再次发起投票，得到集群中半数以上follower接受的candidate将成为leader\n\nleader节点依靠定时向 follower 发送heartbeat来保持其地位。\n\n任何时候如果其它 follower 在 election timeout 期间都没有收到来自 leader 的 heartbeat，同样会将自己的状态切换为 candidate 并发起选举。每成功选举一次，新 leader 的任期（Term）都会比之前leader 的任期大1。\n\n## 日志复制\n\n当接Leader收到客户端的日志（事务请求）后先把该日志追加到本地的Log中，然后通过heartbeat把该Entry同步给其他Follower，Follower接收到日志后记录日志然后向Leader发送ACK，当Leader收到大多数（n/2+1）Follower的ACK信息后将该日志设置为已提交并追加到本地磁盘中，通知客户端并在下个heartbeat中Leader将通知所有的Follower将该日志存储在自己的本地磁盘中。\n\n## 安全性\n\n安全性是用于保证每个节点都执行相同序列的安全机制，如当某个Follower在当前Leadercommit Log时变得不可用了，稍后可能该Follower又会被选举为Leader，这时新Leader可能会用新的Log覆盖先前已committed的Log，这就是导致节点执行不同序列；Safety就是用于保证选举出来的Leader一定包含先前 committed Log的机制；\n\n选举安全性（Election Safety）：每个任期（Term）只能选举出一个Leader\n\nLeader完整性（Leader Completeness）：指Leader日志的完整性，当Log在任期Term1被Commit后，那么以后任期Term2、Term3…等的Leader必须包含该Log；Raft在选举阶段就使用Term的判断用于保证完整性：当请求投票的该Candidate的Term较大或Term相同Index更大则投票，否则拒绝该请求。\n\n## 失效处理\n\n1) Leader失效：其他没有收到heartbeat的节点会发起新的选举，而当Leader恢复后由于步进数小会自动成为follower（日志也会被新leader的日志覆盖）\n\n2）follower节点不可用：follower 节点不可用的情况相对容易解决。因为集群中的日志内容始终是从 leader 节点同步的，只要这一节点再次加入集群时重新从 leader 节点处复制日志即可。\n\n3）多个candidate：冲突后candidate将随机选择一个等待间隔（150ms ~ 300ms）再次发起投票，得到集群中半数以上follower接受的candidate将成为leader\n\n## wal日志\n\nwal日志是二进制的，解析出来后是以上数据结构LogEntry。其中第一个字段type，只有两种，一种是0表示Normal，1表示ConfChange（ConfChange表示 Etcd 本身的配置变更同步，比如有新的节点加入等）。第二个字段是term，每个term代表一个主节点的任期，每次主节点变更term就会变化。第三个字段是index，这个序号是严格有序递增的，代表变更序号。第四个字段是二进制的data，将raft request对象的pb结构整个保存下。etcd 源码下有个tools/etcd\u0002dump-logs，可以将wal日志dump成文本查看，可以协助分析Raft协议。\n\nRaft协议本身不关心应用数据，也就是data中的部分，一致性都通过同步wal日志来实现，每个节点将从主节点收到的data apply到本地的存储，Raft只关心日志的同步状态，如果本地存储实现的有bug，比如没有正确的将data apply到本地，也可能会导致数据不一致。\n\n![image.png](Assets/image_1665854364816_0.png)\n\n# etcd v3 存储，Watch以及过期机制\n\n![image.png](Assets/image_1665854412122_0.png)\n\n## 存储机制\n\netcd v3 store 分为两部分，一部分是内存中的索引，kvindex，是基于Google开源的一个Golang的btree实现的，另外一部分是后端存储。按照它的设计，backend可以对接多种存储，当前使用的boltdb。boltdb是一个单机的支持事务的kv存储，etcd 的事务是基于boltdb的事务实现的。etcd 在boltdb中存储的key是reversion，value是 etcd 自己的key-value组合，也就是说 etcd 会在boltdb中把每个版本都保存下，从而实现了多版本机制。\n\nreversion主要由两部分组成，第一部分main rev，每次事务进行加一，第二部分sub rev，同一个事务中的每次操作加一。\n\netcd 提供了命令和设置选项来控制compact，同时支持put操作的参数来精确控制某个key的历史版本数。\n\n内存kvindex保存的就是key和reversion之前的映射关系，用来加速查询。\n\n![image.png](Assets/image_1665909521087_0.png)\n\n## Watch机制\n\netcd v3 的watch机制支持watch某个固定的key，也支持watch一个范围（可以用于模拟目录的结构的watch），所以 watchGroup 包含两种watcher，一种是 key watchers，数据结构是每个key对应一组watcher，另外一种是 range watchers, 数据结构是一个 IntervalTree，方便通过区间查找到对应的watcher。\n\n同时，每个 WatchableStore 包含两种 watcherGroup，一种是synced，一种是unsynced，前者表示该group的watcher数据都已经同步完毕，在等待新的变更，后者表示该group的watcher数据同步落后于当前最新变更，还在追赶。\n\n当 etcd 收到客户端的watch请求，如果请求携带了revision参数，则比较请求的revision和store当前的revision，如果大于当前revision，则放入synced组中，否则放入unsynced组。同时 etcd 会启动一个后台的goroutine持续同步unsynced的watcher，然后将其迁移到synced组。也就是这种机制下，etcd v3 支持从任意版本开始watch，没有v2的1000条历史event表限制的问题（当然这是指没有compact的情况下）\n\n## 容量管理\n\n- 单个对象不建议超过1.5M\n- 默认容量2G\n- 不建议超过8G\n\n# etcd在集群中所处的位置\n\n![image.png](Assets/image_1665928249958_0.png)\n\n# etcd拓扑\n\n堆叠式etcd集群的高可用拓扑\n\n这种拓扑将相同节点上的控制平面和etcd成员耦合在一起。优点在于建立起来非常容易，并且对副本的管理也更容易。但是，堆叠式存在耦合失败的风险。如果一个节点发生故障，则etcd成员和控制平面实例都会丢失，并且集群冗余也会受到损害。可以通过添加更多控制平面节来减轻这种风险。因此为实现集群高可用应该至少运行三个堆叠的Master节点。\n\n![image.png](Assets/image_1665928763555_0.png)\n\n外部etcd集群的高可用拓扑\n\n该拓扑将控制平面和etcd成员解耦。如果丢失一个Master节点，对etcd成员的影响较小，并且不会像堆叠式拓扑那样对集群冗余产生太大影响。但是，此拓扑所需的主机数量是堆叠式拓扑的两倍。具有此拓扑的群集至少需要三个主机用于控制平面节点，三个主机用于etcd集群。\n\n![image.png](Assets/image_1665928794361_0.png)","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/istio":{"title":"istio","content":"\n# 问题\n\n## No tracing问题\n\n- 没有正确设置meshConfig.defaultConfig.tracing.zipkin.address和meshConfig.defaultConfig.tracing.sampling\n\t- `istioctl install --set profile=demo --set meshConfig.defaultConfig.tracing.zipkin.address=jaeger-collector.istio-system:9411 --set meshConfig.defaultConfig.tracing.sampling=100  -y`\n- 重建、更新istio后，可能需要把sidecar重启/重建\n- jaeger暂时使用allinone即可，因为用es做后端存储太难配置\n\n## kiali的Graph为空或prometheus没有数据\n\nprometheus.io的annotiation不支持，需要为prometheus添加基于kubernetes_sd_configs服务发现的relabel配置\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Kubernetes"]},"/CS/Kubernetes/kube-proxy":{"title":"kube-proxy","content":"\n# kube-proxy\n\n每台机器上都运行一个kube-proxy服务，它监听API server中service和endpoint的变化情况，并通过iptables等来为服务配置负载均衡（仅支持TCP和UDP）。\n\nkube-proxy可以直接运行在物理机上，也可以以static pod或者DaemonSet的方式运行。\n\nkube-proxy当前支持一下几种实现\n\n- userspace：最早的负载均衡方案，它在用户空间监听一个端口，所有服务通过iptables转发到这个端口，然后在其内部负载均衡到实际的Pod。该方式最主要的问题是效率低，有明显的性能瓶颈。\n- iptables：目前推荐的方案，完全以iptables规则的方式来实现service负载均衡。该方式最主要的问题是在服务多的时候产生太多的iptables规则，非增量式更新会引入一定的时延，大规模情况下有明显的性能问题\n- ipvs：为解决iptables模式的性能问题，v1.8新增了ipvs模式，采用增量式更新，并可以保证service更新期间连接保持不断开\n- winuserspace∶ 同userspace，但仅工作在windows上\n\n# Linux内核处理数据包：Netfilter框架\n\n![image.png](Assets/image_1666196681318_0.png)\n\n# Netfilter和iptables\n\n![image.png](Assets/image_1666197364236_0.png)\n\n# iptables\n\n![image.png](Assets/image_1666197458799_0.png)\n\n# iptables支持的锚点\n\n![image.png](Assets/image_1666198206945_0.png)\n\n# kube-proxy工作原理\n\n![image.png](Assets/image_1666198537603_0.png)\n\n# Kubernetes iptables规则\n\n![image.png](Assets/image_1666199038246_0.png)\n\n# IPVS\n\n![image.png](Assets/image_1666199163455_0.png)\n\n# IPVS支持的锚点和核心函数\n\n![image.png](Assets/image_1666199183091_0.png)\n\n# 域名服务\n\nKubernetes Service通过虚拟IP地址或者节点端口为用户应用提供访问入口\n\n然而这些IP地址和端口是动态分配的，如果用户重建一个服务，其分配的clusterlP和nodePort，以及LoadBalancerIP都是会变化的，我们无法把一个可变的入口发布出去供他人访问\n\nKubernetes提供了内置的域名服务，用户定义的服务会自动获得域名，而无论服务重建多少次，只要服务名不改变，其对应的域名就不会改变\n\n# CoreDNS\n\nCoreDNS包含一个内存态DNS，以及与其他controller类似的控制器\n\nCoreDNS的实现原理是，控制器监听Service和Endpoint的变化并配置DNS，客户端Pod在进行域名解析时，从 CoreDNS中查询服务对应的地址记录\n\n![image.png](Assets/image_1666251811723_0.png)\n\n# 不同类型服务的DNS记录\n\n- 普通Service\n\t- ClusterIP、nodePort、LoadBalancer 类型的Service都拥有API Server分配的ClusterlP，CoreDNS会为这些Service创建FQDN格式为`$svcname.$namespace.svc.$clusterdomain∶clusterIP`的A记录及PTR记录，并为端口创建SRV记录。\n- Headless Service\n\t- 顾名思义，无头，是用户在Spec显式指定ClusterlP为None的Service，对于这类Service，APIServer不会为其分配ClusterIP。CoreDNS为此类Service创建多条A记录，并且目标为每个就绪的PodIP。\n\t- 另外，每个Pod会拥有一个FQDN格式为`$podname.$svcname.$namespace.svc.$clusterdomain`的A记录指向PodIP。\n- ExternalName Service\n\t- 此类Service用来引用一个已经存在的域名，CoreDNS会为该Service创建一个CName记录指向目标域名\n\n# Kubernetes中的域名解析\n\nKubernetes Pod有一个与DNS策略相关的属性DNSPolicy，默认值是ClusterFirst\n\nPod启动后的/etc/resolv.conf会被改写，所有的地址解析优先发送至CoreDNS\n\n```bash\n$cat /etc/resolv.conf\nsearch ns1.svc.cluster.local svc.cluster.localcluster.local\nnameserver 192.168.0.10\nOptions ndots:4\n```\n\n当Pod启动时，同一Namespace的所有Service都会以环境变量的形式设置到容器内\n\n影响?\n\n# 关于DNS的落地实践\n\n- Kubernetes作为企业基础架构的一部分，Kubernetes服务也需要发布到企业DNS，需要定制企业DNS控制器\n\t- 对于Kubernetes中的服务，在企业DNS同样创建A/PTR/SRV records（通常解析地址是LOadBalancer VIP）\n\t- 针对headless service，在PodIP可全局路由的前提下，按需创建DNS records\n\t- Headless service的DNS记录，应该按需创建，否则对企业DNS冲击过大\n- 服务在集群内通过CoreDNS寻址，在集群外通过企业DNS寻址，服务在集群内外有统一标识。\n\n# Kubernetes中的负载均衡技术\n\n- **基于L4的服务**\n\t- 基于iptables/ipvs的分布式四层负载均衡技术\n\t- 多种Load Balancer Provider提供与企业现有ELB的整合\n\t- kube-proxy基于iptables rules为Kubernetes形成全局统一的distributed load balancer\n\t- kube-proxy是一种mesh，Internal Client无论通过podip， nodeport还是LB VIP都经由kube-proxy跳转至pod\n\t- 属于Kubernetes core\n- **基于L7的Ingress**\n\t- 基于七层应用层，提供更多功能\n\t- TLS termination\n\t- L7 path forwarding\n\t- URL/http header rewrite\n\t- 采用7层软件紧密相关\n\n# Service中的Ingress的对比\n\n**基于L4的服务**\n\n- 每个应用独占ELB，浪费资源\n- 为每个服务动态创建DNS记录，频繁的DNS更新·\n- 支持TCP和UDP，业务部门需要启动HTTPS服务，自己管理证书\n\n![image.png](Assets/image_1666253529410_0.png)\n\n**基于L7的Ingress**\n\n- 多个应用共享ELB，节省资源\n- 多个应用共享一个Domain，可采用静态DNS配置\n- TLS termination发生在Ingress层，可集中管理证书\n- 更多复杂性，更多的网络hop\n\n![image.png](Assets/image_1666253658823_0.png)\n\n# Ingress\n\n**Ingress**\n\n- Ingress是一层代理\n- 负责根据hostname和path将流量转发到不同的服务上，使得一个负载均衡器用于多个后台应用\n- Kubernetes Ingress Spec是转发规则的集合\n\n**Ingress Controller**\n\n- 确保实际状态（Actual）与期望状态（Desired）一致的Control Loop\n- Ingress Controller确保\n- 负载均衡配置\n- 边缘路由配置\n- DNS配置\n\n![image.png](Assets/image_1666258785632_0.png)\n\n# 传统应用网络拓扑\n\n![image.png](Assets/image_1666259333625_0.png)\n\n# 为什么需要构建SLB方案\n\n![image.png](Assets/image_1666259361666_0.png)\n\n---\n\n需要解决问题∶如何让domain用户自定义后台应用的访问地址，如何优化访问路径\n\n![image.png](Assets/image_1666259779802_0.png)\n\n# L4 集群架构\n\n![image.png](Assets/image_1666260769431_0.png)\n\n# L7集群架构\n\n![image.png](Assets/image_1666260815158_0.png)\n\n# 数据流\n\n![image.png](Assets/image_1666264176705_0.png)\n\n# 跨大陆的互联网调用\n\n![image.png](Assets/image_1666264576724_0.png)\n\n# 互联网路径的不确定性\n\n![image.png](Assets/image_1666264600590_0.png)\n\n# 边缘加速方案综述\n\n![image.png](Assets/image_1666264618205_0.png)\n\n# 边缘加速组件\n\n![image.png](Assets/image_1666264634297_0.png)\n\n# 对网络路径的优化\n\n![image.png](Assets/image_1666264650876_0.png)\n\n# 不同方案的响应时间对比\n\n![image.png](Assets/image_1666264674482_0.png)\n-\n\n","lastmodified":"2023-05-01T11:46:37.896589235Z","tags":["CS/Kubernetes"]},"/CS/Linux/%E5%86%85%E6%A0%B8":{"title":"内核","content":"\n`/etc/modules-load.d` 与 In  `/etc/modprobe.d`的区别\n\n`/etc/modules-load.d` can only load modules, nothing more. You can’t blacklist a module and you can’t insert a module with special options.\n\nIn  `/etc/modprobe.d` you can blacklist a module, you can crate an alias for a modulename and you can add special options for modules.\n\nSo basically  `/etc/modules-load.d` can be used to insert a module at boot.  `/etc/modprobe.d` cab be used to blacklist or adds options.\n\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["CS/Linux"]},"/CS/Linux/%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%BA%90%E8%AE%A1%E7%AE%97":{"title":"系统资源计算","content":"\n# 内存\n\n## VIRT\n\n1. 进程“需要的”虚拟内存大小，包括进程使用的库、代码、数据，以及malloc、new分配的堆空间和分配的栈空间等；\n2. 假如进程新申请10MB的内存，但实际只使用了1MB，那么它会增长10MB，而不是实际的1MB使用量。\n3. VIRT = SWAP + RES\n\n## RES\n\n1. 进程当前使用的内存大小，包括使用中的malloc、new分配的堆空间和分配的栈空间，但不包括swap out量；\n2. 包含其他进程的共享；\n3. 如果申请10MB的内存，实际使用1MB，它只增长1MB，与VIRT相反；\n4. 关于库占用内存的情况，它只统计加载的库文件所占内存大小。\n5. RES = CODE + DATA\n\n## SHR\n\n1. 除了自身进程的共享内存，也包括其他进程的共享内存；\n2. 虽然进程只使用了几个共享库的函数，但它包含了整个共享库的大小；\n3. 计算某个进程所占的物理内存大小公式：RES – SHR；\n4. swap out后，它将会降下来。","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["CS/Linux"]},"/CS/Linux/%E8%BF%9B%E7%A8%8B":{"title":"进程","content":"\n# 进程与线程\n\n进程：资源分配的基本单位\n\n线程：调度的基本单位\n\n无论是线程还是进程，在 linux 中都以 task_struct 描述，从内核角度看，与进程无本质区别\n\nGlibc 中的 pthread 库提供 NPTL（Native POSIX Threading Library）支持\n\n![image.png](Assets/image_1665579620119_0.png)\n\n# 进程的内存使用\n\n![image.png](Assets/image_1665579663729_0.png)\n\n# CPU对内存的访问\n\nCPU 上有个 Memory Management Unit（MMU） 单元\n\nCPU 把虚拟地址给 MMU，MMU 去物理内存中查询页表，得到实际的物理地址\n\nCPU 维护一份缓存 Translation Lookaside Buffer（TLB），缓存虚拟地址和物理地址的映射关系\n\n![image.png](Assets/image_1665579718778_0.png)\n\n# 进程切换开销\n\n## 直接开销\n\n切换页表全局目录（PGD）\n\n切换内核堆栈\n\n切换硬件上下文（进程恢复前，必须装入寄存器的数据统称为硬件上下文）\n\n刷新TLB\n\n系统调度器的代码执行\n\n## 间接开销\n\nCPU 缓存失效导致的进程需要到内存直接访问的 IO 操作变多\n\n# 线程切换开销\n\n线程本质上只是一批共享资源的进程，线程切换本质上依然需要内核进行进程切换\n\n一组线程因为共享内存资源，因此一个进程的所有线程共享虚拟地址空间，线程切换相比进程切换，**主要节省了虚拟地址空间的切换**\n\n# 用户线程\n\n无需内核帮助，应用程序在用户空间创建的可执行单元，创建销毁完全在用户态完成。\n\n![image.png](Assets/image_1665580048890_0.png)\n\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["CS/Linux"]},"/CS/Linux/%E9%95%9C%E5%83%8F%E7%AB%99":{"title":"镜像站","content":"\n[北京外国语大学开源软件镜像站](https://mirrors.bfsu.edu.cn/)\n\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["CS/Linux"]},"/CS/Linux/ZFS":{"title":"ZFS","content":"\nARC缓存限制\n\n需要把参数写到cmdline中，写在 `/etc/modprobe.d` 中不生效\n\n`GRUB_CMDLINE_LINUX=\"zfs.zfs_arc_max=8589934592 zfs.zfs_arc_min=4294967296\"`\n\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["CS/Linux"]},"/CS/Linux/homelab":{"title":"homelab","content":"\n* 路由器\n\t* openwrt\n\t* openclash\n\t* zerotier\n* K8S\n\t* K9S\n* registry\n\t* harbor\n\t\t* 支持docker镜像\n\t\t* 支持K8S helm\n* OSS\n\t* Minio","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["CS/Linux"]},"/CS/Linux/tmux":{"title":"tmux","content":"\n# 快捷键\n\n1. tmux窗口最大化：prefix+z","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["CS/Linux"]},"/CS/Linux/zerotier-moon":{"title":"zerotier-moon","content":"\n* 使用docker搭建\n\t* 部署命令：`docker run --name zerotier-moon -d --restart always -p 9993:9993/udp -v \u003czerotier-moon-config dir\u003e:/var/lib/zerotier-one seedgou/zerotier-moon -4 \u003cpublic ipv4 addression\u003e`\n\t* 查看moon id：`docker logs zerotier-moon`\n* 防火墙设置\n\t* 9993端口的TCP和UDP均要打开\n* 客户端配置\n\t* 直接使用命令 `zerotier-cli orbit xxxxxx xxxxxx` 添加\n\t* 将moon生成的配置的moons.d下的000000xxxxxx.moon文件拷入客户端的相同目录下，并重启服务\n\t* macos的配置路径为`/Library/Application Support/ZeroTier/One`\n\t* 添加完后需要重启服务（不确定）\n* 测试\n\t* `zerotier-cli listpeers`：如果命令输出中带有moon服务器的IP地址，并且role为moon，即说明moon连接成功\n\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["CS/Linux"]},"/CS/MongoDB/%E7%B4%A2%E5%BC%95":{"title":"索引","content":"\n# 索引排序\n\n* 单字段的索引，创建时不论指定升序还是降序，它查询时用的 sort 可以是任意方向的（升序或降序）。\n\t* 多字段的索引按前缀匹配\n\t* 排序字段的 “排序顺序”，必须和索引中的对应字段的排序顺序完全相同或完全相反。\n\t* sort 条件是 index 的前缀子集时，query 条件可以为空，也可以为任何 index 的前缀子集。\n\t* sort 条件不是 index 的前缀子集时，query 条件必须包含 index 字段中 sort 字段前方的所有字段。\n\n# 数据结构\n\n## MongoDb 真的用的是 B 树吗？\n\n通过查阅资料，我从 MongoDb 的官网和 WiredTiger 官网找到了答案。MongoDb 官网关于存储引擎（Storage Engine）的描述写道：从 MongoDb 3.2 版本开始，其使用了 WiredTiger 作为其默认的存储引擎。\n\n而从 WiredTiger 官网文档，我们可以知道：**WiredTiger 使用的是 B+ 树作为其存储结构**。\n\n那为什么会出现很多资料说 MongoDb 使用 B 树作为存储的数据结构呢？我想可能有两个原因：一个原因可能是 B+ Tree 本身是 B 树的一种优化，所以很多人就直接把 B+ 树说成 B 树了。另一个原因可能是 MongoDb 3.2 之前，确实使用 B 树作为存储的数据结构。\n\n## B树和B+树的优缺点\n\n\u003e B 树与 B+ 树，其比较大的特点是：B 树对于特定记录的查询，其时间复杂度更低。而 B+ 树对于范围查询则更加方便。\n\nB+**树查询速度更稳定**：B+所有关键字数据地址都存在**叶子**节点上，所以每次查找的次数都相同所以查询速度要比B树更稳定。\n\nB+**树天然具备排序功能：** B+树所有的**叶子**节点数据构成了一个有序链表，在查询大小区间的数据时候更方便，数据紧密性很高，缓存的命中率也会比B树高。\n\nB+**树全节点遍历更快：** B+树遍历整棵树只需要遍历所有的**叶子**节点即可，而不需要像B树一样需要对每一层进行遍历，这有利于数据库做全表扫描。\n\n**B树**相对于**B+树**的优点是，如果经常访问的数据离根节点很近，而**B树**的**非叶子**节点本身存有关键字和数据，所以在查询这种数据检索的时候会要比**B+树**快。\n\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["CS/MongoDB"]},"/CS/RSS/%E5%9F%BA%E4%BA%8ERSS%E7%9A%84%E6%8E%A8%E9%80%81%E7%B3%BB%E7%BB%9F":{"title":"基于RSS的推送系统","content":"\n# 想法\n\n使用RSSHUB搜索原RSS中的特定信息并生成新的RSS（刷新速度极快），再使用RSS客户端比如浏览器扩展或telegram bot读取新生成的RSS并进行推送（同样需要刷新速度极快）。\n\n# 实践\n\n## 订阅telegram频道\n\n将频道username与需要搜索的关键字填到RSSHUB的路由中\n\n将上一步RSSHUB的路由添加RSS客户端中\n\nRSS客户端实现推送\n\n## RSS客户端最快刷新时间\n\n[[CS/RSS/RSS Reader]]\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["RSS"]},"/CS/RSS/%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84RSS%E6%9C%8D%E5%8A%A1":{"title":"搭建自己的RSS服务","content":"\n# 为什么搭建自己的RSS服务平台\n\n基于Docker，自建难度低，迁移性好。\n\n自建RSS订阅器平台足够强大。有filter功能等。\n\n数据完全掌握在自己手中。可通过duplicati备份。\n\n无广告，不担心托管平台倒闭。RSS阅读器平台经常会由于盈利或政策原因关门。\n\n没有订阅数上限或其它恶心的限制。完全免费。\n\n进一步压榨VPS。毕竟有这么多性能溢出！\n\n通过插件功能可以**支持内嵌Youtube、Bilibili、西瓜视频**等。\n\n将多RSS源合并成一个URL分享给他人\n\n# 思路\n\n自己搭一个 RSS Server （如 FreshRSS 或者 TTRSS ），在服务端管理自己的 RSS 源。\n\n对客户端的要求就是支持 Google Reader API 或者 Fever API ，客户端能通过 API 连接到 RSS Server 就可以了。\n\n# 方案\n\n由 阅读器 + rss服务 + rss生成器 + rss探测器 组成：\n\n**fluent reader**（免费跨端阅读器）+\n\n**freshRSS**（用于管理订阅源及暴露fever api供阅读器使用）+\n\n**RSShub**（用于生成订阅源）+\n\n**RSShub radar**（用于嗅探订阅源，以及一键录入到freshrss）\n\n## fluent reader —— 阅读器\n\n一款开源的跨端（移动+桌面）rss阅读器，不收集个人数据。\n\n它提供了服务模式，即把订阅源交给外部服务管理，自身作为一个阅读器及外部服务的代理。\n\n我选择了「fever api」服务模式，只要对应的服务实现了fever api，就可以使用，比如freshrss、tiny tiny rss。\n\n进入软件后，会通过服务提供的fever api拉取订阅源。当点击开始阅读文章时，也会通过fever api发送请求，在web服务端将文章标注为已读，这样不管在桌面还是手机，都可以获取到相同的阅读状态。\n\n## freshRSS —— RSS管理服务\n\n一款开源的rss源管理软件，支持多用户、分类、打标签，以及丰富的订阅配置，比如数据清理配置。\n\n最重要的是提供了**fever api**、google reader compatible api，让支持这些api的阅读器可以方便代理。\n\n## RSShub —— RSS集散地\n\n一个开源易扩展的rss生成器，简单来说：让万物皆可rss。\n\n对于读者来说，它是一个rss集散地。\n\n对于开发者来说，它本质上是一个爬虫路由及解析服务。开发者可以通过got、cheerio等库，来抓取网页数据，交由rsshub生成rss源。\n\n## RSShub radar —— RSS探测器\n\n一款用于探测rss源的chrome插件。\n\n比如你访问某个知乎用户的主页，rsshub radar会自动探测页面里的rss源，并且可以一键添加进freshrss\n\n# 参考资料\n\n1. [打造完美的rss方案（一）：器](https://juejin.cn/post/7108731159017685006)\n2. [Docker系列 安装个人RSS服务TTRSS 手机完美适配](https://blognas.hwb0307.com/linux/docker/788)\n\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["RSS"]},"/CS/RSS/RSS-Reader":{"title":"RSS Reader","content":"\n|客户端|平台|是否收费|最短刷新时间|优点|缺点|\n|--|--|--|--|--|--|\n|Reeder 5|Mac/iOS|10$||界面美观|收费|\n|inoreader|web/iOS|可免费使用|实时/10分钟|功能丰富||\n|Tiny Tiny RSS|web|免费|15分钟|自建||\n|Feeder|web/浏览器扩展|免费|1分钟|||\n|Feedbro|浏览器扩展|免费|5分钟|||\n|NetNewsWire|Mac/iOS|免费|10分钟|||\n|FreshRSS|web|免费|20分钟|自建||\n|miniflux|web|免费|无法设置|自建、简单||\n|fluent-reader|全平台|免费|10分钟|||\n|QuiteRSS|Mac/Win/Linux|免费|1秒||复古，太丑|\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["RSS"]},"/Inbox/%E5%99%AA%E9%9F%B3":{"title":"噪音","content":"\n# 楼上\n以前没感觉到楼上吵，但搬到现在这个小区后，楼上吵的不行，噪音不断，真的受不了。\n\n1. 砸地板\n2. 跑动\n3. 拖家具\n4. 走路声音也特别大\n\n# 音乐\n1. 小区有弹钢琴的，而且不关门窗，声音传播好远。\n2. 以前小区隔壁住户天天从晚上6点到8点，唱歌两个小时，风吹雨打从不休息。\n\n# 办法\n1. 找物业帮忙\n2. 找居委会帮忙\n3. 加微信告知不要再吵了\n4. 反击 - 共振音响\n\n# 心态\n1. 杀不死我的必使我强大\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Inbox/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99":{"title":"学习资料","content":"\nBilibili\n\niPad、英语等技巧、资源\n\n[【iPad学英语整合版】听·说·读·写：优秀App+技巧+白嫖资源！一个视频全囊括！](https://www.bilibili.com/video/BV1LB4y1U7sV?vd_source=95893bf3c956973d41f5d92572ef8a93)\n\nobsidian\n\n[程序员使用 Obsidian 的经验 --- 我的工作流](https://www.bilibili.com/video/BV18Y4y1H7Gu?vd_source=95893bf3c956973d41f5d92572ef8a93)\n\n主题是哪个\n\n英语\n\n[【Udemy IELTS 排名第一的课程】雅思 7+ 备考课程](https://www.bilibili.com/video/BV1PN4y1K7Hk?vd_source=95893bf3c956973d41f5d92572ef8a93)\n\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Inbox/%E5%B7%A5%E5%85%B7%E5%88%86%E4%BA%AB":{"title":"工具分享","content":"\n# 基础设施\n\n## 服务器\n\n### deskmini 介绍\n\n![[Assets/Pasted image 20230416112309.png]]\n\n1. 小型化APU平台，体积仅1.92升\n2. 最大支持内存64G\n3. 2x M.2 PCIe Gen3 x4 + 2x 2.5\" SATA 6Gb SSD/HDD\n4. 可以将其中一个M.2接口转接有线网卡\n\n### 具体配置\n\n1. CPU:  AMD Ryzen™ 5 3400G with Radeon™ RX Vega 11 Graphics（938 ¥）\n2. MEM: DDR4 64G 3200MHz （800 ¥）\n3. 准系统+机箱：DeskMini A300 （987 ¥）\n4. 硬盘: 256G（NVME） + 4T（2x SATA 2.5\" SSD）（265 ¥ + 1800 ¥）\n5. 风扇: 猫头鹰 NF-A9x14 PWM 9cm （125 ¥）\n\n总价：4915 ¥\n\n纯SSD硬盘+猫扇：静音，几乎没有声音，可直接放在卧室。\n\n### 操作系统\n\n#### Gentoo Linux\n\n优点：\n1. 从源码构建\n2. 稳定性高\n3. 自由度高\n4. 滚动升级\n5. 软件包多且新\n\n缺点：\n1. 从源码构建费时、费电\n2. 安装、配置复杂\n\n#### openwrt\n\n无须自己编译，直接使用别人构建好的镜像与软件包，安装软件跟其他linux系统一样简单\n\n1. 代理：openclash\n2. 加速器：uu游戏加速器\n3. VPN组网：zerotier/tailsacle\n\n### 网络拓扑\n\n![[Assets/Pasted image 20230416211344.png]]\n\n### 软件\n\n1. 文件系统：ext4(btrfs) + zfs\n2. 网络文件共享：NFS + samba\n3. 系统管理：smartd, nginx等\n4. 虚拟机：libvirt + qemu + kvm\n5. 下载：aria2 + qbittorrent\n\n### 容器\n\ndocker + k8s\n\n1. 基础设施：cert-manager, ingress, metallb, prometheus-stack, EFK\n2. 媒体服务器：plex/emby/jellyfin\n3. RSS： Awesome TTRSS + RSSHUB\n4. 管理面板：portainer + kubesphere\n5. 数据库：PostgreSQL, Redis, Elasticsearch, couchdb\n6. 其他应用：vaultwarden/bitwarden, umami等\n\n### 缺失\n\n1. 图片服务器\n2. 漫画服务器\n\n## 其他\n\n### YubiKey\n\n介绍：[[CS/安全/玩转Yubikey]]\n\n配置：[[CS/安全/YubiKey配置]]\n\n## 架构图\n\n![[Assets/Pasted image 20230416200045.png]]","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Inbox/%E6%9E%81%E5%B7%A6%E5%B7%A6%E6%B4%BE%E5%8F%B3%E6%B4%BE%E6%9E%81%E5%8F%B3%E7%9A%84%E5%8C%BA%E5%88%86%E4%B8%8E%E7%8E%B0%E7%8A%B6":{"title":"极左、左派、右派、极右的区分与现状","content":"\n# 西方的左右派划分标准\n\n法国大革命的口号非常动听，叫“自由、平等、博爱”。但任何激动人心的口号都有一个缺点，就是经不起推敲。每个人的天资、生存环境都是不同的，如果让每个人都“自由”发展，那么他们的财富、地位就不可能平等。如果要让每个人都在经济上“平等”，那么必然会限制强者的自由以保障弱者。左右派起源于法国制宪会议，但很快定型成与初始含义毫不相干的两个集团。其中左派比较支持平等，强调建设福利国家，更多的通过国家干预手段帮助弱者，右派比较强调自由，反对过高福利，比较支持竞争，反对国家干预，强调建立“弱”政府，反对对于强者的过多限制。但左派和右派的区别只基于对平等与自由的偏重上。左派更偏重平等一点，右派更偏重自由一点。对基本限度的平等与自由权利，均持有同样的共识。\n\n什么是极左，什么是极右所谓极左，就是把左派的思路推向极端，突破“自由的底限”。为获得无差别的公正，而取消绝大部分的自由，为取消绝大部分的自由，必须建立一个无比强大的国家机器，将人民的一切活动处于国家的控制之下。所谓极右，如果把右派的思路推向极端，突破“平等的底限”。把反对国家限制强者推演成要强者控制国家欺凌弱者，宣称“国家就是为强者存在的”（斯托雷平），实行寡头专政，取消对弱者的一切保护，一切自由。\n\n  \n# 为什么说极左制度是伪公平？\n\n极左的目的是为获得经济上无差别的公正，但由于每个人能力、背景各不相同，要压制每个人的个性寻求公正，就必须实行极权。这样尽管每个人在经济上基本平等，但极权会造成权力的不平等。位高权重的，呼风唤雨，无所不为。地位卑贱的，连性命都无法保障。在权力倾轧中被淘汰下来的，往往境遇悲惨。这些大伙都很熟，我们曾经在这种制度下生存了很长时间。\n\n为什么说极右制度是伪自由？极左到极右的角色变换这才是我想讲的东西。作为七十年代末八十年代初的一代人，小学的时候，学的是公有财产神圣不可侵犯，个人利益服从集体利益，甘做螺丝钉和驯服工具。初中的时候，学的是邓小平同志的英明论断：中国不可能出现百万富翁！高中的时候，一切都颠倒了，一部分人“先富起来”了，国有企业“卖给私人”了，工人阶级要“自己养活自己”了。上大学以后，很无奈，中国的贫富差距已经变成世界第一了。\n\n极右制度不合理的关键在于忽视“起点平等”。刘少奇曾经握着淘粪工人时传祥的手，笑着说：“我们没有高低贵贱之分，只有分工不同。”在一个极左制度下的工厂里，虽然厂家资金的实际支配权在厂长和书记这里，但名义上是属于大家的。忽然有一天，分家了，厂长和书记拿到了厂，原先许诺给工人们的退休工资和医疗保障全都作废了，工人们每人拿到了几千元分家费。厂长对工人们说：我们现在不搞大锅饭了，大家今后要自由竞争！话虽好听，可这种“分家”方案，这种取消弱者的一切社会保障，取消一切退休金、医疗保险的“自由竞争”，难道真会是“自由”的竞争吗？\n\n极右制度，往往表现为权贵资本主义与寡头专政。南美、东南亚模式可为前鉴。极右与右派的距离很远，离极左却是咫尺之遥。极左与极右有相同的“根”，在极左制度中，国民的财产名属全民，而支配权属于权力中心，转变成极右制度很简单，只要把“全民所有”的遮羞布拿下来就是了，直接依靠权力化公为私。\n\n  \n# 左右翼分派混乱的原因\n\n大陆的左派、右派名词来源与欧洲不同，在中国大陆，派别的划分都是以政府为参照系的。由于历史上政府一直是极左，因此在人们思想上有一个惯性：完全支持政府的就是极左，大部分支持政府的是左派，反对政府的是右派。可以说在九十年代之前，这种划分都是比较合理的。\n\n但现在情况变了，大家都能看到。农民问题、失业工人问题、学生就业问题，基本上都是自由主义者提出来的。按常理，自由主义应该属于右翼阵营，对平等问题的关注较弱。但在国内，连他们都开始关注平等问题，表现得“左”了。说明目前的参照系已经偏向极右。\n\n极左阵营一分为二。有一部分人停住了追随变革的脚步。如果说工人失业、资本家入党还可以被认为是“阵痛”和“权益之计”的话，国有资产的快速私有化却是令人心下雪亮。有些地区，在九十年代末，私有经济比重还只有百分之十几，但过了四五年，就上升到百分之五十到八十。这可不是什么私有经济的“优越性”，而是大家心知肚明的国有财产瓜分。私有经济再“优越”，也不可能几年就翻上几倍的。这些是目前坚持极左的“毛派”反对“邓派”的基础。\n\n福布斯在二零零一年给出了中国富豪排行榜，中国大陆有形形色色的排行榜，但绝没有这张有用。中国公安们就按着这张排行榜一个个查下来，富豪们纷纷入狱。我可以一个个扳着指头数下来：在排行榜上位居第二的杨斌，通过奇迹性的行政“划拨”到3000亩土地获利七十多亿，贵为朝鲜特区行政长官的身份，在吉林被捕。在排行榜上位居第三的仰融，在华晨的权钱交谊中“栽了跟头”，琅珰入狱。不多举例，大伙也能知道是哪批人“先富起来”了。\n\n极左分裂了，不少人可以归为极左与极右派系分裂。极左称为毛派，已经失去了实际的政治权力，转移到网上成为另一类反对派。现在有些网友看见极左派和右派都在批评政府，就想当然地认为执政者是中间派，其实不然。还有一点不能忽略的是：不少极左派系转型成为民族主义派系，我认为他们的转型是为了逃避面对国内现实问题。骂日本骂美国，多容易呀，多安全呀，也不需要什么判断力，中国做的就是对的呗！哪有谈国内问题那么难？\n\n  \n# 讨论假问题的知识分子\n\n中国知识分子除了“吃苦耐劳”，没什么特别的优点。缺点倒是很多，攀附权贵，空谈，抄袭，寡廉鲜耻的知识分子是屡见不鲜。现在是好点了，独立的，面对现实的知识分子越来越多，但我还得说上几句，中国一些善良又独立，还有点学者风范的知识分子有另一个特点，就是喜欢“白日做梦”。\n\n怎么“白日做梦”呢？当权力迅速腐化，中国开始进行以权换钱的“原始积累”的时候。学者们开始憧憬“中产阶级”的产生会给中国“送来”民主制度和自由市场经济。当香港回归的时候。学者开始幻想“香港的多党制”会“普及”大陆。当工人纷纷下岗，贫富严重分化的时候。学者们又会认为“威权体制”下的经济增长将是“民主化”不可逾越的短暂阶段。他们从来不想，不去自己争取权益，不去推动制度民主化建设，不去抨击社会的不公，难道自由民主与公正会随着“经济增长”像天上掉馅饼一样落到国民手里吗？远看欧美各国，近看台湾韩国，民主化进程都是血与泪凝成的，好东西不会光顾睡大觉的民族。不去追求民主，哪来的民主？不去追求自由，哪来的自由？不去呼唤平等，哪来的平等？企图等着经济发展后制度“水到渠成”，笑话，没看见别人的渠都是自己挖的么？不挖渠，水到了只会把人淹死。\n\n九十年代末期的知识界现象，叫作“自由主义与新左派的对话”。看似与国外右派与左派的对话相似。但对些什么呢？中国太平等了？中国太自由了？国内没几个人能听懂“新左派”的“后现代”论述。想想也是，跟一个吃不饱饭的人谈减肥，他能听懂么？自由主义也面临“少谈公正”的指责而纷纷转型。以前叱咤风云的厉以宁，被人指责为权贵辩护。当人们越来越关注穷人的时候，谈股份、谈市场争夺、谈MBA，意义便明显褪色了。\n\n俺认为现在左和右的“对话”根本没有意义。左派与右派根本没有“对话的必要”。自由多一点平等少一点，还是自由少一点平等多一点，这种讨论在中国毫无意义。真正要做的是建立“自由与平等的底限”。在一个既不自由又不平等的社会，谈哪个多哪个少不是“空谈”又是什么呢？\n\n阻止极右倾向可能为时已晚目前什么是中国最大的经济现象？我认为就是“私有化”。对此网上早已直言无忌，也有一些报刊胆子比较大，敢于直呼“私有化进程”。现实中的大多数媒体要遮掩一些，换个说法，什么“改制”、“转制”、“股份化”、“鼓励私有成分”。其实都是一个意思。\n\n我不谈怎样“阻止”私有化。长期极左造成权力不受制约，同样这种权力“市场化”、权贵“资本化”，国民也已无力量制约。利益与不受制约的权力促成极左到极右的转变而无可阻挡。在不可能阻止私有化的情况下，知识分子应该呼唤的，就是保证这种“私有化”能够尽量公平。不要出现那种私有化：厂长书记拿到了厂子，工人一次性下岗。然后大家开始在“公平的市场”中进行“平等竞争”。这种分家最后只会造成社会动荡和经济下滑。类似的例子可以在苏东私有化中看见。分家分得比较公平的东欧国家，经济在短期下滑以后立刻回升起飞，而做得不好的俄罗斯等国，则造就金融寡头与垄断集团，经济低迷很久才逐渐回升。\n\n目前私有化的关键就是在国有资产被分光以前，建立一个比较公平的“分家”策略。但从现在经济比重来看，国有资产已经被分掉了将近一半，对“分家”策略的讨论尚未开始。\n\n  \n# 中国特色？中国没有特色！\n\n绝对不要相信“文化”会造成经济问题的解决方法不同。经济也许不是“制度决定”。但制度对经济的影响力，远远超过文化对经济的影响力。台湾与香港的经济制度与规律，离美国近而离同种文化的大陆远。东德与西德，南韩与北韩，经济实体的差异程度，与文化的近似程度恰成对比。广东企业与北京企业的相似程度，远远超过广东和广西企业的相似程度。决定经济的仍将是制度，我们中国不会因为“文化不同”而走上与其它国家不同的经济道路。\n\n拿一个影响最广的误解来谈，曾经吹得神乎其神的乡镇企业。九十年代一度被有很多人认为，乡镇企业是中国文化的“伟大创造”，是世界经济的“第三条道路”，农民企业家是中国独一无二的经济现象。我本科的时候，在北大听过不少讲座，这种观点早已让人耳边起茧。但九十年代末，乡镇企业集体“进城”和潮水般的民工以实际行动嘲弄了这种“发现”，以至于现在都没人提乡镇企业了。其实多看历史，就可以发现乡镇企业“似曾相识”。实际上这是国家转轨的一种现象，在政府的经济控制力减弱，而农奴制依然保留的情况下，乡镇企业就会大量涌现。\n\n在十九世纪中期的俄罗斯，工业化已经起步，而依然保留了农奴制。在农奴制下，农民并不是一种职业，而是一种世袭身份。由于在农奴制下，俄罗斯通过划分“农民身份”与“非农民身份”限制了农民进城，一批农民就通过工业化形成了整个整个村庄的乡镇企业与大量的家族制的“农民企业家”。俄罗斯涌现了如莫罗佐夫家族、格拉乔夫家族、鲍里索夫家族等“农民企业家”，乡镇企业也如雨后春笋，以纺织闻名的莫斯科省的伊凡诺沃村，以冶金闻名的科斯特罗马省的达尼洛夫村，以制鞋业闻名的特维尔省的基拉姆村。农奴制改革后，这种现象就渐渐消失。中国也是一样，当放宽了农民进城限制以后，大量农民从乡镇企业中涌出，纷纷进城务工，形成“民工潮”。“中国文化的伟大创造”不攻自破。\n\n同样，在国有资产私有化中，中国也不会因为“文化”而有所不同。分家的公平与否直接影响今后的社会稳定。如果仍旧像现在这样，政府不断地涌现亿元量级的腐败大案，而又以经济困难的理由取消了下岗工人的退休金和“没有失业”的庄严承诺，开始“自由竞争”。如果仍旧像现在这样，各种工程一投就是上百亿，而九八年百年一遇的洪灾农民只能分到每人每月三十元的“安家费”。如果仍旧像现在这样，一方面不断“扩招”以实现“教育产业化”，另一方面大学生失业率居高不下，在学习期间打工陪聊，女大学生向百万富翁们“投怀送抱”。那中国文化的“熏陶”并不会使得农民、工人、知识分子们变得特别“稳重、深沉、善良”的。\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Inbox/%E9%87%91%E5%8F%A5%E6%94%B6%E8%97%8F":{"title":"金句收藏","content":"\n1. 用心若镜，不将不迎。\n    \n    这句话出自庄子《应帝王》篇，讲的是列子学道的故事。列子拜师壶子学道，好像没有学到什么一样回家了，之后三年不出门。每天帮着妻子烧火做饭好像正经工作一样，每天喂猪好像伺候人一样，各种事情在他这里不分亲疏也没有偏私，这便是“用心若镜”。\n    \n    镜子内本身是空无一物的，心好像镜子一样，也就是说心中没有自己的主观偏私之见。喂猪和伺候人，在我眼中没什么区别；烧火做饭和正经工作，对我来说都同等对待。所以并不会对喂猪产生抗拒，也不会对烧火做饭不耐烦。事来了，我用一样的心境去应对处理而已。\n    \n    这样首先，我不会对来寻我的物（包含事），产生抗拒心理。比如碰到自己不喜欢的事就不想做，或者往后拖。它来了就来了，走了就走了，我都能坦然接受，这就叫不将不迎。\n    \n    其次，我的心会平和安宁，不与外物相构斗，不会因为它们而产生相应的情绪，这样就不会耗散心神。表现出来就是，做事心不累，不会一边不想做，一边又不得不做，而痛苦不堪，产生深深的厌烦。\n    \n    [https://zhuanlan.zhihu.com/p/362220510](https://zhuanlan.zhihu.com/p/362220510)\n    \n2. 如果一个人不想做某件事，通常不是由于客观条件不允许，而是他有下面四种心态之一：恐惧（Fear）、排斥（Rejection）、自卑（Low self-esteem）、怠惰（Laziness）。\n3. 杀不死我的必使我强大\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Inbox/%E9%AB%98%E6%95%88%E4%BC%91%E6%81%AF":{"title":"高效休息","content":"\n![image.png](Assets/image_1664737228970_0.png)\n\n[【你为什么总是很累?】如何高效休息!](https://www.bilibili.com/video/BV1GG411V7ye?vd_source=95893bf3c956973d41f5d92572ef8a93)\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Inbox/Yoga-Nidra":{"title":"Yoga Nidra","content":"\n# 什么是Yoga Nidra?\n\n![image.png](Assets/image_1665148601053_0.png)\n\n## 1种精神状态\n\n介于清醒和睡眠之间的状态\n\n## 1种引导式冥想\n\n比静坐式冥想更易入门，减少因失败带来的挫败感。\n\n## 1种高效的放松方式\n\n据说45分钟的Yoga Nidra，约等于4个小时的深度睡眠。\n\n瑜伽式睡眠，每90分钟，休息20分钟\n\n# Yoga Nidra有什么好处？\n\n* 缓解焦虑、抑郁和各种心理障碍\n* 减少失眠、提高睡眠质量\n* 提高认知能力\n* 增强学习能力和记忆力\n* 增强免疫系统功能\n\n# 如何进行Yoga Nidra？\n\nYoga Nidra的步骤可能因引导老师不同而各有所异，但基本都包含：\n\n跟随老师的引导语，进行以下动作。\n\n1. 坐着，或者平躺在垫子上，身体呈“大”字\n3. 闭上双眼，深呼吸，放松身体\n4. 慢慢地移动注意力，扫描身体的每一个小部位（从头到腹至每一根脚趾头～）\n5. 深呼吸，缓慢地睁开眼睛，坐立起来\n\n![image.png](Assets/image_1665148637099_0.png)\n\n# 揭秘Yoga Nidra的科学原理\n\n## 激活副交感神经系统\n\n![image.png](Assets/image_1665148677225_0.png)\n\n激活副交感神经系统，会抑制「皮质醇」的产生，促进「血清素」、「GABA」和「催产素」的合成。\n\n压力荷尔蒙「皮质醇」和快乐递质「血清素」，已经介绍过很多次。\n\n「GABA」是大脑内的抑制性神经递质，可以减缓神经元的信号传递。在压力事件中，可以缓解焦虑情绪、肌肉紧张和情绪起伏。\n\n「催产素」则在母婴联结、浪漫感情维系、性关系中发挥作用。当作用于大脑时，可以帮助调节血压、缓解压力、帮助伤口愈合，并产生安全、舒适、冷静的感觉。\n\n## 改变脑电波\n\n当跟随着老师的引导语，逐个感受身体的各个部位时，你的大脑将从**β脑波状态**（大脑活动频繁的觉醒状态）转变为**α脑波状态**（大脑活动速度放缓），随着实践的深入，脑电波还可能进入**θ脑波状态**（器官进行自我修复），即可得到非常深度的放松，以及进入创造力迸发、记忆巩固的阶段。\n\n# 资源\n\n1. [20分钟冥想=1小时睡眠 YogaNidra古老的卧姿冥想，即使睡不着，依然能恢复精力。推荐的练习时间：午休时，晚上睡前，半夜醒来睡不着，早上醒太早又很困。](https://www.bilibili.com/video/BV1Br4y1D7P4?vd_source=95893bf3c956973d41f5d92572ef8a93)\n2. [20分钟冥想=2小时深度睡眠，YogaNidra躺平疗愈冥想引导加长版，脑科学研究最有效的深度放松](https://www.bilibili.com/video/BV1i44y1j7WL?vd_source=95893bf3c956973d41f5d92572ef8a93)\n3. [躺平吧！ 10分钟冥想=1小时睡眠，午睡神器瑜伽大休息术，脑科学研究最有效的深度放松，Yoga Nidra](https://www.bilibili.com/video/BV1tL411775C?vd_source=95893bf3c956973d41f5d92572ef8a93)\n4. [Yoga Nidra 20 Minute Guided Meditation](https://www.youtube.com/watch?v=7H0FKzeuVVs)\n\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Inbox/obsidian":{"title":"obsidian","content":"\n# 教程\n\n- MOC\n\t- [Obsidian教程｜5步打造个人写作系统\\_哔哩哔哩\\_bilibili](https://www.bilibili.com/video/BV1yg41157rB/?spm_id_from=333.788\u0026vd_source=3d08273269e6ecf3831e425417a2cfa9)\n- bullet journal\n\t- [基于 Logseq 重构个人知识管理体系 - gq's blog](https://zgq.ink/posts/knowledge-base-refactoring)\n- 卡片笔记\n\t- [ ] 到底什么是卡片笔记？\n\t - 教程\n\t\t- [找笔记方便、写作又给力的知识库，我是这么搭建出来的 - 少数派](https://sspai.com/post/77144)\n\t- 方法论\n\t- 1-闪念笔记\n\t\t- 与 daily note 有些类似，使用时间戳记录灵感、感悟（较短的笔记）\n\t\t- 但感觉 daily note 更加零散，什么都可以往里放，而每条闪念笔记都有一个明确的主题\n\t- 2-文献笔记\n\t- 3-永久笔记\n\t\t- 有点像写一篇实用的 blog\n- LYT\n- 插件\n\t- [ChatV: Obsidian，可能是最适合做个人知识库的软件 —— 备注：本帖将持续更新。有需要的即友，可转发或收藏，便于查看更新。 —— 配置好的 Obsidian，与刚开始接触的 Obsidian，看起来完全像两个不同的东西。 刚开始用的时候，我不禁疑惑：就这？网上很多人称赞，但我觉得这不就一 Markdown 编辑器吗，连 Typora 都不如。 但是 - 即刻](https://web.okjike.com/originalPost/6317dd9989e1f3feb56f30b2)\n\t- [Obsidian 新手系列之你不可不知的插件 - 经验分享 - Obsidian 中文论坛](https://forum-zh.obsidian.md/t/topic/194)\n\t- [obsidian插件之dataview入门 - 经验分享 - Obsidian 中文论坛](https://forum-zh.obsidian.md/t/topic/195)\n\t- [Obsidian最强插件：quickadd :: 喜于微](https://lillianwho.com/posts/obsidian/obsidian%E6%9C%80%E5%BC%BA%E6%8F%92%E4%BB%B6quickadd/)\n\t- [我的 Obsidian 工作流：模板+QuickAdd+Dataview 快速创建和自动索引 :: 喜于微](https://lillianwho.com/posts/obsidian/obsidian%E5%B7%A5%E4%BD%9C%E6%B5%81/)\n\t\t- 闪念笔记：随时记录闪念，到当天日记。如果有需要展开的闪念，则在时间充裕时单独做一张卡片。\n\t\t- 文献笔记：即阅读和观影笔记。阅读产生的思考会在阅读完之后拆分到新的卡片。\n\t\t- 项目笔记：为写作而服务，分为灵感提案、写作中的文章和已完成的文章。\n- 综合\n\t- [Obsidian文档咖啡豆版 | obsidian文档咖啡豆版](https://coffeetea.top/zh/)\n\t- [2021年新教程 - Obsidian中文教程 - Obsidian Publish](https://publish.obsidian.md/chinesehelp/01+2021%E6%96%B0%E6%95%99%E7%A8%8B/2021%E5%B9%B4%E6%96%B0%E6%95%99%E7%A8%8B)\n-  工作流\n\t- [工作流初探 | obsidian文档咖啡豆版](https://coffeetea.top/zh/workflow/workflow-guide.html)\n- 三大原则\n\t- 链接\n\t\t- link first, folder least.\n\t\t- 尽量少的建立层级\n\t\t- 通过“中枢”方式创建新笔记\n\t- 标签\n\t\t- 标题分类，树状、namespace 类似\n\t\t- 标签位置，font matter 或最前、最后\n\n# 文件夹结构\n\n![[Assets/Pasted Graphic.png]]\n\n# 插件\n\n![[Assets/Pasted Graphic 1.png]]\n\n# 快捷键\n\n- `Ctrl/Cmd-E` ：编辑、预览模式切换\n- 按住 `Ctrl/Cmd` 点击链接：在新面板中打开该链接\n- 编辑模式下按住 `Cmd` 再单击链接：跳转链接\n- `Ctrl/Cmd-alt-左方向键` 或 `鼠标侧键`：回到之前的页面\n- `Ctrl/Cmd-P`：打开命令面板\n- `Ctrl/Cmd-Enter`：切换 list 和 todo\n- `Ctrl/Cmd-T`：编辑或新建 task\n\n# TODO\n\n- [ ] 整理 obsidian 正确使用姿势以及方法论","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["Inbox"]},"/Journal/2023/04/4%E6%9C%8816-2023-%E5%91%A8%E6%97%A5":{"title":"4月16, 2023, 周日","content":"- 明天继续处理自定义工作流 [[Work/Zadig/服务变量]]\n\t- 云器bug初步找到原因，待复现\n\t\t1. 使用Studio 3T导出数据库\n\t\t2.  在product表里查找到，`{ \"product_name\" : “${projectName}, \"env_name\" : “${envName}“ }`，导出\n\t\t3.  在上面查到数据中，找出`render.revision和services[0].service_name`和`revision`\n\t\t4.  在render_set表里查找，`{ \"product_tmpl\" : \"${projectName}\", \"revision\" : NumberLong(2) }`，导出\n\t\t5.  在product_template_service/template_service表中查找，`{ \"service_name\" : \"${serviceName}”, \"product_name\" : \"${projectName}, \"revision\" : NumberLong(1) }`，导出\n\t\t6. 还需查看操作日志，查看是怎么部署的\n\t- 变量合并无法解决，需要更新设计","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Journal/2023/04/4%E6%9C%8817-2023-%E5%91%A8%E4%B8%80":{"title":"4月17, 2023, 周一","content":"- 自定义工作流[[Work/Zadig/服务变量]] 开发、调试\n\t- 配置自定义变量和执行时获取值的接口没有分开，耦合太严重","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Journal/2023/04/4%E6%9C%8818-2023-%E5%91%A8%E4%BA%8C":{"title":"4月18, 2023, 周二","content":"- 发版回归测试\n\t- [[Work/Zadig/测试资源]]\n\t\t- 测试脚本\n\t\t\t- https://zadigx-demo.koderover.com/v1/projects/detail/yaml-poc/test/function/pytest-yaml 。demo/demo123。不要执行写操作\n\t\t- 机器人通知\n\t\t\t- 企业微信机器人地址：https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=dc80b1ee-49fc-4c11-b474-c29ae2d04f31\n\t\t\t- 钉钉机器人地址：https://oapi.dingtalk.com/robot/send?access_token=b9c3d9236ef9d958d68ecbfb9aa60fbc7e3761457ac8bb1b29fd3892a805c6f2\n\t\t\t- 飞书机器人地址：https://open.feishu.cn/open-apis/bot/v2/hook/cfc3d4f4-ab80-4529-9dd5-8b45b46412ce\n\t\t\t\t- Lilian的飞书userID：c9b6e25c\n\t\t\t- 文档：[https://docs.koderover.com/zadig/dev/project/workflow/#im-%E7%8A%B6%E6%80%81%E9%80%9A%E7%9F%A5](https://docs.koderover.com/zadig/dev/project/workflow/#im-%E7%8A%B6%E6%80%81%E9%80%9A%E7%9F%A5)\n\t- 自定义工作流，固定环境，env中带有\u003c+fixed\u003e前缀，造成获取product出错\n\t- FilterEnv接口500报错\n\t\t- ![[Assets/CleanShot 2023-04-18 at 21.08.31@2x.png]]\n\t- filterEnv检查tmplSvc是否为nil","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Journal/2023/04/4%E6%9C%8820-2023-%E5%91%A8%E5%9B%9B":{"title":"4月20, 2023, 周四","content":"- 尝试[[Work/Zadig/安装|安装]]zadig，helm和plutus costomer\n- 构思[[Work/Zadig/服务变量|变量]]新设计","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Journal/2023/04/4%E6%9C%8821-2023-%E5%91%A8%E4%BA%94":{"title":"4月21, 2023, 周五","content":"- 讨论变量设计\n- 错误 revision bug 处理","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Journal/2023/04/4%E6%9C%8823-2023-%E5%91%A8%E6%97%A5":{"title":"4月23, 2023, 周日","content":"- [[Work/Zadig/服务变量]]新设计\n\t- unmarshal 数组 yaml 可以实现，但需要和 map 类型进行区分，合并到 parent 也没有问题\n\t- yaml 转 kv 还是原来方式？合并只显示第一层的值，完全需要前端来说，后端还是不变？kv 格式是否要改变？\n\t- 确认 KV 的作用\n\t- extract 模版第一层 key 验证可行\n - revision 错误问题\n\t - 是由于自动更新，该功能原本只有测试服务才有，它更新了所有环境的同名服务造成的，包括生产环境，覆盖了原本生产环境中正常的 revision，复写成了测试服务的 revision\n\t - 检查了所有查询 product 的地方，添加了仅限测试服务的条件\n\t - 查不到tmplSvc需要返回报错而不是panic\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Journal/2023/04/4%E6%9C%8824-2023-%E5%91%A8%E4%B8%80":{"title":"4月24, 2023, 周一","content":"# Notes\n- 继续研究 [[Work/Zadig/ChatGPT]]\n\t- Azure OpenAI\n\t\t- https://azure.microsoft.com/zh-cn/products/cognitive-services/openai-service\n\t\t- 微软 Azure 是 OpenAI 独家云服务提供商，可在云平台上直接调用 OpenAI 模型，包括 GPT-3.5、Codex 和 DALL.E 模型。\n\t\t- GPT 不支持微调模型。但其他模型支持。\n\t  - Chat2SQL\n\t\t   - 生成 SQL\n\t\t\t   - 接收用户的自然语言查询请求，例如“每个品牌的退款额是多少”；\n\t\t   - 解释 SQL\n\t\t\t - “这段 SQL 是否可做进一步性能优化”\n\t\t   - 浏览器插件形式\n\t  - OpenAI Codex\n\t\t  - Github Copilot 使用的就是这个\n\t\t  - Codex 可以总结已编写的函数、解释 SQL 查询或表，以及将函数从一种编程语言转换为另一种编程语言。\n\t -  ChatGPT + 向量数据库（vector database）+ prompt-as-code\n\t\t - 提供回答准确性\n\t\t - 向量数据库性能不好\n\t - GPT 模型非常擅长完成若干自然语言任务，其中包括：\n\t\t - 汇总文本\n\t\t - 对文本进行分类\n\t\t - 生成名称或短语\n\t\t - 翻译\n\t\t - 回答问题\n\t\t - 建议内容\n\t - 轻量使用场景\n\t\t - codex 补全 yaml、~~基于我们提供的 yaml 例子，进行工作流 yaml 格式校验~~，工作流 yaml 解释\n\t\t - 构建脚本 lint、解释\n\t - 模型微调\n\t\t  - [Fine-tuning - OpenAI API](https://platform.openai.com/docs/guides/fine-tuning)\n\t\t  - 可以训练数据\n\t - 插件\n\t\t  - 插件可以获取实时数据，获取天气、股票、新闻等；检索知识库信息，像公司文档或个人笔记；代表用户执行操作，像是预定机票，点外卖等。\n\t\t  - [Introduction - OpenAI API](https://platform.openai.com/docs/plugins/introduction)\n - 继续讨论[[Work/Zadig/服务变量]]细节\n\t - 变更太多，变量涉及、入口太多\n\t - 服务中有全局变量模块？那环境呢？\n\t - 数据变更确认\n\t - 接口变更\n\t\t  - 工作流获取变量接口\n- [[Work/Zadig/服务变量]]确定内容\n\t- 保存服务自动解析变量\n\t- 先做最核心的功能\n\t- kv 和 yaml 转换接口\n\t- 使用变量时，只以 kv 为主，yaml 做预览，不能修改\n\t- 全局变量模块是项目维度，环境里的以项目的全局变量定义为主\n\t- 细粒度可见性如何聚合成以 prefix 为主的粗粒度可见性\n\t- 修改服务变量引用全局变量，增加版本号？更换了设计\n\t- 环境需要展示全局变量关联关系\n\n# TODO\n- [ ] 解决 ssh remote 在命令行中，每次历史记录都消失的问题\n- [x] 添加每日日志模版 ✅ 2023-04-29","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Journal/2023/04/4%E6%9C%8825-2023-%E5%91%A8%E4%BA%8C":{"title":"4月25, 2023, 周二","content":"# TODO\n\n- [ ] 整理mongodb文档","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Journal/2023/04/4%E6%9C%8826-2023-%E5%91%A8%E4%B8%89":{"title":"4月26, 2023, 周三","content":"- 服务变量 kv 与 yaml 转换\n\t- 调试、自测\n- swagger 支持\n- 变量新文档\n- [[Work/Zadig/测试]]超时错误 bug\n\t- 不知道在测试页面可以执行task\n\t- 回归测试不完善\n\t- 没有交叉测试\n\t\t- 首先自测完备\n\t\t- 理解需求\n\t\t- 思维碰撞，每个人想法的差异\n\t- 测试多花一些时间\n\t- 自测完善，自测完再回归再自测一遍没什么意义","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Journal/2023/04/4%E6%9C%8827-2023-%E5%91%A8%E5%9B%9B":{"title":"4月27, 2023, 周四","content":"- [[Work/Zadig/镜像名称]]问题\n\t- 梳理获取镜像列表的场景\n\t\t- Build 生成镜像，使用镜像的场景？\n\t\t- 自定义工作流\n\t\t\t- 部署\n\t\t\t- 镜像分发\n\t\t\t- k8s 部署\n\t\t\t- 红绿发布\n\t\t\t- 金丝雀发布\n\t\t- 产品工作流\n\t\t\t- 交付物部署\n\t- 梳理镜像生成规则\n\t\t- 没有设置代码源\n\t\t- 有设置代码源\n\t\t\t- 默认没有保存镜像规则\n\t\t\t\t- 使用 service name 作为镜像名\n\t\t\t- 保持了镜像规则\n\t\t\t\t- 根据镜像规则来设置\n\t\t- 镜像名应该不可改，否则会出现选择不到的情况。\n\t\t\t- 仅部署的情况下，无法获取构建生成的完整image\n\t\t\t- 存储的 imageName 没有按照生成规则来\n\t- 优化获取镜像名逻辑\n\t\t- 根据 image 解析 image name\n\t\t\t- image 解析可能和服务最新定义不一致。部署时使用哪个？\n- Ymal linter\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Journal/2023/04/4%E6%9C%8829-2023-%E5%91%A8%E5%85%AD":{"title":"4月29, 2023, 周六","content":"\n\u003c\u003c [[ Journal/2023/04/4月28, 2023, 周五|前一天的日记]]  |  [[Journal/2023/04/4月30, 2023, 周日|后一天的日记]] \u003e\u003e\n\n# 每日记录\n\n- 梳理镜像名称\n- 添加 obsidian 日志模版\n\n# 待办事项\n\n\n\n# 本日到期\u0008\n\n```tasks\ndue on today\n```\n\n# 本日完成\n\n```tasks\ndone 2023-04-29\n```\n\n# 未完成\n\n```tasks\npath does not include 4月29, 2023, 周六\nnot done\nsort by due reverse\nsort by description\n```\n\n# 灵光一闪\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["daily-notes"]},"/Journal/2023/04/4%E6%9C%8830-2023-%E5%91%A8%E6%97%A5":{"title":"4月30, 2023, 周日","content":"\n\u003c\u003c [[ Journal/2023/04/4月29, 2023, 周六|前一天的日记]]  |  [[Journal/2023/05/5月1, 2023, 周一|后一天的日记]] \u003e\u003e\n\n# 每日记录\n\n\n\n# 待办事项\n\n- [ ] 整理、合并、迁移 blog 到 obsidian\n\n# 本日到期\u0008\n\n```tasks\ndue on 2023-04-30\n```\n\n# 本日完成\n\n```tasks\ndone  2023-04-30\n```\n\n# 未完成\n\n```tasks\npath does not include 4月30, 2023, 周日\nnot done\nsort by due reverse\nsort by description\n```\n\n# 灵光一闪\n","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["daily-notes"]},"/Journal/2023/05/5%E6%9C%881-2023-%E5%91%A8%E4%B8%80":{"title":"5月1, 2023, 周一","content":"\n\u003c\u003c [[ Journal/2023/04/4月30, 2023, 周日|前一天的日记]]  |  [[Journal/2023/05/5月2, 2023, 周二|后一天的日记]] \u003e\u003e\n\n# 每日记录\n\n- 学习obsidian\n\n# 待办事项\n\n- [ ] 整理tags，树状层级\n\n# 本日到期\u0008\n\n```tasks\ndue on 2023-05-01\n```\n\n# 本日完成\n\n```tasks\ndone  2023-05-01\n```\n\n# 未完成\n\n```tasks\npath does not include 5月1, 2023, 周一\nnot done\nsort by due reverse\nsort by description\n```\n\n# 灵光一闪\n\n- 00:05 #笔记软件/对比\u003cbr\u003e\u003cbr\u003e其他人的使用 ：\u003cbr\u003e\u003cbr\u003eFlomo（灵感收集）、幕布（思维导图）、思源\u0026Obsidian（汇总研究）、语雀（发布分享）\n- 00:05 #博客/需求\u003cbr\u003e\u003cbr\u003e我希望blog能分为两部分，一部分是像wiki一样能根据目录分类的，一部分是跟普通blog那样按时间顺序的，最好格式还是MD的，有这种blog吗？\n- 00:07 #灵感/iPad\u003cbr\u003e\u003cbr\u003eiPad 用笔记软件学习？\n- 00:08 #灵感/iPad\u003cbr\u003e\u003cbr\u003eiPad软件：\u003cbr\u003e1. Notability\u003cbr\u003e2. GoodNotes\u003cbr\u003e3. Flexcil\u003cbr\u003e4. MarginNote \u003cbr\u003e5. Planner\u003cbr\u003e6. Prodrafts\u003cbr\u003e7. 欧陆词典\n- 00:08 #灵感\u003cbr\u003e\u003cbr\u003e需要深入的使用了解，才能了解一个事物，比如了解一款软件，就需要自己一段时间的使用，才能够深入的了解它的优缺点，而不是再停留在各种评测的表面印象中。\u003cbr\u003e\u003cbr\u003e光看文章还不够，还需要实践。\n- 00:08 #灵感/生活\u003cbr\u003e\u003cbr\u003e放送、自在，不要约束，不要受他人影响\n- 00:08 #灵感/成长\u003cbr\u003e\u003cbr\u003e人成长的过程，就是一个不断输入信息、分析信息、归纳总结并学以致用的过程 。\n- 00:08 #灵感/成长\u003cbr\u003e\u003cbr\u003e自我驱动，不断成长 ^h5odua\n- 00:10 #灵感/减肥\u003cbr\u003e\u003cbr\u003estay hungry\n- 00:10 #设计原则\u003cbr\u003e\u003cbr\u003e架构/设计原则：\u003cbr\u003e1. 高内聚松耦合\u003cbr\u003e2. K.I.S.S\n- 00:10 #iptables/排查\u003cbr\u003e\u003cbr\u003e1. 查看dmesg（如果某个包被挡了，dmesg里会有记录）\u003cbr\u003e2. tcpdump\n- 00:11 #golang/slice\u003cbr\u003e\u003cbr\u003e左闭右开\n- 00:11 #灵感/成长\u003cbr\u003e\u003cbr\u003e1. 万事开头难\u003cbr\u003e2. Just do it\u003cbr\u003e3. 从现在开始行动\n- 00:12 #灵感/言论\u003cbr\u003e\u003cbr\u003e生命太短暂，不能花在那些不值得阅读的内容上面。\u003cbr\u003e\u003cbr\u003e就算你是一个很爱读书的人，活到70岁最多大概能阅读15000本书，这只占世界最大图书馆美国国会图书馆3800万册藏书的0.04%。\u003cbr\u003e\u003cbr\u003e我们一生中能够阅读的书籍其实很少。因此，关键技能不是多读，而是跳过那些不值得读的内容。\u003cbr\u003e\u003cbr\u003e有些领域变化非常快，在有人写书之前，博客有时是唯一的信息来源。Stable diffusion 模型出现后的第二天，人们就已经在写博客了，书籍永远不会那么快。\u003cbr\u003e\u003cbr\u003e而且，博客往往是免费的，而书籍和论文则被锁定在付费墙之后。因此，你可以这么认为，博客获取灵感，书籍获取知识。\n- 00:12 #灵感/编程\u003cbr\u003e\u003cbr\u003e魔鬼藏在细节中\n- 00:12 #灵感/成长\u003cbr\u003e\u003cbr\u003e什么是能力？\u003cbr\u003e\u003cbr\u003e遇到问题的态度\u003cbr\u003e\u003cbr\u003e处理问题的思路和方法\u003cbr\u003e\u003cbr\u003e这就是能力","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":["daily-notes"]},"/Work/Zadig/%E4%BA%91%E5%99%A8":{"title":"云器","content":"云器导出数据库，软件Studio 3T\n\n1.  在product表里查找到，`{ \"product_name\" : “${projectName}, \"env_name\" : “${envName}“ }`，导出\n2.  在上面查到数据中，找出render.revision和services[0].service_name和revision\n3.  在render_set表里查找，`{ \"product_tmpl\" : \"${projectName}\", \"revision\" : NumberLong(2) }`，导出\n4.  在product_template_service/template_service表中查找，`{ \"service_name\" : \"${serviceName}”, \"product_name\" : \"${projectName}, \"revision\" : NumberLong(1) }`，导出\n  \n查询操作日志：\n\n首页-\u003e系统设置-\u003e操作日志-\u003e查询项目-\u003e查询服务-\u003e查到最早的部署日志\n\n---\n\n清除脏数据 ：\n1. product 表，查找生产环境，{ \"product_name\" : ${projectName}, \"env_name\" : ${envName}}\n2. 删除上面查到的 services 里的对应服务\n3. 使用 1 中查找到的 render. name 和 render. revision，在 render_set 表，查找{ \"name\" : ${render. name}, \"revision\" : ${render. revision} }\n4. 删除上面查找到的 service_variables 里的对应服务","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Work/Zadig/%E4%BA%A7%E5%93%81%E5%B7%A5%E4%BD%9C%E6%B5%81":{"title":"产品工作流","content":"","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Work/Zadig/%E5%AE%89%E8%A3%85":{"title":"安装","content":"# Helm\n- 文档：[https://docs.koderover.com/zadig/Zadig%20v1.17.0/install/helm-deploy/](https://docs.koderover.com/zadig/Zadig%20v1.17.0/install/helm-deploy/)\n- `helm upgrade --install ${releaseName} koderover-chart/zadigx --namespace ${namespace} --version=${version} -f ${valuesFile}`\n- 参考values（从ee-dev1复制）：\n```yaml\nconnections:\n  mongodb:\n    connectionString: mongodb://172.16.0.23:27017\n    db: zadig_ee_dev1\n  mysql:\n    auth:\n      password: zadig\n      user: root\n    host: kr-mysql:3306\ndex:\n  config:\n    issuer: http://zadig-ee-dev1-dex:5556/dex\n    staticClients:\n    - id: zadig\n      name: zadig\n      redirectURIs:\n      - http://zadig-ee-dev1.8slan.com/api/v1/callback\n      secret: ZXhhbXBsZS1hcHAtc2VjcmV0\n  fullnameOverride: zadig-ee-dev1-dex\nendpoint:\n  FQDN: zadig-ee-dev1.8slan.com\n  IP: null\n  type: FQDN\nglobal:\n  extensions:\n    extAuth:\n      extauthzServerRef:\n        namespace: zadig-ee-env-dev1\n  image:\n    registry: koderover.tencentcloudcr.com/koderover-public\ngloo:\n  gatewayProxies:\n    gatewayProxy:\n      service:\n        type: NodePort\n      gatewaySettings:\n        customHttpGateway:\n          options:\n            httpConnectionManagerSettings:\n              streamIdleTimeout: 60m\n        customHttpsGateway:\n          options:\n            httpConnectionManagerSettings:\n              streamIdleTimeout: 60m\n  settings:\n    singleNamespace: true\ninit:\n  adminEmail: admin@koderover.com\n  adminPassword: zadig\nee:\n  autoSyncClient: false\n  mongodb:\n    db: \"plutus_zadig_ee_dev1\"\nprotocol: http\ntags:\n  enterprise: true\n  minio: true\n  mongodb: false\n  mysql: true\n\n```\n\n# Plutus Customer\n- 文档：[https://koderover.feishu.cn/docx/doxcnqacwHoNSSol7lvUg32BZah](https://koderover.feishu.cn/docx/doxcnqacwHoNSSol7lvUg32BZah)\n- 需要安装md5sum\n\t- `./install_plutus_customer.sh: line 166: md5sum: command not found`\n - mac的sed和linux sed不兼容\n\t - `sed: 1: \"customer-server.yaml\": command c expects \\ followed by text`\n  - 部署失败\n\t  - `1 error occurred: * failed to install service: zadig-ee, err: unable to build kubernetes objects from release manifest: error validating \"\": error validating data: unknown object type \"nil\" in ConfigMap.data.CLIENT_SECRET`\n\t\t  - `dex.config.staticClients[0].secret`\n\t   - `1 error occurred: * failed to install service: zadig-ee, err: rendered manifests contain a resource that already exists. Unable to continue with install: ClusterRole \"zadig-dex\" in namespace \"\" exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error: key \"meta.helm.sh/release-name\" must equal \"patrick-install-test-zadig-ee-01\": current value is \"zadigx-fansi\"; annotation validation error: key \"meta.helm.sh/release-namespace\" must equal \"patrick-install-test-zadig-01\": current value is \"zadigx-fansi\"`","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Work/Zadig/%E5%B7%A5%E4%BD%9C%E6%B5%81%E5%8F%98%E9%87%8F":{"title":"工作流变量","content":"字段位置：\n- 全局变量：WorkflowV4.Params\n- Job变量：FreestyleJobSpec.Properties.Envs\n\n渲染变量：\n- 全局变量：jobctl.RenderGlobalVariables\n- Job变量：BuildJobExcutorContext","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Work/Zadig/%E5%BE%85%E5%8A%9E%E4%BA%8B%E9%A1%B9":{"title":"待办事项","content":"- [x] 添加 swagger 的可能性 📅 2023-04-26 ✅ 2023-04-28\n- [x] 确认在保存变量的时候需要 clip 哪些值 ✅ 2023-04-28 ^bscuh3\n- [x] 梳理镜像选择、镜像生成规则 ✅ 2023-04-29\n- [ ] 工具分享\n- [ ] ChatGPT 分享","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Work/Zadig/%E6%9C%8D%E5%8A%A1%E5%8F%98%E9%87%8F":{"title":"服务变量","content":"# 现状\n\n1. 初次部署，环境里没有，服务模板里有\n2. 环境里有，服务模板里也有\n3. 环境里没有，服务模板里有\n4. 环境里有，服务模板里没有\n\n  整体流程：\n1. 从服务模版（k8s）或 values（helm）读取可配置变量 tmplVars，并配置变量可见性\n2. 工作流配置变量可选范围，renderVars + tmplVars 合并再根据可见性过滤和配置得到 workflowVars。tmplVars 覆盖 renderVars。\n\t1. 获取key和获取value的覆盖方式不同。\n\t2. tmplVars以可见性的第一层key为主过滤，得到最终结果的所有key\n\t3. 再根据key去查找render中有没有相同的key，如果有，则使用它的值\n3. 执行工作流时获取workflowVars的key和默认value，再修改默认值，最终得到实际变量 taskVars\n4. taskVars（设置值） + renderVars（环境值） + tmplVars（默认值）合并得到execVars。taskVars覆盖renderVars，再覆盖tmplVars。\n\n可见性：\n1. 可见性关闭，用户不可配置，使用时从render的全局变量和服务的默认变量获取值，render的全局变量优先，fallback到服务的默认变量\n2. 可见性打开，用户可见，可以配置\n3. 不管可不可见，该变量最终一定会用到\n\n---\n\n问题：\n- render和tmplSvc合并可能会出现问题，render默认值的问题。render和tmplSvc之间key可能会出现结构改变。kv转yaml报错，yaml合并不符合预期。\n\t- 新设计：使用全局变量时，就不使用render。\n\t\t- ~~render里和全局变量相同key的不合并，剩余的合并，再以tmplSvc为主，将render里有相同key的合并，最后覆盖引用的全局变量，最终根据全局变量进行clip；key还是使用tmplSvc里的，但是value去全局变量中查找，找不到再fallback回tmplSvc~~\n\t- 旧设计：在可见性范围（prefix）里的变量，以服务变量为主，删除render里的部分\n - 代码冗余，理清逻辑，设计新接口？？以前的接口哪都用，搞不清楚作用。还区分生产和测试，没必要\n - 如何扩展数组，而且又不影响变量合并和可配置范围，以prefix为主又有可能会造成结构变化，预期的限制可填范围效果没有达到。\n\n# 新设计\n\n- 增加全局变量模块，去掉可见性，改为是否引用全局变量中的相同的（相同prefix的）变量\n\t- ~~每个服务的配置细微的不同，无法直接使用全局变量中的结构或值，这时候不应该使用全局变量。~~这时候就不应该使用全局变量。\n\t - 所有变量只以第一层key为主。存储是用yaml还是kv？如果存储kv则无法存类型？yaml和kv都存，但是kv只存第一层？\n - 增加GUI\n\n梳理：\n- 服务变量，全局变量，环境变量\n- 以prefix key为主\n- 扩展数组需要自己去全局变量或服务变量里扩展\n- config（key）：tmplSvc去除引用了全局变量的部分\n- exec（value）：以上述key为主，值先去render里查找，fallback到tmplSvc\n- apply：taskVars + renderVars(global+svc) + tmplVars(global+svc)\n- save：taskVars 需要保存哪些值\n\t- ![[Work/Zadig/待办事项#^bscuh3]]\n \n实现：\n- 新建一个variable表，存储所有变量？不止全局变量，考虑扩展性\n- 或者在现有renderset里扩展\n- 数据变更，全局变量、服务变量（kv）、工作流变量，都使用新字段\n- 接口解耦，配置、执行、测试、生产、字段\n\n---\n\n## 分解\n\n### 模块\n\n- 定义变量数据格式\n\t- 定义在项目，实例化、关联关系在环境\n\t- 基础变量\n\t\t- yaml\n\t\t\t- 多行文本\n\t\t- kv\n\t\t\t- 键名，类型，可选范围，默认值，描述\n\t- 全局变量\n\t\t- 基础变量 + 关联关系\n- 模版\n- 服务变量\n\t- 从服务模版提取变量\n- 全局变量\n\t- 在环境中\n- 环境变量\n\t- 全局变量\n\t- 服务变量\n- 工具\n\t- [x] kv 与 yaml 转换 ✅ 2023-04-26\n- Notes\n\t- 生产环境没有全局变量\n\t- 变量在各个阶段的格式\n\t\t- 服务：kv+yaml\n\t\t- 工作流配置：kv+yaml\n\t\t- 工作流执行：kv+yaml\n\t\t- 运行时：yaml\n\t\t- 环境：yaml","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Work/Zadig/%E6%B5%8B%E8%AF%95":{"title":"测试","content":"","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Work/Zadig/%E6%B5%8B%E8%AF%95%E8%B5%84%E6%BA%90":{"title":"测试资源","content":"","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Work/Zadig/%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B7%A5%E4%BD%9C%E6%B5%81":{"title":"自定义工作流","content":"# 分类\n\n- pipeline_task 旧的单产品工作流，已弃用\n- workflow_task 产品工作流\n- workflow_task_v4 自定义工作流\n\njob 运行完会回写pipeline_task_v2\n\n# Job\n\n## JobDeploy\n\n### k8s\n\n- 部署时勾选服务配置：jobTaskSpec.DeployContents 包含 config\n- 并且是系统创建的，并且服务的UpdateConfig == true，则UpdateServiceRevision = true\n- DeployContents不仅仅包含image，则重新部署服务\n- 服务UpdateConfig为是否使用新配置，updatable是否可更新\n- 对仅更新workload镜像的情景，仅返回涉及的deployments和statefulsets的manifest\n\n### Helm\n\n服务配置：使用最新配置，拉取最新服务配置\n\n服务变量：变量可选值为values的值，不需要先在服务里配置变量，直接在工作流里配置可选范围，最终执行时再配置具体的值，其中默认值为环境中的值，环境中如果没有，则去服务模版里拿最新的？默认值是从GetFilteredEnvServices里获取的，可配置范围从template_svc里获取？\n\nhelm是拿最新的values，而不是env中的values，k8s呢？\n\n从template services获取values，合并render_set的values\n\nHelm没有直接使用template_service，tempate_service里的是原始仓库、服务中的values，render_set里的是合并后的、最终生效的values\n\n检查运行时旧的污染变量是否有生效","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Work/Zadig/%E8%B6%85%E6%97%B6%E6%8E%A7%E5%88%B6":{"title":"超时控制","content":"1.  /picket/projects/helm-simple-service 跳转 /project/products\n2.  timeout数据存在了：template_product表\n\n- [[Work/Zadig/自定义工作流|自定义工作流]]：Workflow/Service/Workflow/Job/job_deploy:207\n- [[Work/Zadig/产品工作流|产品工作流]]：workflow/service/workflow/pipeline_controller:252\n\n- [[Work/Zadig/自定义工作流|自定义工作流]]部署超时控制： pkg/microservice/aslan/core/common/service/workflowcontroller/jobcontroller/job_deploy.go:452\n- [[Work/Zadig/产品工作流|产品工作流]]部署超时控制：pkg/microservice/warpdrive/core/service/taskplugin/deploy.go:110\n\n- 发送消息\n\t- 发送消息到[[Work/Zadig/warpdrive|warpdrive]]：pkg/microservice/aslan/core/workflow/service/workflow/pipeline_controller.go:289\n\t- 消息结构：pkg/microservice/aslan/core/common/repository/models/queue.go:28\n- 接收消息\n\t- [[Work/Zadig/warpdrive|warpdrive]]处理消息：pkg/microservice/warpdrive/core/service/taskcontroller/task_handler.go:66\n\t- 消息结构：pkg/microservice/warpdrive/core/service/types/task/model.go:29\n- MQ -\u003e Task.Stage.SubTasks -\u003e Deploy.timeout","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Work/Zadig/%E9%95%9C%E5%83%8F%E5%90%8D%E7%A7%B0":{"title":"镜像名称","content":"- helm部署 -\u003e service module -\u003e get image name\n- K8s部署 -\u003e service -\u003e get image name\n\n- [[Work/Zadig/产品工作流|产品工作流]]创建：pkg/microservice/aslan/core/workflow/service/workflow/workflow_task.go:537\n- [[Work/Zadig/产品工作流|产品工作流]]设置ImageName的地方：pkg/microservice/aslan/core/workflow/service/workflow/workflow_task.go:2401\n\nimage_name数据库存在了哪里\n- [[自定义工作流]]会存在：workflow_task.stages.jobs.spec.envs.key=IMAGE中\n- [[Work/Zadig/产品工作流|产品工作流]]会存在：pipeline_task_v2.stages.sub_tasks.job_ctx.image中\n\n---\n\n# 梳理\n\n- 梳理获取镜像列表的场景\n\t- Build 生成镜像\n\t- 使用镜像的场景\n\t\t- 产品工作流 -\u003e 交付物部署 （原来就是 imageName）\n\t\t- 自定义工作流 -\u003e 部署、镜像分发、k8s 部署 （已改成 imageName）\n\t\t- 发布工作流 -\u003e 蓝绿部署、金丝雀部署、灰度发布、istio发布（还是serviceModule）\n\t\t- 添加触发器\n- 梳理镜像生成规则\n\t- 没有设置代码源\n\t\t- 使用 imageName\n\t- 有设置代码源\n\t\t- 默认没有保存镜像规则\n\t\t\t- 后端默认使用 service module 作为镜像名\n\t\t\t- 前端默认显示的是 IMAGE_NAME\n\t\t\t- 没有 service module 变量，只有 SERVICE（服务名）\n\t\t- 保持了镜像规则\n\t\t\t- 根据镜像规则来设置\n\t- 镜像名更改后，可能会出现选择不到的情况\n\t\t- 仅部署的情况下，无法获取构建生成的完整 image，只能根据规则来选择对应的镜像，或者使用固定的策略\n\t\t- 存储的 imageName 没有按照生成规则来\n- 优化获取镜像名逻辑\n\t- 根据 image 解析 image name\n\t\t- image 解析可能和使用的镜像生成规则不一致。\n\t\t\t- 仅部署时使用哪个，仅部署不知道都已生成的镜像是按什么规则生成的\u0008\n\t\t\t\t- 根据规则来选择\n\t- 从未构建，但是可以部署\n\t\t- 直接服务定义里的镜像名获取镜像\n\t\t- 可修过获取镜像的规则，也就是可修改镜像名称","lastmodified":"2023-05-01T11:46:37.90458941Z","tags":[]},"/Work/Zadig/%E9%97%AE%E9%A2%98":{"title":"问题","content":"- [ ] recover 没有打印堆栈\n- [ ] Build 超时立即重启不正确，需要检测 termination\n- [ ] ensureServiceTmpl、fillServiceTmpl 和 SetCurrentContainerImages，代码冗余","lastmodified":"2023-05-01T11:46:37.90458941Z","tags":[]},"/Work/Zadig/ChatGPT":{"title":"ChatGPT","content":"1. 是否有了实质性的变更？？\n2.  构建代码检查 \n3. 面板数据分析\n4.  Zadig独有的东西，自己能做到，而别人无法做到\n5.  通过chatGPT，根据项目，生成构建脚本、工作流，并执行、接入zadig，最终完成，实现一个完成的devops流程。chatGPT实现生成脚本、接入平台，分发任务等。\n\n1.  聊天ai无法理解项目，无法帮助生成构建脚本\n2.  强烈需要 ChatGPT4的账号，或许可以帮助解析无法理解项目的问题 \n3.  无法访问api申请页面，也算可以访问，充值也需要国外信用卡，api also very expensive。\n4.  chatgpt 4也需要充值\n\n---\n\n-  [How ChatGPT Works Technically | ChatGPT Architecture - YouTube](https://www.youtube.com/watch?v=bSvTVREwSNw)\n- [Top 7 ChatGPT Developer Hacks - YouTube](https://www.youtube.com/watch?v=9W_U1y7RYuE)\n\n---\n\n# 前情提要\n\n## 限制\n\n# 基础功能\n\n# 扩展功能\n\n# 使用技巧\n\n# 短期应用场景\n\n# 长期应用场景\n\n# 参考资料","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]},"/Work/Zadig/warpdrive":{"title":"warpdrive","content":"","lastmodified":"2023-05-01T11:46:37.900589322Z","tags":[]}}